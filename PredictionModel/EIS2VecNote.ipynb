{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07804a10",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "51a22399",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import gc\n",
    "import sys\n",
    "\n",
    "from loguru import logger\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "# %matplotlib qt\n",
    "%matplotlib qt\n",
    "\n",
    "# Detect device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5889426",
   "metadata": {},
   "source": [
    "# Input Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "0cfa4725",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SearchELE(rootPath, ele_pattern = re.compile(r\"(.+?)_归档\")):\n",
    "    '''==================================================\n",
    "        Search all electrode directories in the rootPath\n",
    "        Parameter: \n",
    "            rootPath: current search path\n",
    "            ele_pattern: electrode dir name patten\n",
    "        Returen:\n",
    "            ele_list: list of electrode directories\n",
    "        ==================================================\n",
    "    '''\n",
    "    ele_list = []\n",
    "    for i in os.listdir(rootPath):\n",
    "        match_ele = ele_pattern.match(i)\n",
    "        if match_ele:\n",
    "            ele_list.append([os.path.join(rootPath, i),match_ele.group(1)])\n",
    "    return ele_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "b10d7797",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-04-25 09:45:00.072\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m4\u001b[0m - \u001b[1mSearch in D:/Baihm/EISNN/Archive/ and find 218 electrodes\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "rootPath = \"D:/Baihm/EISNN/Archive/\"\n",
    "ele_list = SearchELE(rootPath)\n",
    "n_ele = len(ele_list)\n",
    "logger.info(f\"Search in {rootPath} and find {n_ele:03d} electrodes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "cbe9174a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Blacklist = [\n",
    "    '01067093',     # Not look like EIS\n",
    "    '01067094',     # Connection Error\n",
    "    '02017385',     # Connection Error\n",
    "    '05127177',     # Open to Short\n",
    "    '06047729',     # Open to Short\n",
    "    '06047730',     # Open to Short\n",
    "    '06047731',     # Open to Short\n",
    "    '09207024',     # Connection Error\n",
    "    '10017038',     # Connection Error\n",
    "    '10037050',     # Connection Error\n",
    "    '10047056',     # Connection Error\n",
    "    '10057069',     # Connection Error\n",
    "    '10057083',     # Always Open\n",
    "    '10057084',     # Chaos\n",
    "    '10057087',     # Connection Error\n",
    "    '22017367',     # Connection Error\n",
    "    '22017371',     # Chaos\n",
    "]\n",
    "\n",
    "GrayList = [\n",
    "    '10037051',     # Connection Error\n",
    "    '10037052',     # Connection Error\n",
    "    '10057071',     # Connection Error\n",
    "    '10067077',     # Wired Shape like connection error\n",
    "    '10150201',     # Wired Shape\n",
    "    '10150202',     # Wired Shape\n",
    "    '10150203',     # Wired Shape\n",
    "    '20037515',     # Wired Shape\n",
    "    '20037516',     # Wired Shape\n",
    "    '20037517',     # Wired Shape\n",
    "    '22037378',     # Connection Error\n",
    "    '22037380',     # Connection Error\n",
    "    '22047376',     # Connection Error\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "b3a68207",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-04-25 09:45:00.323\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m30\u001b[0m - \u001b[1mELE [0/218]: D:/Baihm/EISNN/Archive/01037160_归档\u001b[0m\n",
      "\u001b[32m2025-04-25 09:45:00.519\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m30\u001b[0m - \u001b[1mELE [1/218]: D:/Baihm/EISNN/Archive/01037161_归档\u001b[0m\n",
      "\u001b[32m2025-04-25 09:45:00.734\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m30\u001b[0m - \u001b[1mELE [2/218]: D:/Baihm/EISNN/Archive/01037162_归档\u001b[0m\n",
      "\u001b[32m2025-04-25 09:45:00.913\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m30\u001b[0m - \u001b[1mELE [5/218]: D:/Baihm/EISNN/Archive/01067095_归档\u001b[0m\n",
      "\u001b[32m2025-04-25 09:45:01.271\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m30\u001b[0m - \u001b[1mELE [9/218]: D:/Baihm/EISNN/Archive/02027373_归档\u001b[0m\n",
      "\u001b[32m2025-04-25 09:45:01.378\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m30\u001b[0m - \u001b[1mELE [10/218]: D:/Baihm/EISNN/Archive/02027390_归档\u001b[0m\n",
      "\u001b[32m2025-04-25 09:45:01.526\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m30\u001b[0m - \u001b[1mELE [11/218]: D:/Baihm/EISNN/Archive/02027393_归档\u001b[0m\n",
      "\u001b[32m2025-04-25 09:45:01.657\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m30\u001b[0m - \u001b[1mELE [12/218]: D:/Baihm/EISNN/Archive/02027406_归档\u001b[0m\n",
      "\u001b[32m2025-04-25 09:45:01.808\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m30\u001b[0m - \u001b[1mELE [13/218]: D:/Baihm/EISNN/Archive/02027407_归档\u001b[0m\n",
      "\u001b[32m2025-04-25 09:45:01.945\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m30\u001b[0m - \u001b[1mELE [14/218]: D:/Baihm/EISNN/Archive/02027408_归档\u001b[0m\n",
      "\u001b[32m2025-04-25 09:45:02.066\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m30\u001b[0m - \u001b[1mELE [15/218]: D:/Baihm/EISNN/Archive/02027409_归档\u001b[0m\n",
      "\u001b[32m2025-04-25 09:45:02.183\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m30\u001b[0m - \u001b[1mELE [16/218]: D:/Baihm/EISNN/Archive/02027410_归档\u001b[0m\n",
      "\u001b[32m2025-04-25 09:45:02.394\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m30\u001b[0m - \u001b[1mELE [18/218]: D:/Baihm/EISNN/Archive/02067447_归档\u001b[0m\n",
      "\u001b[32m2025-04-25 09:45:02.516\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m30\u001b[0m - \u001b[1mELE [19/218]: D:/Baihm/EISNN/Archive/02067448_归档\u001b[0m\n",
      "\u001b[32m2025-04-25 09:45:02.641\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m30\u001b[0m - \u001b[1mELE [21/218]: D:/Baihm/EISNN/Archive/02067450_归档\u001b[0m\n",
      "\u001b[32m2025-04-25 09:45:02.880\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m30\u001b[0m - \u001b[1mELE [22/218]: D:/Baihm/EISNN/Archive/05087163_归档\u001b[0m\n",
      "\u001b[32m2025-04-25 09:45:03.050\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m30\u001b[0m - \u001b[1mELE [23/218]: D:/Baihm/EISNN/Archive/05087164_归档\u001b[0m\n",
      "\u001b[32m2025-04-25 09:45:03.286\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m30\u001b[0m - \u001b[1mELE [24/218]: D:/Baihm/EISNN/Archive/05097165_归档\u001b[0m\n",
      "\u001b[32m2025-04-25 09:45:03.483\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m30\u001b[0m - \u001b[1mELE [25/218]: D:/Baihm/EISNN/Archive/05097166_归档\u001b[0m\n",
      "\u001b[32m2025-04-25 09:45:03.606\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m30\u001b[0m - \u001b[1mELE [26/218]: D:/Baihm/EISNN/Archive/05107167_归档\u001b[0m\n",
      "\u001b[32m2025-04-25 09:45:03.792\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m30\u001b[0m - \u001b[1mELE [27/218]: D:/Baihm/EISNN/Archive/05107180_归档\u001b[0m\n",
      "\u001b[32m2025-04-25 09:45:03.967\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m30\u001b[0m - \u001b[1mELE [28/218]: D:/Baihm/EISNN/Archive/05117174_归档\u001b[0m\n",
      "\u001b[32m2025-04-25 09:45:04.167\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m30\u001b[0m - \u001b[1mELE [29/218]: D:/Baihm/EISNN/Archive/05117178_归档\u001b[0m\n",
      "\u001b[32m2025-04-25 09:45:04.422\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m30\u001b[0m - \u001b[1mELE [30/218]: D:/Baihm/EISNN/Archive/05127168_归档\u001b[0m\n",
      "\u001b[32m2025-04-25 09:45:04.558\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m30\u001b[0m - \u001b[1mELE [32/218]: D:/Baihm/EISNN/Archive/06017403_归档\u001b[0m\n",
      "\u001b[32m2025-04-25 09:45:04.646\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m30\u001b[0m - \u001b[1mELE [33/218]: D:/Baihm/EISNN/Archive/06017404_归档\u001b[0m\n",
      "\u001b[32m2025-04-25 09:45:04.737\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m30\u001b[0m - \u001b[1mELE [34/218]: D:/Baihm/EISNN/Archive/06017405_归档\u001b[0m\n",
      "\u001b[32m2025-04-25 09:45:04.984\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m30\u001b[0m - \u001b[1mELE [35/218]: D:/Baihm/EISNN/Archive/06017758_归档\u001b[0m\n",
      "\u001b[32m2025-04-25 09:45:05.187\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m30\u001b[0m - \u001b[1mELE [36/218]: D:/Baihm/EISNN/Archive/06017760_归档\u001b[0m\n",
      "\u001b[32m2025-04-25 09:45:05.352\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m30\u001b[0m - \u001b[1mELE [40/218]: D:/Baihm/EISNN/Archive/08260405_归档\u001b[0m\n",
      "\u001b[32m2025-04-25 09:45:05.549\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m30\u001b[0m - \u001b[1mELE [41/218]: D:/Baihm/EISNN/Archive/08260407_归档\u001b[0m\n",
      "\u001b[32m2025-04-25 09:45:05.770\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m30\u001b[0m - \u001b[1mELE [42/218]: D:/Baihm/EISNN/Archive/08260408_归档\u001b[0m\n",
      "\u001b[32m2025-04-25 09:45:06.366\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m30\u001b[0m - \u001b[1mELE [50/218]: D:/Baihm/EISNN/Archive/09047412_归档\u001b[0m\n",
      "\u001b[32m2025-04-25 09:45:06.766\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m30\u001b[0m - \u001b[1mELE [55/218]: D:/Baihm/EISNN/Archive/09047417_归档\u001b[0m\n",
      "\u001b[32m2025-04-25 09:45:07.205\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m30\u001b[0m - \u001b[1mELE [60/218]: D:/Baihm/EISNN/Archive/09047492_归档\u001b[0m\n",
      "\u001b[32m2025-04-25 09:45:07.446\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m30\u001b[0m - \u001b[1mELE [62/218]: D:/Baihm/EISNN/Archive/09060402_归档\u001b[0m\n",
      "\u001b[32m2025-04-25 09:45:07.554\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m30\u001b[0m - \u001b[1mELE [63/218]: D:/Baihm/EISNN/Archive/09060403_归档\u001b[0m\n",
      "\u001b[32m2025-04-25 09:45:07.647\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m30\u001b[0m - \u001b[1mELE [64/218]: D:/Baihm/EISNN/Archive/09190702_归档\u001b[0m\n",
      "\u001b[32m2025-04-25 09:45:07.786\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m30\u001b[0m - \u001b[1mELE [65/218]: D:/Baihm/EISNN/Archive/09190703_归档\u001b[0m\n",
      "\u001b[32m2025-04-25 09:45:07.895\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m30\u001b[0m - \u001b[1mELE [66/218]: D:/Baihm/EISNN/Archive/09190704_归档\u001b[0m\n",
      "\u001b[32m2025-04-25 09:45:08.179\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m30\u001b[0m - \u001b[1mELE [69/218]: D:/Baihm/EISNN/Archive/09207021_归档\u001b[0m\n",
      "\u001b[32m2025-04-25 09:45:08.526\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m30\u001b[0m - \u001b[1mELE [72/218]: D:/Baihm/EISNN/Archive/09207029_归档\u001b[0m\n",
      "\u001b[32m2025-04-25 09:45:08.761\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m30\u001b[0m - \u001b[1mELE [73/218]: D:/Baihm/EISNN/Archive/09207030_归档\u001b[0m\n",
      "\u001b[32m2025-04-25 09:45:08.905\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m30\u001b[0m - \u001b[1mELE [74/218]: D:/Baihm/EISNN/Archive/09207031_归档\u001b[0m\n",
      "\u001b[32m2025-04-25 09:45:09.074\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m30\u001b[0m - \u001b[1mELE [75/218]: D:/Baihm/EISNN/Archive/09230301_归档\u001b[0m\n",
      "\u001b[32m2025-04-25 09:45:09.239\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m30\u001b[0m - \u001b[1mELE [76/218]: D:/Baihm/EISNN/Archive/09230302_归档\u001b[0m\n",
      "\u001b[32m2025-04-25 09:45:09.486\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m30\u001b[0m - \u001b[1mELE [77/218]: D:/Baihm/EISNN/Archive/09230303_归档\u001b[0m\n",
      "\u001b[32m2025-04-25 09:45:09.722\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m30\u001b[0m - \u001b[1mELE [78/218]: D:/Baihm/EISNN/Archive/09230410_归档\u001b[0m\n",
      "\u001b[32m2025-04-25 09:45:09.858\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m30\u001b[0m - \u001b[1mELE [79/218]: D:/Baihm/EISNN/Archive/09230509_归档\u001b[0m\n",
      "\u001b[32m2025-04-25 09:45:10.004\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m30\u001b[0m - \u001b[1mELE [80/218]: D:/Baihm/EISNN/Archive/09230510_归档\u001b[0m\n",
      "\u001b[32m2025-04-25 09:45:10.341\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m30\u001b[0m - \u001b[1mELE [81/218]: D:/Baihm/EISNN/Archive/09230513_归档\u001b[0m\n",
      "\u001b[32m2025-04-25 09:45:10.468\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m30\u001b[0m - \u001b[1mELE [82/218]: D:/Baihm/EISNN/Archive/09230602_归档\u001b[0m\n",
      "\u001b[32m2025-04-25 09:45:11.020\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m30\u001b[0m - \u001b[1mELE [94/218]: D:/Baihm/EISNN/Archive/09231409_归档\u001b[0m\n",
      "\u001b[32m2025-04-25 09:45:11.276\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m30\u001b[0m - \u001b[1mELE [95/218]: D:/Baihm/EISNN/Archive/09250102_归档\u001b[0m\n",
      "\u001b[32m2025-04-25 09:45:11.523\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m30\u001b[0m - \u001b[1mELE [96/218]: D:/Baihm/EISNN/Archive/09250103_归档\u001b[0m\n",
      "\u001b[32m2025-04-25 09:45:11.731\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m30\u001b[0m - \u001b[1mELE [97/218]: D:/Baihm/EISNN/Archive/09250104_归档\u001b[0m\n",
      "\u001b[32m2025-04-25 09:45:11.949\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m30\u001b[0m - \u001b[1mELE [98/218]: D:/Baihm/EISNN/Archive/09250210_归档\u001b[0m\n",
      "\u001b[32m2025-04-25 09:45:12.133\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m30\u001b[0m - \u001b[1mELE [99/218]: D:/Baihm/EISNN/Archive/09290311_归档\u001b[0m\n",
      "\u001b[32m2025-04-25 09:45:12.313\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m30\u001b[0m - \u001b[1mELE [100/218]: D:/Baihm/EISNN/Archive/09290312_归档\u001b[0m\n",
      "\u001b[32m2025-04-25 09:45:12.524\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m30\u001b[0m - \u001b[1mELE [101/218]: D:/Baihm/EISNN/Archive/09290314_归档\u001b[0m\n",
      "\u001b[32m2025-04-25 09:45:12.769\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m30\u001b[0m - \u001b[1mELE [102/218]: D:/Baihm/EISNN/Archive/09290511_归档\u001b[0m\n",
      "\u001b[32m2025-04-25 09:45:13.081\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m30\u001b[0m - \u001b[1mELE [104/218]: D:/Baihm/EISNN/Archive/10017037_归档\u001b[0m\n",
      "\u001b[32m2025-04-25 09:45:13.323\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m30\u001b[0m - \u001b[1mELE [107/218]: D:/Baihm/EISNN/Archive/10037051_归档\u001b[0m\n",
      "\u001b[32m2025-04-25 09:45:13.498\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m30\u001b[0m - \u001b[1mELE [108/218]: D:/Baihm/EISNN/Archive/10037052_归档\u001b[0m\n",
      "\u001b[32m2025-04-25 09:45:13.664\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m30\u001b[0m - \u001b[1mELE [110/218]: D:/Baihm/EISNN/Archive/10047059_归档\u001b[0m\n",
      "\u001b[32m2025-04-25 09:45:13.800\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m30\u001b[0m - \u001b[1mELE [111/218]: D:/Baihm/EISNN/Archive/10047060_归档\u001b[0m\n",
      "\u001b[32m2025-04-25 09:45:14.022\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m30\u001b[0m - \u001b[1mELE [114/218]: D:/Baihm/EISNN/Archive/10057071_归档\u001b[0m\n",
      "\u001b[32m2025-04-25 09:45:14.243\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m30\u001b[0m - \u001b[1mELE [118/218]: D:/Baihm/EISNN/Archive/10067077_归档\u001b[0m\n",
      "\u001b[32m2025-04-25 09:45:14.319\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m30\u001b[0m - \u001b[1mELE [119/218]: D:/Baihm/EISNN/Archive/10067078_归档\u001b[0m\n",
      "\u001b[32m2025-04-25 09:45:14.489\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m30\u001b[0m - \u001b[1mELE [120/218]: D:/Baihm/EISNN/Archive/10067080_归档\u001b[0m\n",
      "\u001b[32m2025-04-25 09:45:14.631\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m30\u001b[0m - \u001b[1mELE [121/218]: D:/Baihm/EISNN/Archive/10080508_归档\u001b[0m\n",
      "\u001b[32m2025-04-25 09:45:14.808\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m30\u001b[0m - \u001b[1mELE [122/218]: D:/Baihm/EISNN/Archive/10080509_归档\u001b[0m\n",
      "\u001b[32m2025-04-25 09:45:15.092\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m30\u001b[0m - \u001b[1mELE [124/218]: D:/Baihm/EISNN/Archive/10080601_归档\u001b[0m\n",
      "\u001b[32m2025-04-25 09:45:15.277\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m30\u001b[0m - \u001b[1mELE [126/218]: D:/Baihm/EISNN/Archive/10080603_归档\u001b[0m\n",
      "\u001b[32m2025-04-25 09:45:15.419\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m30\u001b[0m - \u001b[1mELE [127/218]: D:/Baihm/EISNN/Archive/10080706_归档\u001b[0m\n",
      "\u001b[32m2025-04-25 09:45:15.549\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m30\u001b[0m - \u001b[1mELE [128/218]: D:/Baihm/EISNN/Archive/10080708_归档\u001b[0m\n",
      "\u001b[32m2025-04-25 09:45:15.745\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m30\u001b[0m - \u001b[1mELE [132/218]: D:/Baihm/EISNN/Archive/10150201_归档\u001b[0m\n",
      "\u001b[32m2025-04-25 09:45:15.984\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m30\u001b[0m - \u001b[1mELE [133/218]: D:/Baihm/EISNN/Archive/10150202_归档\u001b[0m\n",
      "\u001b[32m2025-04-25 09:45:16.179\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m30\u001b[0m - \u001b[1mELE [134/218]: D:/Baihm/EISNN/Archive/10150203_归档\u001b[0m\n",
      "\u001b[32m2025-04-25 09:45:16.349\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m30\u001b[0m - \u001b[1mELE [135/218]: D:/Baihm/EISNN/Archive/10160301_归档\u001b[0m\n",
      "\u001b[32m2025-04-25 09:45:16.481\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m30\u001b[0m - \u001b[1mELE [136/218]: D:/Baihm/EISNN/Archive/10160302_归档\u001b[0m\n",
      "\u001b[32m2025-04-25 09:45:16.716\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m30\u001b[0m - \u001b[1mELE [138/218]: D:/Baihm/EISNN/Archive/10210301_归档\u001b[0m\n",
      "\u001b[32m2025-04-25 09:45:16.866\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m30\u001b[0m - \u001b[1mELE [139/218]: D:/Baihm/EISNN/Archive/10210302_归档\u001b[0m\n",
      "\u001b[32m2025-04-25 09:45:17.021\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m30\u001b[0m - \u001b[1mELE [140/218]: D:/Baihm/EISNN/Archive/10210303_归档\u001b[0m\n",
      "\u001b[32m2025-04-25 09:45:17.435\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m30\u001b[0m - \u001b[1mELE [146/218]: D:/Baihm/EISNN/Archive/11037454_归档\u001b[0m\n",
      "\u001b[32m2025-04-25 09:45:17.766\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m30\u001b[0m - \u001b[1mELE [149/218]: D:/Baihm/EISNN/Archive/11037715_归档\u001b[0m\n",
      "\u001b[32m2025-04-25 09:45:17.963\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m30\u001b[0m - \u001b[1mELE [150/218]: D:/Baihm/EISNN/Archive/11037716_归档\u001b[0m\n",
      "\u001b[32m2025-04-25 09:45:18.182\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m30\u001b[0m - \u001b[1mELE [151/218]: D:/Baihm/EISNN/Archive/11037717_归档\u001b[0m\n",
      "\u001b[32m2025-04-25 09:45:18.436\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m30\u001b[0m - \u001b[1mELE [152/218]: D:/Baihm/EISNN/Archive/11047722_归档\u001b[0m\n",
      "\u001b[32m2025-04-25 09:45:18.639\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m30\u001b[0m - \u001b[1mELE [153/218]: D:/Baihm/EISNN/Archive/11047725_归档\u001b[0m\n",
      "\u001b[32m2025-04-25 09:45:18.859\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m30\u001b[0m - \u001b[1mELE [154/218]: D:/Baihm/EISNN/Archive/11047726_归档\u001b[0m\n",
      "\u001b[32m2025-04-25 09:45:19.020\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m30\u001b[0m - \u001b[1mELE [155/218]: D:/Baihm/EISNN/Archive/11050201_归档\u001b[0m\n",
      "\u001b[32m2025-04-25 09:45:19.124\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m30\u001b[0m - \u001b[1mELE [156/218]: D:/Baihm/EISNN/Archive/11050202_归档\u001b[0m\n",
      "\u001b[32m2025-04-25 09:45:19.234\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m30\u001b[0m - \u001b[1mELE [157/218]: D:/Baihm/EISNN/Archive/11050203_归档\u001b[0m\n",
      "\u001b[32m2025-04-25 09:45:19.457\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m30\u001b[0m - \u001b[1mELE [158/218]: D:/Baihm/EISNN/Archive/11057709_归档\u001b[0m\n",
      "\u001b[32m2025-04-25 09:45:19.697\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m30\u001b[0m - \u001b[1mELE [159/218]: D:/Baihm/EISNN/Archive/11057710_归档\u001b[0m\n",
      "\u001b[32m2025-04-25 09:45:19.904\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m30\u001b[0m - \u001b[1mELE [160/218]: D:/Baihm/EISNN/Archive/11057712_归档\u001b[0m\n",
      "\u001b[32m2025-04-25 09:45:20.179\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m30\u001b[0m - \u001b[1mELE [162/218]: D:/Baihm/EISNN/Archive/11067222_归档\u001b[0m\n",
      "\u001b[32m2025-04-25 09:45:20.356\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m30\u001b[0m - \u001b[1mELE [163/218]: D:/Baihm/EISNN/Archive/11067223_归档\u001b[0m\n",
      "\u001b[32m2025-04-25 09:45:20.512\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m30\u001b[0m - \u001b[1mELE [164/218]: D:/Baihm/EISNN/Archive/11067224_归档\u001b[0m\n",
      "\u001b[32m2025-04-25 09:45:20.981\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m30\u001b[0m - \u001b[1mELE [179/218]: D:/Baihm/EISNN/Archive/16037225_归档\u001b[0m\n",
      "\u001b[32m2025-04-25 09:45:21.114\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m30\u001b[0m - \u001b[1mELE [180/218]: D:/Baihm/EISNN/Archive/16037226_归档\u001b[0m\n",
      "\u001b[32m2025-04-25 09:45:21.244\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m30\u001b[0m - \u001b[1mELE [181/218]: D:/Baihm/EISNN/Archive/16037227_归档\u001b[0m\n",
      "\u001b[32m2025-04-25 09:45:21.538\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m30\u001b[0m - \u001b[1mELE [182/218]: D:/Baihm/EISNN/Archive/20037515_归档\u001b[0m\n",
      "\u001b[32m2025-04-25 09:45:21.711\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m30\u001b[0m - \u001b[1mELE [183/218]: D:/Baihm/EISNN/Archive/20037516_归档\u001b[0m\n",
      "\u001b[32m2025-04-25 09:45:21.880\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m30\u001b[0m - \u001b[1mELE [184/218]: D:/Baihm/EISNN/Archive/20037517_归档\u001b[0m\n",
      "\u001b[32m2025-04-25 09:45:21.998\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m30\u001b[0m - \u001b[1mELE [185/218]: D:/Baihm/EISNN/Archive/20057520_归档\u001b[0m\n",
      "\u001b[32m2025-04-25 09:45:22.151\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m30\u001b[0m - \u001b[1mELE [186/218]: D:/Baihm/EISNN/Archive/20057521_归档\u001b[0m\n",
      "\u001b[32m2025-04-25 09:45:22.277\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m30\u001b[0m - \u001b[1mELE [187/218]: D:/Baihm/EISNN/Archive/20067523_归档\u001b[0m\n",
      "\u001b[32m2025-04-25 09:45:22.384\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m30\u001b[0m - \u001b[1mELE [188/218]: D:/Baihm/EISNN/Archive/20067524_归档\u001b[0m\n",
      "\u001b[32m2025-04-25 09:45:22.559\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m30\u001b[0m - \u001b[1mELE [189/218]: D:/Baihm/EISNN/Archive/20067525_归档\u001b[0m\n",
      "\u001b[32m2025-04-25 09:45:22.707\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m30\u001b[0m - \u001b[1mELE [191/218]: D:/Baihm/EISNN/Archive/22017368_归档\u001b[0m\n",
      "\u001b[32m2025-04-25 09:45:22.789\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m30\u001b[0m - \u001b[1mELE [192/218]: D:/Baihm/EISNN/Archive/22017369_归档\u001b[0m\n",
      "\u001b[32m2025-04-25 09:45:22.931\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m30\u001b[0m - \u001b[1mELE [193/218]: D:/Baihm/EISNN/Archive/22017370_归档\u001b[0m\n",
      "\u001b[32m2025-04-25 09:45:23.084\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m30\u001b[0m - \u001b[1mELE [195/218]: D:/Baihm/EISNN/Archive/22017372_归档\u001b[0m\n",
      "\u001b[32m2025-04-25 09:45:23.270\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m30\u001b[0m - \u001b[1mELE [197/218]: D:/Baihm/EISNN/Archive/22027365_归档\u001b[0m\n",
      "\u001b[32m2025-04-25 09:45:23.360\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m30\u001b[0m - \u001b[1mELE [198/218]: D:/Baihm/EISNN/Archive/22027366_归档\u001b[0m\n",
      "\u001b[32m2025-04-25 09:45:23.446\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m30\u001b[0m - \u001b[1mELE [199/218]: D:/Baihm/EISNN/Archive/22037378_归档\u001b[0m\n",
      "\u001b[32m2025-04-25 09:45:23.540\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m30\u001b[0m - \u001b[1mELE [200/218]: D:/Baihm/EISNN/Archive/22037379_归档\u001b[0m\n",
      "\u001b[32m2025-04-25 09:45:23.701\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m30\u001b[0m - \u001b[1mELE [201/218]: D:/Baihm/EISNN/Archive/22037380_归档\u001b[0m\n",
      "\u001b[32m2025-04-25 09:45:23.795\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m30\u001b[0m - \u001b[1mELE [202/218]: D:/Baihm/EISNN/Archive/22047374_归档\u001b[0m\n",
      "\u001b[32m2025-04-25 09:45:23.904\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m30\u001b[0m - \u001b[1mELE [203/218]: D:/Baihm/EISNN/Archive/22047376_归档\u001b[0m\n",
      "\u001b[32m2025-04-25 09:45:23.995\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m30\u001b[0m - \u001b[1mELE [204/218]: D:/Baihm/EISNN/Archive/22047377_归档\u001b[0m\n",
      "\u001b[32m2025-04-25 09:45:24.380\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m30\u001b[0m - \u001b[1mELE [210/218]: D:/Baihm/EISNN/Archive/23177396_归档\u001b[0m\n",
      "\u001b[32m2025-04-25 09:45:24.620\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m30\u001b[0m - \u001b[1mELE [217/218]: D:/Baihm/EISNN/Archive/29036057_归档\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "240000"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "MODEL_SUFFIX = \"Matern12_Ver01\"\n",
    "\n",
    "all_data_list = []\n",
    "all_id_list = []\n",
    "\n",
    "_ch_pattern = re.compile(r\"ch_(\\d{3})\")\n",
    "\n",
    "for i in range(n_ele):\n",
    "# for i in range(3):\n",
    "    if ele_list[i][1] in Blacklist:\n",
    "        continue\n",
    "\n",
    "    fd_pt = os.path.join(ele_list[i][0], MODEL_SUFFIX, f\"{ele_list[i][1]}_{MODEL_SUFFIX}.pt\")\n",
    "    if not os.path.exists(fd_pt):\n",
    "        # logger.warning(f\"{fd_pt} does not exist\")\n",
    "        continue\n",
    "    data_pt = torch.load(fd_pt, weights_only=False)\n",
    "    _meta_group = data_pt[\"meta_group\"]\n",
    "    _data_group = data_pt[\"data_group\"]\n",
    "\n",
    "    n_day       = _meta_group[\"n_day\"]\n",
    "    n_ch        = _meta_group[\"n_ch\"]\n",
    "    n_valid_ch  = len(_data_group[\"Channels\"])\n",
    "\n",
    "    # ignore abnormal ele\n",
    "    if n_ch != 128 or n_valid_ch != n_ch:\n",
    "        if n_day < 5 or n_valid_ch <= 100:\n",
    "            continue\n",
    "\n",
    "    logger.info(f\"ELE [{i}/{n_ele}]: {ele_list[i][0]}\")\n",
    "\n",
    "\n",
    "    ele_data_list = []\n",
    "    ele_id_list = []\n",
    "    # Iteration by channel\n",
    "    for j in _data_group['Channels']:\n",
    "        _ch_data = _data_group[j][\"y_eval\"]\n",
    "        ele_data_list.append(_ch_data)\n",
    "\n",
    "        _ch_id = _ch_pattern.match(j)\n",
    "        _ch_id = int(_ch_id.group(1))\n",
    "\n",
    "        _id = [i, _ch_id] * np.shape(_ch_data)[0]\n",
    "        _id = np.array(_id).reshape(-1,2)\n",
    "        ele_id_list.append(_id)\n",
    "        \n",
    "    \n",
    "    all_data_list.append(ele_data_list)\n",
    "    all_id_list.append(ele_id_list)\n",
    "    \n",
    "    # ele_data_list = np.vstack(ele_data_list)\n",
    "    # all_data_list.append(ele_data_list)\n",
    "\n",
    "# all_data_list = np.vstack(all_data_list)\n",
    "# all_id_list = np.vstack(all_id_list)\n",
    "\n",
    "\n",
    "del data_pt, _meta_group, _data_group, _ch_data\n",
    "gc.collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641a414d",
   "metadata": {},
   "source": [
    "# Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "d6a08f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "f9f8c480",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all2seq(data_list, id_list = None):\n",
    "    seq_data_list    = []\n",
    "    seq_id_list     = []\n",
    "    for i in range(len(data_list)):\n",
    "        for j in range(len(data_list[i])):\n",
    "            seq_data_list.append(data_list[i][j])\n",
    "            if id_list is not None:\n",
    "                seq_id_list.append(id_list[i][j])\n",
    "    return seq_data_list, seq_id_list\n",
    "\n",
    "def load_all2ch(data_list, id_list = None):\n",
    "    ch_data_list, ch_id_list = load_all2seq(data_list, id_list)\n",
    "    ch_data_list = np.vstack(ch_data_list)\n",
    "    if id_list is not None:\n",
    "        ch_id_list = np.vstack(ch_id_list)\n",
    "    return ch_data_list, ch_id_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b4f0bb",
   "metadata": {},
   "source": [
    "## Tran & Eval Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "54674192",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_loss(x_rec, x, mu, logvar, beta=1e-3):\n",
    "    rec = F.mse_loss(x_rec, x)\n",
    "    kld = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return rec + beta * kld\n",
    "\n",
    "# ===== 训练函数 =====\n",
    "def train_model(model, train_ds, val_ds, num_epochs=20, batch_size=64, lr=1e-3):\n",
    "    # train_ds = EISDataset_CNN(train_list)\n",
    "    # val_ds   = EISDataset_CNN(val_list)\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "    val_loader   = DataLoader(val_ds,   batch_size=batch_size)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    train_loss_recorder = []\n",
    "    eval_loss_recorder = []\n",
    "\n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for x in train_loader:\n",
    "            x = x.to(device)\n",
    "\n",
    "            x_rec, mu, lv = model(x)\n",
    "            loss = vae_loss(x_rec, x, mu, lv)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * x.size(0)\n",
    "\n",
    "        # 验证\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for x in val_loader:\n",
    "                x = x.to(device)\n",
    "\n",
    "                x_rec, mu, lv = model(x)\n",
    "                loss = vae_loss(x_rec, x, mu, lv)\n",
    "                # mu, lv = model.encode(x)\n",
    "                # x_rec = model.decode(mu)\n",
    "                # loss = vae_loss(x_rec, x, mu, lv)\n",
    "                \n",
    "                val_loss += loss.item() * x.size(0)\n",
    "\n",
    "        train_loss /= len(train_ds)\n",
    "        val_loss   /= len(val_ds)\n",
    "\n",
    "        train_loss_recorder.append(train_loss)\n",
    "        eval_loss_recorder.append(val_loss)\n",
    "        print(f\"Epoch {epoch}/{num_epochs}  Train: {train_loss:.6f}  Val: {val_loss:.6f}\")\n",
    "\n",
    "    return model, train_loss_recorder, eval_loss_recorder\n",
    "\n",
    "\n",
    "# ===== 可视化重建 =====\n",
    "def visualize_EISVAECNN(model, ds, num=5):\n",
    "    # ds = EISDataset_CNN(data_list)\n",
    "    loader = DataLoader(ds, batch_size=num, shuffle=True)\n",
    "    x = next(iter(loader)).to(device)   # [num,2,101]\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        x_rec, mu, lv = model(x)\n",
    "\n",
    "    x = x.cpu().numpy()\n",
    "    x_rec = x_rec.cpu().numpy()\n",
    "\n",
    "    for i in range(num):\n",
    "        plt.figure(figsize=(6,3))\n",
    "        # 实部\n",
    "        plt.subplot(1,2,1)\n",
    "        plt.plot(x[i,0], label=\"orig\", alpha = 0.5)\n",
    "        plt.plot(x_rec[i,0], '--', label=\"rec\")\n",
    "        plt.title(f\"Sample {i} Real\")\n",
    "        plt.legend()\n",
    "        # 虚部\n",
    "        plt.subplot(1,2,2)\n",
    "        plt.plot(x[i,1], label=\"orig\", alpha = 0.5)\n",
    "        plt.plot(x_rec[i,1], '--', label=\"rec\")\n",
    "        plt.title(f\"Sample {i} Imag\")\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc456ed9",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "3d4df681",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EISDataset_DNN(Dataset):\n",
    "    def __init__(self, data_list, id_list = None):\n",
    "        # data_list: n x m x k x l x 2 list\n",
    "        # n: number of electrodes\n",
    "        # m: number of channels\n",
    "        # k: number of timestamps\n",
    "        # l: number of freq as dimensions\n",
    "        # 2: real and imaginary parts after logrithm\n",
    "\n",
    "        _data, _id  = load_all2ch(data_list, id_list)\n",
    "        _data = [torch.tensor(x, dtype=torch.float32) for x in _data]\n",
    "\n",
    "        self.data = _data\n",
    "        self.id = _id\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Return [101,2] for Linear\n",
    "        return self.data[idx]\n",
    "   \n",
    "class EISDataset_CNN(Dataset):\n",
    "    def __init__(self, data_list, id_list = None):\n",
    "        # data_list: n x m x k x l x 2 list\n",
    "        # n: number of electrodes\n",
    "        # m: number of channels\n",
    "        # k: number of timestamps\n",
    "        # l: number of freq as dimensions\n",
    "        # 2: real and imaginary parts after logrithm\n",
    "\n",
    "        _data, _id  = load_all2ch(data_list, id_list)\n",
    "        _data = [torch.tensor(x, dtype=torch.float32) for x in _data]\n",
    "\n",
    "        self.data = _data\n",
    "        self.id = _id\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Return [2,101] for Conv1D\n",
    "        return self.data[idx].permute(1,0)  # [2,101] [in_ch, in_dim]\n",
    "\n",
    "class EISDataset_SEQ(Dataset):\n",
    "    def __init__(self, data_list, id_list = None):\n",
    "        # data_list: n x m x k x l x 2 list\n",
    "        # n: number of electrodes\n",
    "        # m: number of channels\n",
    "        # k: number of timestamps\n",
    "        # l: number of freq as dimensions\n",
    "        # 2: real and imaginary parts after logrithm\n",
    "\n",
    "        _data, _id  = load_all2seq(data_list, id_list)\n",
    "        _data = [torch.tensor(x, dtype=torch.float32) for x in _data]\n",
    "\n",
    "        self.data = _data\n",
    "        self.id = _id\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx].permute(0,2,1)  # [k,2,101] [in_ch, in_dim]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7297b4",
   "metadata": {},
   "source": [
    "## Plot Latent Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "65200e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def VAE_latent(model, ds, batch_size=64):\n",
    "    loader = DataLoader(ds, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    _len_data = ds.__len__()\n",
    "    _poi = 0\n",
    "\n",
    "    latent_space_inst = []\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x in loader:\n",
    "            x = x.to(device)\n",
    "            mu, lv = model.encoder(x)\n",
    "            latent_space_inst.append(mu.cpu().numpy())\n",
    "\n",
    "            _poi = _poi + x.size(0)\n",
    "            if _poi % 1000 == 0:\n",
    "                logger.info(f\"[{_poi}]/[{_len_data}]\")\n",
    "\n",
    "    latent_space_inst = np.concatenate(latent_space_inst, axis=0)  # [B,z_dim]\n",
    "\n",
    "\n",
    "    _pca_inst = PCA(n_components=latent_space_inst.shape[1])\n",
    "    latent_dd = _pca_inst.fit_transform(latent_space_inst)\n",
    "    \n",
    "    \n",
    "    explained = _pca_inst.explained_variance_ratio_\n",
    "    eff_dim = (explained.cumsum() < 0.99).sum() + 1\n",
    "\n",
    "\n",
    "    fig, axis = plt.subplots(2,1,\n",
    "                gridspec_kw={'height_ratios': [4,1]},\n",
    "                figsize=(9, 9))\n",
    "    axis[0].scatter(latent_dd[:, 0], latent_dd[:, 1], alpha=0.5, s = 0.001)\n",
    "\n",
    "    axis[0].set_aspect('equal', adjustable='box')\n",
    "    axis[0].set_box_aspect(1)\n",
    "    axis[0].set_title(\"Latent Space\")\n",
    "    \n",
    "    axis[1].plot(_pca_inst.explained_variance_ratio_,\n",
    "                 label = f\"Valid Dimension = {eff_dim}\")\n",
    "    axis[1].legend()\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "\n",
    "    return latent_dd, eff_dim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34d2dab",
   "metadata": {},
   "source": [
    "## Kernel Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "c0453b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_conv1d_layers(model, layer_type = nn.Conv1d):\n",
    "    conv_layers = []\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, layer_type):\n",
    "            conv_layers.append((name, module))\n",
    "    return conv_layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "6abd5dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_conv1d_kernel_importance(conv_layer, layer_name=None, mode='l2'):\n",
    "    \"\"\"\n",
    "    mode: 'l2' 或 'l1'\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        weights = conv_layer.weight.cpu()  # [out_ch, in_ch, kernel_size]\n",
    "\n",
    "        if mode == 'l2':\n",
    "            importance = torch.norm(weights.view(weights.size(0), -1), p=2, dim=1)  # 每个 out_ch 的 L2 norm\n",
    "        elif mode == 'l1':\n",
    "            importance = torch.norm(weights.view(weights.size(0), -1), p=1, dim=1)\n",
    "        else:\n",
    "            raise ValueError(\"Only 'l2' and 'l1' are supported\")\n",
    "\n",
    "        importance = importance.numpy()\n",
    "\n",
    "    # 可视化\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.bar(np.arange(len(importance)), importance)\n",
    "    plt.xlabel('Kernel Index')\n",
    "    plt.ylabel(f'{mode.upper()} Norm')\n",
    "    plt.title(f'Kernel Importance in {layer_name or \"Conv1d Layer\"}')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def visualize_conv1d_kernels(conv, title='Conv1d Kernels'):\n",
    "    weight = conv.weight.data.cpu()  # shape: [out_ch, in_ch, kernel_size]\n",
    "    out_ch, in_ch, k_size = weight.shape\n",
    "\n",
    "    # 每个输出通道我们可以画成一个子图，每行是一个输入通道的 kernel\n",
    "    n_rows = np.floor(np.sqrt(out_ch)).astype(int)\n",
    "    n_cols = np.ceil(out_ch / n_rows).astype(int)\n",
    "\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(n_cols*2, n_rows*2), sharex=True, sharey=True)\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for idx in range(out_ch):\n",
    "        ax = axes[idx]\n",
    "        kernel = weight[idx]  # shape: [in_ch, k_size]\n",
    "\n",
    "        for ic in range(in_ch):\n",
    "            ax.plot(kernel[ic].numpy(), label=f'InCh {ic}', alpha=0.7)\n",
    "\n",
    "        ax.set_title(f'OutCh {idx}', fontsize=8)\n",
    "        if idx % n_cols != 0:\n",
    "            ax.set_yticklabels([])\n",
    "        if idx < (n_rows - 1) * n_cols:\n",
    "            ax.set_xticklabels([])\n",
    "\n",
    "    # Remove unused subplots\n",
    "    for i in range(out_ch, len(axes)):\n",
    "        fig.delaxes(axes[i])\n",
    "\n",
    "    fig.suptitle(title, fontsize=14)\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "7c6f33ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel_dimensionality(conv, plot=True):\n",
    "    W = conv.weight.data.cpu().numpy()  # [out_ch, in_ch, k_size]\n",
    "    W_flat = W.reshape(W.shape[0], -1)  # [out_ch, in_ch * k_size]\n",
    "\n",
    "    pca = PCA(n_components=W_flat.shape[0])\n",
    "    pca.fit(W_flat)\n",
    "    explained = pca.explained_variance_ratio_\n",
    "\n",
    "    if plot:\n",
    "        import matplotlib.pyplot as plt\n",
    "        plt.figure(figsize=(5,3))\n",
    "        plt.plot(explained.cumsum(), marker='o')\n",
    "        plt.xlabel('Number of Components')\n",
    "        plt.ylabel('Explained Variance Ratio')\n",
    "        plt.title('Cumulative PCA on Conv Kernels')\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "    # Return effective dimension \n",
    "    # (minimum number of dimensions that explain 99% variance)\n",
    "    \n",
    "    eff_dim = (explained.cumsum() < 0.99).sum() + 1\n",
    "    return eff_dim, explained"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0062a601",
   "metadata": {},
   "source": [
    "# Vectorization Model Design"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0836947",
   "metadata": {},
   "source": [
    "## Ver01 - 3 x CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09881e0c",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "a72a3241",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Curve2VecEncoder_Ver01(nn.Module):\n",
    "    def __init__(self, in_ch, in_dim, hid_ch, \n",
    "                 z_dim, kernel_size):\n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "        _layers = []\n",
    "\n",
    "        pre_ch = in_ch\n",
    "        poi_ch = hid_ch\n",
    "        _layers.append(nn.Conv1d(pre_ch, poi_ch, kernel_size=kernel_size))\n",
    "        _layers.append(nn.ReLU())\n",
    "        # _layers.append(nn.BatchNorm1d(poi_ch))\n",
    "        \n",
    "        pre_ch = poi_ch\n",
    "        poi_ch = poi_ch * 2\n",
    "        _layers.append(nn.Conv1d(pre_ch, poi_ch, kernel_size=kernel_size))\n",
    "        _layers.append(nn.ReLU())\n",
    "        # _layers.append(nn.BatchNorm1d(poi_ch))\n",
    "        \n",
    "        pre_ch = poi_ch\n",
    "        poi_ch = poi_ch * 2\n",
    "        _layers.append(nn.Conv1d(pre_ch, poi_ch, kernel_size=kernel_size))\n",
    "        _layers.append(nn.ReLU())\n",
    "        # _layers.append(nn.BatchNorm1d(poi_ch))\n",
    "\n",
    "\n",
    "        self.conv = nn.Sequential(*_layers)\n",
    "        self.pool = nn.AdaptiveAvgPool1d(1)\n",
    "\n",
    "\n",
    "        self.fc_mu = nn.Linear(poi_ch, z_dim)\n",
    "        self.fc_lv = nn.Linear(poi_ch, z_dim)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.conv(x)                # [B,ch,in_dim]\n",
    "        h = self.pool(h).squeeze(-1)    # [B,ch]\n",
    "        return self.fc_mu(h), self.fc_lv(h) \n",
    "\n",
    "\n",
    "class Curve2VecDecoder_Ver01(nn.Module):\n",
    "    def __init__(self, out_ch, out_dim, hid_ch, \n",
    "                 z_dim, kernel_size):\n",
    "        super().__init__()\n",
    "        self.hid_ch = hid_ch\n",
    "        self.out_dim = out_dim\n",
    "\n",
    "\n",
    "        self.fc_expand = nn.Linear(z_dim, hid_ch * out_dim)\n",
    "\n",
    "\n",
    "        _layers = []\n",
    "        _layers.append(nn.ReLU())\n",
    "\n",
    "        pre_ch = hid_ch\n",
    "        poi_ch = hid_ch//2\n",
    "        _layers.append(nn.ConvTranspose1d(pre_ch, poi_ch, kernel_size=kernel_size, padding=kernel_size//2))\n",
    "        _layers.append(nn.ReLU())\n",
    "        # _layers.append(nn.BatchNorm1d(poi_ch))\n",
    "        \n",
    "        # pre_ch = poi_ch\n",
    "        # poi_ch = poi_ch//2\n",
    "        # _layers.append(nn.ConvTranspose1d(pre_ch, poi_ch, kernel_size=kernel_size, padding=kernel_size//2))\n",
    "        # _layers.append(nn.ReLU())\n",
    "        # # _layers.append(nn.BatchNorm1d(poi_ch))\n",
    "\n",
    "        pre_ch = poi_ch\n",
    "        poi_ch = out_ch\n",
    "        _layers.append(nn.Conv1d(pre_ch, poi_ch, kernel_size=kernel_size, padding=kernel_size//2))\n",
    "\n",
    "\n",
    "        # pre_ch = hid_ch\n",
    "        # poi_ch = out_ch\n",
    "        # _layers.append(nn.Conv1d(pre_ch, poi_ch, kernel_size=kernel_size, padding=kernel_size//2))\n",
    "\n",
    "\n",
    "        \n",
    "        self.deconv = nn.Sequential(*_layers)\n",
    "\n",
    "\n",
    "    def forward(self, z):\n",
    "        h = self.fc_expand(z)           # [B,in_ch*in_dim]\n",
    "        h = h.view(-1, self.hid_ch, self.out_dim)\n",
    "        h = self.deconv(h)               # [B,in_ch,in_dim]\n",
    "        return h                        # [B,in_ch,in_dim]\n",
    "\n",
    "class Curve2VecVAE_Ver01(nn.Module):\n",
    "    def __init__(self, in_ch=2, in_dim=101, \n",
    "                 enc_hid_ch = 16,\n",
    "                 dec_hid_ch = 16,\n",
    "                 z_dim = 16, kernel_size = 13):\n",
    "        super().__init__()\n",
    "        self.encoder = Curve2VecEncoder_Ver01(in_ch, in_dim, enc_hid_ch, z_dim, kernel_size)\n",
    "        self.decoder = Curve2VecDecoder_Ver01(in_ch, in_dim, dec_hid_ch, z_dim, kernel_size)\n",
    "\n",
    "    def reparam(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, lv = self.encoder(x)\n",
    "        z = self.reparam(mu, lv)\n",
    "        x_rec = self.decoder(z)\n",
    "        return x_rec, mu, lv \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596f2188",
   "metadata": {},
   "source": [
    "### Running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "dd429ac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65242\n"
     ]
    }
   ],
   "source": [
    "model_ver01 = Curve2VecVAE_Ver01().to(device)\n",
    "print(count_parameters(model_ver01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "1a900e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 150\n",
    "batch_size=128\n",
    "lr=1e-3\n",
    "random_state = None\n",
    "\n",
    "train_list, val_list = train_test_split(all_data_list, test_size=0.2, random_state=random_state)\n",
    "len(val_list)\n",
    "\n",
    "all_ds = EISDataset_CNN(all_data_list)\n",
    "train_ds = EISDataset_CNN(train_list)\n",
    "val_ds   = EISDataset_CNN(val_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "2a3f1a77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150  Train: 0.668775  Val: 0.065801\n",
      "Epoch 2/150  Train: 0.044477  Val: 0.038572\n",
      "Epoch 3/150  Train: 0.033252  Val: 0.023493\n",
      "Epoch 4/150  Train: 0.025913  Val: 0.036400\n",
      "Epoch 5/150  Train: 0.023307  Val: 0.018160\n",
      "Epoch 6/150  Train: 0.021148  Val: 0.030491\n",
      "Epoch 7/150  Train: 0.019845  Val: 0.014824\n",
      "Epoch 8/150  Train: 0.018673  Val: 0.025329\n",
      "Epoch 9/150  Train: 0.017164  Val: 0.013390\n",
      "Epoch 10/150  Train: 0.015754  Val: 0.012688\n",
      "Epoch 11/150  Train: 0.014678  Val: 0.012148\n",
      "Epoch 12/150  Train: 0.013849  Val: 0.011507\n",
      "Epoch 13/150  Train: 0.013463  Val: 0.011766\n",
      "Epoch 14/150  Train: 0.012242  Val: 0.008790\n",
      "Epoch 15/150  Train: 0.011915  Val: 0.010949\n",
      "Epoch 16/150  Train: 0.011485  Val: 0.016782\n",
      "Epoch 17/150  Train: 0.013469  Val: 0.009198\n",
      "Epoch 18/150  Train: 0.010976  Val: 0.009268\n",
      "Epoch 19/150  Train: 0.010512  Val: 0.007781\n",
      "Epoch 20/150  Train: 0.010166  Val: 0.010468\n",
      "Epoch 21/150  Train: 0.009866  Val: 0.007529\n",
      "Epoch 22/150  Train: 0.009772  Val: 0.008094\n",
      "Epoch 23/150  Train: 0.009559  Val: 0.011930\n",
      "Epoch 24/150  Train: 0.009219  Val: 0.006850\n",
      "Epoch 25/150  Train: 0.009044  Val: 0.007202\n",
      "Epoch 26/150  Train: 0.008814  Val: 0.006660\n",
      "Epoch 27/150  Train: 0.008546  Val: 0.006639\n",
      "Epoch 28/150  Train: 0.008437  Val: 0.006640\n",
      "Epoch 29/150  Train: 0.008328  Val: 0.009091\n",
      "Epoch 30/150  Train: 0.008106  Val: 0.007152\n",
      "Epoch 31/150  Train: 0.007956  Val: 0.008531\n",
      "Epoch 32/150  Train: 0.007849  Val: 0.006411\n",
      "Epoch 33/150  Train: 0.007784  Val: 0.010340\n",
      "Epoch 34/150  Train: 0.007623  Val: 0.008353\n",
      "Epoch 35/150  Train: 0.007444  Val: 0.008597\n",
      "Epoch 36/150  Train: 0.007370  Val: 0.005589\n",
      "Epoch 37/150  Train: 0.007249  Val: 0.005761\n",
      "Epoch 38/150  Train: 0.007173  Val: 0.011318\n",
      "Epoch 39/150  Train: 0.007074  Val: 0.005367\n",
      "Epoch 40/150  Train: 0.006939  Val: 0.005134\n",
      "Epoch 41/150  Train: 0.007651  Val: 0.006207\n",
      "Epoch 42/150  Train: 0.006745  Val: 0.008903\n",
      "Epoch 43/150  Train: 0.006781  Val: 0.006185\n",
      "Epoch 44/150  Train: 0.006750  Val: 0.007318\n",
      "Epoch 45/150  Train: 0.006695  Val: 0.005528\n",
      "Epoch 46/150  Train: 0.008329  Val: 0.006604\n",
      "Epoch 47/150  Train: 0.006863  Val: 0.005584\n",
      "Epoch 48/150  Train: 0.006682  Val: 0.005606\n",
      "Epoch 49/150  Train: 0.006475  Val: 0.009621\n",
      "Epoch 50/150  Train: 0.006476  Val: 0.004822\n",
      "Epoch 51/150  Train: 0.006414  Val: 0.005484\n",
      "Epoch 52/150  Train: 0.006287  Val: 0.004929\n",
      "Epoch 53/150  Train: 0.006256  Val: 0.007867\n",
      "Epoch 54/150  Train: 0.006152  Val: 0.005401\n",
      "Epoch 55/150  Train: 0.006117  Val: 0.007707\n",
      "Epoch 56/150  Train: 0.006098  Val: 0.004493\n",
      "Epoch 57/150  Train: 0.006014  Val: 0.007945\n",
      "Epoch 58/150  Train: 0.005976  Val: 0.005850\n",
      "Epoch 59/150  Train: 0.006039  Val: 0.004652\n",
      "Epoch 60/150  Train: 0.005955  Val: 0.005653\n",
      "Epoch 61/150  Train: 0.005861  Val: 0.004658\n",
      "Epoch 62/150  Train: 0.005738  Val: 0.004578\n",
      "Epoch 63/150  Train: 0.005953  Val: 0.006018\n",
      "Epoch 64/150  Train: 0.005760  Val: 0.004664\n",
      "Epoch 65/150  Train: 0.005741  Val: 0.004611\n",
      "Epoch 66/150  Train: 0.005620  Val: 0.004700\n",
      "Epoch 67/150  Train: 0.005692  Val: 0.008334\n",
      "Epoch 68/150  Train: 0.005571  Val: 0.005284\n",
      "Epoch 69/150  Train: 0.005586  Val: 0.004311\n",
      "Epoch 70/150  Train: 0.005503  Val: 0.004268\n",
      "Epoch 71/150  Train: 0.005576  Val: 0.004557\n",
      "Epoch 72/150  Train: 0.005469  Val: 0.005232\n",
      "Epoch 73/150  Train: 0.005362  Val: 0.006393\n",
      "Epoch 74/150  Train: 0.005381  Val: 0.004623\n",
      "Epoch 75/150  Train: 0.005442  Val: 0.005733\n",
      "Epoch 76/150  Train: 0.005342  Val: 0.004314\n",
      "Epoch 77/150  Train: 0.006478  Val: 0.007015\n",
      "Epoch 78/150  Train: 0.005845  Val: 0.004888\n",
      "Epoch 79/150  Train: 0.005432  Val: 0.004176\n",
      "Epoch 80/150  Train: 0.005339  Val: 0.004075\n",
      "Epoch 81/150  Train: 0.005222  Val: 0.004943\n",
      "Epoch 82/150  Train: 0.005165  Val: 0.004041\n",
      "Epoch 83/150  Train: 0.005181  Val: 0.004109\n",
      "Epoch 84/150  Train: 0.005128  Val: 0.004119\n",
      "Epoch 85/150  Train: 0.005111  Val: 0.003999\n",
      "Epoch 86/150  Train: 0.005075  Val: 0.005150\n",
      "Epoch 87/150  Train: 0.005085  Val: 0.005313\n",
      "Epoch 88/150  Train: 0.005088  Val: 0.005223\n",
      "Epoch 89/150  Train: 0.005029  Val: 0.005338\n",
      "Epoch 90/150  Train: 0.005002  Val: 0.004476\n",
      "Epoch 91/150  Train: 0.004961  Val: 0.004630\n",
      "Epoch 92/150  Train: 0.005050  Val: 0.007981\n",
      "Epoch 93/150  Train: 0.004958  Val: 0.004035\n",
      "Epoch 94/150  Train: 0.004968  Val: 0.004746\n",
      "Epoch 95/150  Train: 0.004935  Val: 0.005076\n",
      "Epoch 96/150  Train: 0.004834  Val: 0.003760\n",
      "Epoch 97/150  Train: 0.004900  Val: 0.004393\n",
      "Epoch 98/150  Train: 0.004798  Val: 0.005850\n",
      "Epoch 99/150  Train: 0.004849  Val: 0.003995\n",
      "Epoch 100/150  Train: 0.004802  Val: 0.004026\n",
      "Epoch 101/150  Train: 0.004819  Val: 0.003789\n",
      "Epoch 102/150  Train: 0.004851  Val: 0.005342\n",
      "Epoch 103/150  Train: 0.004724  Val: 0.004226\n",
      "Epoch 104/150  Train: 0.004740  Val: 0.004049\n",
      "Epoch 105/150  Train: 0.004744  Val: 0.003903\n",
      "Epoch 106/150  Train: 0.004752  Val: 0.003647\n",
      "Epoch 107/150  Train: 0.004691  Val: 0.006614\n",
      "Epoch 108/150  Train: 0.004932  Val: 0.004085\n",
      "Epoch 109/150  Train: 0.004695  Val: 0.003596\n",
      "Epoch 110/150  Train: 0.004716  Val: 0.003838\n",
      "Epoch 111/150  Train: 0.004654  Val: 0.004529\n",
      "Epoch 112/150  Train: 0.004680  Val: 0.005041\n",
      "Epoch 113/150  Train: 0.004713  Val: 0.007848\n",
      "Epoch 114/150  Train: 0.005238  Val: 0.003771\n",
      "Epoch 115/150  Train: 0.004475  Val: 0.004625\n",
      "Epoch 116/150  Train: 0.004547  Val: 0.003665\n",
      "Epoch 117/150  Train: 0.004595  Val: 0.003622\n",
      "Epoch 118/150  Train: 0.004612  Val: 0.003648\n",
      "Epoch 119/150  Train: 0.004539  Val: 0.003777\n",
      "Epoch 120/150  Train: 0.004554  Val: 0.003961\n",
      "Epoch 121/150  Train: 0.004580  Val: 0.003987\n",
      "Epoch 122/150  Train: 0.006889  Val: 0.025596\n",
      "Epoch 123/150  Train: 0.007183  Val: 0.004868\n",
      "Epoch 124/150  Train: 0.005366  Val: 0.004318\n",
      "Epoch 125/150  Train: 0.005090  Val: 0.004935\n",
      "Epoch 126/150  Train: 0.005003  Val: 0.004178\n",
      "Epoch 127/150  Train: 0.004967  Val: 0.004420\n",
      "Epoch 128/150  Train: 0.004762  Val: 0.004160\n",
      "Epoch 129/150  Train: 0.004794  Val: 0.004578\n",
      "Epoch 130/150  Train: 0.004697  Val: 0.007772\n",
      "Epoch 131/150  Train: 0.004641  Val: 0.004089\n",
      "Epoch 132/150  Train: 0.004648  Val: 0.003754\n",
      "Epoch 133/150  Train: 0.004657  Val: 0.004092\n",
      "Epoch 134/150  Train: 0.009549  Val: 0.004901\n",
      "Epoch 135/150  Train: 0.005401  Val: 0.004404\n",
      "Epoch 136/150  Train: 0.005195  Val: 0.004491\n",
      "Epoch 137/150  Train: 0.004957  Val: 0.003848\n",
      "Epoch 138/150  Train: 0.004818  Val: 0.006578\n",
      "Epoch 139/150  Train: 0.004646  Val: 0.003679\n",
      "Epoch 140/150  Train: 0.004623  Val: 0.005256\n",
      "Epoch 141/150  Train: 0.004560  Val: 0.003867\n",
      "Epoch 142/150  Train: 0.004507  Val: 0.008963\n",
      "Epoch 143/150  Train: 0.004494  Val: 0.005354\n",
      "Epoch 144/150  Train: 0.004459  Val: 0.003644\n",
      "Epoch 145/150  Train: 0.004432  Val: 0.004896\n",
      "Epoch 146/150  Train: 0.004388  Val: 0.004644\n",
      "Epoch 147/150  Train: 0.004386  Val: 0.004880\n",
      "Epoch 148/150  Train: 0.004374  Val: 0.003544\n",
      "Epoch 149/150  Train: 0.004322  Val: 0.003613\n",
      "Epoch 150/150  Train: 0.004333  Val: 0.005296\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_ver01, train_loss, eval_loss = train_model(model_ver01, train_ds, val_ds, num_epochs=num_epochs, batch_size=batch_size, lr=lr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed555a32",
   "metadata": {},
   "source": [
    "### Plot Loss & Eval Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "5c27ddad",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "# plt.semilogy(train_loss, label=\"train\")\n",
    "# plt.semilogy(eval_loss, label=\"eval\")\n",
    "plt.semilogy(train_loss, label=\"train\")\n",
    "plt.semilogy(eval_loss, label=\"eval\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "2c1b45b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_EISVAECNN(model_ver01, val_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c77cce4",
   "metadata": {},
   "source": [
    "### Plot Latent Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9b16ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-04-25 12:43:06.554\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mVAE_latent\u001b[0m:\u001b[36m18\u001b[0m - \u001b[1m[8000]/[333535]\u001b[0m\n",
      "\u001b[32m2025-04-25 12:43:06.640\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mVAE_latent\u001b[0m:\u001b[36m18\u001b[0m - \u001b[1m[16000]/[333535]\u001b[0m\n",
      "\u001b[32m2025-04-25 12:43:06.720\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mVAE_latent\u001b[0m:\u001b[36m18\u001b[0m - \u001b[1m[24000]/[333535]\u001b[0m\n",
      "\u001b[32m2025-04-25 12:43:06.799\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mVAE_latent\u001b[0m:\u001b[36m18\u001b[0m - \u001b[1m[32000]/[333535]\u001b[0m\n",
      "\u001b[32m2025-04-25 12:43:06.879\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mVAE_latent\u001b[0m:\u001b[36m18\u001b[0m - \u001b[1m[40000]/[333535]\u001b[0m\n",
      "\u001b[32m2025-04-25 12:43:06.942\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mVAE_latent\u001b[0m:\u001b[36m18\u001b[0m - \u001b[1m[48000]/[333535]\u001b[0m\n",
      "\u001b[32m2025-04-25 12:43:07.006\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mVAE_latent\u001b[0m:\u001b[36m18\u001b[0m - \u001b[1m[56000]/[333535]\u001b[0m\n",
      "\u001b[32m2025-04-25 12:43:07.070\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mVAE_latent\u001b[0m:\u001b[36m18\u001b[0m - \u001b[1m[64000]/[333535]\u001b[0m\n",
      "\u001b[32m2025-04-25 12:43:07.133\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mVAE_latent\u001b[0m:\u001b[36m18\u001b[0m - \u001b[1m[72000]/[333535]\u001b[0m\n",
      "\u001b[32m2025-04-25 12:43:07.196\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mVAE_latent\u001b[0m:\u001b[36m18\u001b[0m - \u001b[1m[80000]/[333535]\u001b[0m\n",
      "\u001b[32m2025-04-25 12:43:07.256\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mVAE_latent\u001b[0m:\u001b[36m18\u001b[0m - \u001b[1m[88000]/[333535]\u001b[0m\n",
      "\u001b[32m2025-04-25 12:43:07.309\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mVAE_latent\u001b[0m:\u001b[36m18\u001b[0m - \u001b[1m[96000]/[333535]\u001b[0m\n",
      "\u001b[32m2025-04-25 12:43:07.365\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mVAE_latent\u001b[0m:\u001b[36m18\u001b[0m - \u001b[1m[104000]/[333535]\u001b[0m\n",
      "\u001b[32m2025-04-25 12:43:07.423\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mVAE_latent\u001b[0m:\u001b[36m18\u001b[0m - \u001b[1m[112000]/[333535]\u001b[0m\n",
      "\u001b[32m2025-04-25 12:43:07.479\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mVAE_latent\u001b[0m:\u001b[36m18\u001b[0m - \u001b[1m[120000]/[333535]\u001b[0m\n",
      "\u001b[32m2025-04-25 12:43:07.536\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mVAE_latent\u001b[0m:\u001b[36m18\u001b[0m - \u001b[1m[128000]/[333535]\u001b[0m\n",
      "\u001b[32m2025-04-25 12:43:07.600\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mVAE_latent\u001b[0m:\u001b[36m18\u001b[0m - \u001b[1m[136000]/[333535]\u001b[0m\n",
      "\u001b[32m2025-04-25 12:43:07.658\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mVAE_latent\u001b[0m:\u001b[36m18\u001b[0m - \u001b[1m[144000]/[333535]\u001b[0m\n",
      "\u001b[32m2025-04-25 12:43:07.719\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mVAE_latent\u001b[0m:\u001b[36m18\u001b[0m - \u001b[1m[152000]/[333535]\u001b[0m\n",
      "\u001b[32m2025-04-25 12:43:07.777\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mVAE_latent\u001b[0m:\u001b[36m18\u001b[0m - \u001b[1m[160000]/[333535]\u001b[0m\n",
      "\u001b[32m2025-04-25 12:43:07.843\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mVAE_latent\u001b[0m:\u001b[36m18\u001b[0m - \u001b[1m[168000]/[333535]\u001b[0m\n",
      "\u001b[32m2025-04-25 12:43:07.901\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mVAE_latent\u001b[0m:\u001b[36m18\u001b[0m - \u001b[1m[176000]/[333535]\u001b[0m\n",
      "\u001b[32m2025-04-25 12:43:07.958\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mVAE_latent\u001b[0m:\u001b[36m18\u001b[0m - \u001b[1m[184000]/[333535]\u001b[0m\n",
      "\u001b[32m2025-04-25 12:43:08.019\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mVAE_latent\u001b[0m:\u001b[36m18\u001b[0m - \u001b[1m[192000]/[333535]\u001b[0m\n",
      "\u001b[32m2025-04-25 12:43:08.083\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mVAE_latent\u001b[0m:\u001b[36m18\u001b[0m - \u001b[1m[200000]/[333535]\u001b[0m\n",
      "\u001b[32m2025-04-25 12:43:08.152\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mVAE_latent\u001b[0m:\u001b[36m18\u001b[0m - \u001b[1m[208000]/[333535]\u001b[0m\n",
      "\u001b[32m2025-04-25 12:43:08.212\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mVAE_latent\u001b[0m:\u001b[36m18\u001b[0m - \u001b[1m[216000]/[333535]\u001b[0m\n",
      "\u001b[32m2025-04-25 12:43:08.267\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mVAE_latent\u001b[0m:\u001b[36m18\u001b[0m - \u001b[1m[224000]/[333535]\u001b[0m\n",
      "\u001b[32m2025-04-25 12:43:08.324\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mVAE_latent\u001b[0m:\u001b[36m18\u001b[0m - \u001b[1m[232000]/[333535]\u001b[0m\n",
      "\u001b[32m2025-04-25 12:43:08.390\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mVAE_latent\u001b[0m:\u001b[36m18\u001b[0m - \u001b[1m[240000]/[333535]\u001b[0m\n",
      "\u001b[32m2025-04-25 12:43:08.451\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mVAE_latent\u001b[0m:\u001b[36m18\u001b[0m - \u001b[1m[248000]/[333535]\u001b[0m\n",
      "\u001b[32m2025-04-25 12:43:08.513\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mVAE_latent\u001b[0m:\u001b[36m18\u001b[0m - \u001b[1m[256000]/[333535]\u001b[0m\n",
      "\u001b[32m2025-04-25 12:43:08.571\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mVAE_latent\u001b[0m:\u001b[36m18\u001b[0m - \u001b[1m[264000]/[333535]\u001b[0m\n",
      "\u001b[32m2025-04-25 12:43:08.628\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mVAE_latent\u001b[0m:\u001b[36m18\u001b[0m - \u001b[1m[272000]/[333535]\u001b[0m\n",
      "\u001b[32m2025-04-25 12:43:08.684\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mVAE_latent\u001b[0m:\u001b[36m18\u001b[0m - \u001b[1m[280000]/[333535]\u001b[0m\n",
      "\u001b[32m2025-04-25 12:43:08.746\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mVAE_latent\u001b[0m:\u001b[36m18\u001b[0m - \u001b[1m[288000]/[333535]\u001b[0m\n",
      "\u001b[32m2025-04-25 12:43:08.803\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mVAE_latent\u001b[0m:\u001b[36m18\u001b[0m - \u001b[1m[296000]/[333535]\u001b[0m\n",
      "\u001b[32m2025-04-25 12:43:08.868\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mVAE_latent\u001b[0m:\u001b[36m18\u001b[0m - \u001b[1m[304000]/[333535]\u001b[0m\n",
      "\u001b[32m2025-04-25 12:43:08.923\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mVAE_latent\u001b[0m:\u001b[36m18\u001b[0m - \u001b[1m[312000]/[333535]\u001b[0m\n",
      "\u001b[32m2025-04-25 12:43:08.982\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mVAE_latent\u001b[0m:\u001b[36m18\u001b[0m - \u001b[1m[320000]/[333535]\u001b[0m\n",
      "\u001b[32m2025-04-25 12:43:09.043\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mVAE_latent\u001b[0m:\u001b[36m18\u001b[0m - \u001b[1m[328000]/[333535]\u001b[0m\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "latent_expr, _ = VAE_latent(model_ver01, all_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8d201c",
   "metadata": {},
   "source": [
    "### Plot Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "83038e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_layers = get_all_conv1d_layers(model_ver01, layer_type=nn.Conv1d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "959e3507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder.conv.0 Conv1d(2, 16, kernel_size=(13,), stride=(1,))\n",
      "encoder.conv.2 Conv1d(16, 32, kernel_size=(13,), stride=(1,))\n",
      "encoder.conv.4 Conv1d(32, 64, kernel_size=(13,), stride=(1,))\n",
      "decoder.deconv.3 Conv1d(8, 2, kernel_size=(13,), stride=(1,), padding=(6,))\n"
     ]
    }
   ],
   "source": [
    "for _layer in conv_layers:\n",
    "    layer_name, layer = _layer\n",
    "    print(layer_name, layer)\n",
    "    # visualize_conv1d_kernels(layer)\n",
    "    # W = layer.weight.data.cpu().numpy()  # [out_ch, in_ch, k_size]\n",
    "    # print(W.shape)\n",
    "    kernel_dimensionality(layer)\n",
    "    # visualize_conv1d_kernel_importance(layer, layer_name=layer_name, mode='l1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00ff714",
   "metadata": {},
   "source": [
    "### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "b6035e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    eis2vec_save_path = \"D:/Baihm/EISNN/PredictionModel/model/Convx2_z_ConvTx1_Convx1.pt\"\n",
    "    torch.save(model_ver01.state_dict(), eis2vec_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971a1c3e",
   "metadata": {},
   "source": [
    "# Prediction Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651c9b53",
   "metadata": {},
   "source": [
    "## Ver01 - LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ba6d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ── 1. Dataset & Collate ─────────────────────────────────────────────\n",
    "\n",
    "\n",
    "def collate_seq(batch):\n",
    "    \"\"\"\n",
    "    batch: list of [T_i, C, D]\n",
    "    returns:\n",
    "      seqs: [B, T_max, C, D],\n",
    "      lengths: [B]\n",
    "    \"\"\"\n",
    "    lengths = torch.tensor([s.shape[0] for s in batch], dtype=torch.long)\n",
    "    B = len(batch)\n",
    "    C, D = batch[0].shape[1:]\n",
    "    T_max = lengths.max().item()\n",
    "    seqs = torch.zeros(B, T_max, C, D)\n",
    "    for i, s in enumerate(batch):\n",
    "        seqs[i, :s.shape[0]] = s\n",
    "    return seqs, lengths\n",
    "\n",
    "# ── 2. LSTM 迭代预测器 ─────────────────────────────────────────────\n",
    "\n",
    "class LatentForecaster(nn.Module):\n",
    "    def __init__(self, z_dim, hidden_dim, num_layers=1):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size=z_dim, hidden_size=hidden_dim,\n",
    "                            num_layers=num_layers, batch_first=True)\n",
    "        self.head = nn.Linear(hidden_dim, z_dim)\n",
    "\n",
    "    def forward(self, z_seq, lengths):\n",
    "        \"\"\"\n",
    "        z_seq: [B, T, z_dim], lengths: [B]\n",
    "        returns:\n",
    "          pred_seq: [B, T-1, z_dim]\n",
    "        \"\"\"\n",
    "        # we predict one-step ahead for each t: input z[:, :-1], target z[:,1:]\n",
    "        z_in = z_seq[:, :-1]      # [B, T-1, z_dim]\n",
    "        packed = pack_padded_sequence(z_in, (lengths-1).cpu(), batch_first=True, enforce_sorted=False)\n",
    "        packed_out, _ = self.lstm(packed)\n",
    "        out, _ = pad_packed_sequence(packed_out, batch_first=True)  # [B, T-1, hidden]\n",
    "        z_pred = self.head(out)   # [B, T-1, z_dim]\n",
    "        return z_pred\n",
    "\n",
    "    def predict(self, z_init, steps):\n",
    "        \"\"\"\n",
    "        Autoregressive predict for inference.\n",
    "        z_init: [B, L, z_dim]  (use last few steps, e.g. L=3)\n",
    "        returns:\n",
    "          preds: [B, steps, z_dim]\n",
    "        \"\"\"\n",
    "        B, L, z_dim = z_init.shape\n",
    "        device = z_init.device\n",
    "        # run encoder LSTM over entire z_init\n",
    "        out, (h, c) = self.lstm(z_init)      # h: [num_layers, B, hidden]\n",
    "        inp = z_init[:, -1, :].unsqueeze(1)  # start with last latent\n",
    "        preds = []\n",
    "        for _ in range(steps):\n",
    "            out_step, (h, c) = self.lstm(inp, (h, c))  # out_step: [B,1,hidden]\n",
    "            z_next = self.head(out_step.squeeze(1))   # [B, z_dim]\n",
    "            preds.append(z_next.unsqueeze(1))\n",
    "            inp = z_next.unsqueeze(1)\n",
    "        return torch.cat(preds, dim=1)  # [B, steps, z_dim]\n",
    "\n",
    "# ── 3. 统一训练框架 ──────────────────────────────────────────────\n",
    "\n",
    "def train_forecasting(eis2vec_model, seq_model,\n",
    "                      seq_dataloader, num_epochs,\n",
    "                      optimizer, criterion,\n",
    "                      device='cuda',\n",
    "                      freeze_eis2vec=False,\n",
    "                      freeze_seq=False):\n",
    "    \"\"\"\n",
    "    eis2vec_model: instance of Curve2VecVAE_Ver01\n",
    "    seq_model: instance of LatentForecaster\n",
    "    seq_dataloader: DataLoader over EISDataset_SEQ\n",
    "    criterion: e.g. nn.MSELoss(reduction='none')  # we'll mask\n",
    "    freeze flags: whether to freeze eis2vec or seq_model\n",
    "    \"\"\"\n",
    "    eis2vec_model.to(device)\n",
    "    seq_model.to(device)\n",
    "\n",
    "    # 冻结模块\n",
    "    def set_requires_grad(model, req):\n",
    "        for p in model.parameters():\n",
    "            p.requires_grad = req\n",
    "    set_requires_grad(eis2vec_model, not freeze_eis2vec)\n",
    "    set_requires_grad(seq_model, not freeze_seq)\n",
    "\n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        eis2vec_model.eval()  # we only train seq_model by default, EIS2Vec frozen or not\n",
    "        if not freeze_seq:\n",
    "            seq_model.train()\n",
    "        total_loss = 0.0\n",
    "        total_count = 0\n",
    "\n",
    "        for seqs, lengths in seq_dataloader:\n",
    "            # seqs: [B, T, C, D]\n",
    "            seqs = seqs.to(device)\n",
    "            lengths = lengths.to(device)\n",
    "\n",
    "            B, T, C, D = seqs.shape\n",
    "            # 1) 把 EIS sequence 编码到 latent sequence\n",
    "            #    flatten batch*time → [B*T, C, D]\n",
    "            flat = seqs.view(-1, C, D)\n",
    "            with torch.set_grad_enabled(not freeze_eis2vec):\n",
    "                mu, lv = eis2vec_model.encoder(flat)\n",
    "                # 可选: use mu 或 reparam\n",
    "                z_flat = mu\n",
    "            z_seq = z_flat.view(B, T, -1)  # [B, T, z_dim]\n",
    "\n",
    "            # 2) 用 seq_model 预测 next-step latent\n",
    "            z_pred = seq_model(z_seq, lengths)  # [B, T-1, z_dim]\n",
    "\n",
    "            # 3) 计算 seq loss，只对有效位置算\n",
    "            z_target = z_seq[:, 1:, :]  # [B, T-1, z_dim]\n",
    "            mask = torch.arange(T-1, device=device)[None, :] < (lengths-1)[:, None]\n",
    "            # criterion reduction='none'\n",
    "            loss_mat = criterion(z_pred, z_target)  # [B, T-1, z_dim]\n",
    "            loss_mat = loss_mat.mean(dim=-1)         # [B, T-1]\n",
    "            loss = (loss_mat * mask.float()).sum() / mask.sum()\n",
    "\n",
    "            # 4) 反向传播\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item() * mask.sum().item()\n",
    "            total_count += mask.sum().item()\n",
    "\n",
    "        avg_loss = total_loss / total_count\n",
    "        print(f\"Epoch {epoch}/{num_epochs}, Forecast MSE: {avg_loss:.6f}\")\n",
    "\n",
    "    return eis2vec_model, seq_model\n",
    "\n",
    "\n",
    "def evaluate_and_plot(eis2vec_model, seq_model, val_list,\n",
    "                      init_len=3, device='cuda',\n",
    "                      num_samples=3):\n",
    "    \"\"\"\n",
    "    eis2vec_model: 已训练好并加载权重的 Curve2VecVAE_Ver01\n",
    "    seq_model: 已训练好的 LatentForecaster\n",
    "    val_list: list of torch.Tensor [T, C, D]\n",
    "    init_len: 启动预测的前几步 (<= min T-1)\n",
    "    \"\"\"\n",
    "    eis2vec_model.eval().to(device)\n",
    "    seq_model.eval().to(device)\n",
    "    mse = nn.MSELoss()\n",
    "\n",
    "    total_loss = 0.\n",
    "    total_count = 0\n",
    "\n",
    "    # 随机选样本用于可视化\n",
    "    vis_indices = random.sample(range(len(val_list)), min(num_samples, len(val_list)))\n",
    "\n",
    "    # 存储可视化数据\n",
    "    vis_data = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, seq in enumerate(val_list):\n",
    "            seq = seq.to(device)  # [T, C, D]\n",
    "            T, C, D = seq.shape\n",
    "            # 1) 编码 entire sequence\n",
    "            flat = seq.view(-1, C, D)               # [T, C, D]\n",
    "            mu, lv = eis2vec_model.encoder(flat)\n",
    "            z_seq = mu.view(1, T, -1)               # [1, T, z_dim]\n",
    "\n",
    "            # 2) 预测后续 (T - init_len) 步\n",
    "            z_init = z_seq[:, :init_len, :]         # [1, init_len, z_dim]\n",
    "            pred_len = T - init_len\n",
    "            z_pred = seq_model.predict(z_init, pred_len)  # [1, pred_len, z_dim]\n",
    "\n",
    "            # 3) 解码成 EIS 曲线\n",
    "            z_pred_flat = z_pred.view(-1, z_pred.size(-1))  # [pred_len, z_dim]\n",
    "            x_pred_flat = eis2vec_model.decoder(z_pred_flat)  # [pred_len, C, D]\n",
    "            x_pred = x_pred_flat.view(1, pred_len, C, D)      # [1, pred_len, C, D]\n",
    "\n",
    "            # 4) 计算 loss（真实 seq[:, init_len:] vs x_pred）\n",
    "            target = seq[init_len:]                          # [pred_len, C, D]\n",
    "            loss = mse(x_pred_flat, target.view(-1, C, D))\n",
    "            total_loss += loss.item() * pred_len\n",
    "            total_count += pred_len\n",
    "\n",
    "            # 收集可视化\n",
    "            if idx in vis_indices:\n",
    "                vis_data.append({\n",
    "                    'real': target.cpu().numpy(),          # [pred_len, C, D]\n",
    "                    'pred': x_pred.cpu().numpy(),          # [1, pred_len, C, D]\n",
    "                    'idx': idx\n",
    "                })\n",
    "\n",
    "    avg_mse = total_loss / total_count\n",
    "    print(f\"Validation MSE over {len(val_list)} sequences: {avg_mse:.6f}\")\n",
    "\n",
    "    # —— 可视化 —— \n",
    "    for sample in vis_data:\n",
    "        real = sample['real']   # [pred_len, C, D]\n",
    "        pred = sample['pred'][0]# [pred_len, C, D]\n",
    "        pred_len, C, D = real.shape\n",
    "\n",
    "        fig, axes = plt.subplots(1, pred_len, figsize=(pred_len*3, 3),\n",
    "                                 sharex=True, sharey=True)\n",
    "        for t in range(pred_len):\n",
    "            ax = axes[t]\n",
    "            # 画实部（通道0）：\n",
    "            ax.plot(real[t, 0], label='Real', linestyle='-')\n",
    "            ax.plot(pred[t, 0], label='Pred', linestyle='--')\n",
    "            ax.set_title(f\"Step {init_len + t}\")\n",
    "            if t == 0:\n",
    "                ax.set_ylabel(\"Amplitude\")\n",
    "            if t == pred_len - 1:\n",
    "                ax.legend(loc='upper right', fontsize='small')\n",
    "        plt.suptitle(f\"Seq idx {sample['idx']} Prediction vs True (Real part)\")\n",
    "        plt.tight_layout(rect=[0,0,1,0.95])\n",
    "        plt.show()\n",
    "\n",
    "    return avg_mse\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a678c3d1",
   "metadata": {},
   "source": [
    "### Running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9fa9f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ── 4. 用法示例 ────────────────────────────────────────────────\n",
    "\n",
    "# 假设你已有：\n",
    "#   data_list_seq: list of tensors [T_i, 2, 101]\n",
    "#   eis2vec_model: 已训练好的 Curve2VecVAE_Ver01\n",
    "# 构造 Dataset, DataLoader\n",
    "dataset = EISDataset_SEQ(train_list)\n",
    "loader  = DataLoader(dataset, batch_size=16, shuffle=True, collate_fn=collate_seq)\n",
    "\n",
    "# 构造预测器\n",
    "z_dim = 16\n",
    "seq_model = LatentForecaster(z_dim=z_dim, hidden_dim=64, num_layers=1)\n",
    "\n",
    "# 优化器 & 损失\n",
    "optimizer = torch.optim.Adam(\n",
    "    list(model_ver01.encoder.parameters()) + list(seq_model.parameters()),\n",
    "    lr=1e-3\n",
    ")\n",
    "# 如果冻结 EIS2Vec 就只传 seq_model.parameters()\n",
    "# criterion MSELoss 'none' 模式\n",
    "criterion = nn.MSELoss(reduction='none')\n",
    "count_parameters(seq_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3cce421",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = []\n",
    "for i,j in enumerate(dataset):\n",
    "    if i>10:\n",
    "        break\n",
    "    a.append(j)\n",
    "    # print(i,j.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dce8b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "packed = pack_padded_sequence(z_in, (lengths-1).cpu(), batch_first=True, enforce_sorted=False)\n",
    "packed_out, _ = self.lstm(packed)\n",
    "out, _ = pad_packed_sequence(packed_out, batch_first=True)  # [B, T-1, hidden]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd078bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 训练，示例冻结 EIS2Vec、训练 Seq2Seq\n",
    "train_forecasting(\n",
    "    model_ver01,\n",
    "    seq_model,\n",
    "    loader,\n",
    "    num_epochs=200,\n",
    "    optimizer=optimizer,\n",
    "    criterion=criterion,\n",
    "    device='cuda',\n",
    "    freeze_eis2vec=True,\n",
    "    freeze_seq=False\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc46fb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_seq_ds = EISDataset_SEQ(val_list)\n",
    "avg_mse = evaluate_and_plot(\n",
    "    eis2vec_model=model_ver01,\n",
    "    seq_model=seq_model,\n",
    "    val_list=val_seq_ds,\n",
    "    init_len=3,\n",
    "    device='cuda',\n",
    "    num_samples=3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17e121b",
   "metadata": {},
   "source": [
    "## Ver02 - TCN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92658c2",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "a664e3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# —— 辅助：去除多余时间步 —— \n",
    "class Chomp1d(nn.Module):\n",
    "    def __init__(self, chomp_size):\n",
    "        super().__init__()\n",
    "        self.chomp_size = chomp_size\n",
    "    def forward(self, x):\n",
    "        # x: [B, C, L + chomp_size]\n",
    "        return x[:, :, :-self.chomp_size]  # 去掉最后 chomp_size 个时间步\n",
    "\n",
    "# —— 因果扩张卷积块 —— \n",
    "class CausalConvBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, kernel_size, dilation, dropout):\n",
    "        super().__init__()\n",
    "        pad = (kernel_size - 1) * dilation\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv1d(in_ch, out_ch, kernel_size,\n",
    "                      padding=pad, dilation=dilation),\n",
    "            Chomp1d(pad),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)  # 长度不变\n",
    "\n",
    "# —— TCN Forecaster —— \n",
    "class TCNForecaster(nn.Module):\n",
    "    def __init__(self, z_dim, num_channels, kernel_size=3, dropout=0.2):\n",
    "        \"\"\"\n",
    "        z_dim: latent dimension\n",
    "        num_channels: e.g. [32,32,32] 三层 TCN\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        for i, ch in enumerate(num_channels):\n",
    "            in_ch = z_dim if i == 0 else num_channels[i-1]\n",
    "            dilation = 2 ** i\n",
    "            layers.append(\n",
    "                CausalConvBlock(in_ch, ch, kernel_size, dilation, dropout)\n",
    "            )\n",
    "        # 最后一层 1x1 卷积映射回 z_dim\n",
    "        layers.append(nn.Conv1d(num_channels[-1], z_dim, kernel_size=1))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, z_seq):\n",
    "        # z_seq: [B, T, z_dim] → [B, z_dim, T]\n",
    "        x = z_seq.transpose(1, 2)\n",
    "        y = self.network(x)           # → [B, z_dim, T]\n",
    "        return y.transpose(1, 2)      # → [B, T, z_dim]\n",
    "\n",
    "# —— 冻结参数辅助 —— \n",
    "def set_requires_grad(model, req):\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad = req\n",
    "\n",
    "# —— 统一训练函数（保持和你之前一致） —— \n",
    "def train_tcn_forecasting(eis2vec_model,\n",
    "                          tcn_model,\n",
    "                          train_loader,\n",
    "                          val_loader,\n",
    "                          num_epochs=20,\n",
    "                          lr=1e-3,\n",
    "                          device='cuda',\n",
    "                          freeze_eis2vec=False,\n",
    "                          freeze_tcn=False):\n",
    "    eis2vec_model.to(device).eval()\n",
    "    tcn_model.to(device)\n",
    "    criterion = nn.MSELoss(reduction='none')\n",
    "\n",
    "    # 冻结模块\n",
    "    set_requires_grad(eis2vec_model.encoder, not freeze_eis2vec)\n",
    "    set_requires_grad(tcn_model,       not freeze_tcn)\n",
    "\n",
    "    # 优化器只管可训练的参数\n",
    "    params = list(tcn_model.parameters())\n",
    "    if not freeze_eis2vec:\n",
    "        params += list(eis2vec_model.encoder.parameters())\n",
    "    optimizer = optim.Adam(params, lr=lr)\n",
    "\n",
    "    train_losses, val_losses = [], []\n",
    "\n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        # —— 训练 —— \n",
    "        tcn_model.train()\n",
    "        tot_loss = cnt = 0\n",
    "        for seqs, lengths in train_loader:\n",
    "            seqs, lengths = seqs.to(device), lengths.to(device)\n",
    "            B, T, C, D = seqs.shape\n",
    "\n",
    "            # 1) EIS2Vec 编码\n",
    "            flat = seqs.view(-1, C, D)\n",
    "            with torch.set_grad_enabled(not freeze_eis2vec):\n",
    "                mu, _ = eis2vec_model.encoder(flat)\n",
    "            z_seq = mu.view(B, T, -1)  # [B, T, z_dim]\n",
    "\n",
    "            # 2) TCN 预测 full one-step\n",
    "            z_pred = tcn_model(z_seq)            # [B, T, z_dim]\n",
    "            pred   = z_pred[:, :-1]              # [B, T-1, z_dim]\n",
    "            target = z_seq  [:, 1: ]             # [B, T-1, z_dim]\n",
    "\n",
    "            # 3) mask + loss\n",
    "            mask = (torch.arange(T-1, device=device)[None, :] < (lengths-1)[:, None])\n",
    "            loss_mat = criterion(pred, target).mean(dim=-1)\n",
    "            loss = (loss_mat * mask.float()).sum() / mask.sum()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            tot_loss += loss.item() * mask.sum().item()\n",
    "            cnt      += mask.sum().item()\n",
    "\n",
    "        train_losses.append(tot_loss/cnt)\n",
    "\n",
    "        # —— 验证 —— \n",
    "        tcn_model.eval()\n",
    "        tot_loss = cnt = 0\n",
    "        with torch.no_grad():\n",
    "            for seqs, lengths in val_loader:\n",
    "                seqs, lengths = seqs.to(device), lengths.to(device)\n",
    "                B, T, C, D = seqs.shape\n",
    "\n",
    "                flat = seqs.view(-1, C, D)\n",
    "                mu, _ = eis2vec_model.encoder(flat)\n",
    "                z_seq = mu.view(B, T, -1)\n",
    "\n",
    "                z_pred = tcn_model(z_seq)\n",
    "                pred   = z_pred[:, :-1]\n",
    "                target = z_seq  [:, 1: ]\n",
    "\n",
    "                mask = (torch.arange(T-1, device=device)[None, :] < (lengths-1)[:, None])\n",
    "                loss_mat = criterion(pred, target).mean(dim=-1)\n",
    "                loss = (loss_mat * mask.float()).sum() / mask.sum()\n",
    "\n",
    "                tot_loss += loss.item() * mask.sum().item()\n",
    "                cnt      += mask.sum().item()\n",
    "\n",
    "        val_losses.append(tot_loss/cnt)\n",
    "        print(f\"Epoch {epoch}/{num_epochs}  \"\n",
    "              f\"Train MSE: {train_losses[-1]:.6f}  \"\n",
    "              f\"Val MSE:   {val_losses[-1]:.6f}\")\n",
    "\n",
    "    return train_losses, val_losses\n",
    "\n",
    "def evaluate_tcn_and_plot(eis2vec_model, tcn_model, eval_list,\n",
    "                          init_len=3, device='cuda',\n",
    "                          num_samples=5):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      eis2vec_model: 训练好的 Curve2VecVAE_Ver01（只用 encoder+decoder）\n",
    "      tcn_model:      训练好的 TCNForecaster\n",
    "      eval_list:      list of torch.Tensor, each [T, C, D]\n",
    "      init_len:       用前几步启动自回归预测\n",
    "      num_samples:    随机可视化多少条序列\n",
    "    \"\"\"\n",
    "    eis2vec_model.to(device).eval()\n",
    "    tcn_model.to(device).eval()\n",
    "    mse = nn.MSELoss()\n",
    "\n",
    "    total_loss = 0.0\n",
    "    total_count = 0\n",
    "\n",
    "    # 随机选几条用于可视化\n",
    "    vis_idxs = random.sample(range(len(eval_list)), \n",
    "                             min(num_samples, len(eval_list)))\n",
    "    vis_data = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, seq in enumerate(eval_list):\n",
    "            # seq: [T, C, D]\n",
    "            seq = seq.to(device)\n",
    "            T, C, D = seq.shape\n",
    "\n",
    "            # 1) 编码整个序列到 latent\n",
    "            flat = seq.view(-1, C, D)                   # [T, C, D]\n",
    "            mu, _ = eis2vec_model.encoder(flat)          # [T, z_dim]\n",
    "            z_seq = mu.view(1, T, -1)                    # [1, T, z_dim]\n",
    "\n",
    "            # 2) 自回归预测后续 (T - init_len) 步\n",
    "            pred_len = T - init_len\n",
    "            curr = z_seq[:, :init_len, :].clone()        # [1, init_len, z_dim]\n",
    "            preds = []\n",
    "            for _ in range(pred_len):\n",
    "                out = tcn_model(curr)                    # [1, curr_len, z_dim]\n",
    "                z_next = out[:, -1, :]                   # [1, z_dim]\n",
    "                preds.append(z_next)\n",
    "                curr = torch.cat([curr, z_next.unsqueeze(1)], dim=1)\n",
    "\n",
    "            z_pred = torch.cat(preds, dim=0)             # [pred_len, z_dim]\n",
    "\n",
    "            # 3) 解码 latent → EIS 曲线\n",
    "            x_pred_flat = eis2vec_model.decoder(z_pred)  # [pred_len, C, D]\n",
    "            x_pred = x_pred_flat.view(pred_len, C, D)    # [pred_len, C, D]\n",
    "\n",
    "            # 4) 计算 MSE（实部+虚部一起算）\n",
    "            real = seq[init_len:]                        # [pred_len, C, D]\n",
    "            loss = mse(x_pred, real)                     # 标量\n",
    "            total_loss += loss.item() * pred_len\n",
    "            total_count += pred_len\n",
    "\n",
    "            # 5) 收集可视化数据（只看实部通道）\n",
    "            if idx in vis_idxs:\n",
    "                vis_data.append({\n",
    "                    'real': real[:, 0, :].cpu().numpy(),   # [pred_len, D]\n",
    "                    'pred': x_pred[:, 0, :].cpu().numpy(), # [pred_len, D]\n",
    "                    'idx' : idx\n",
    "                })\n",
    "\n",
    "    avg_mse = total_loss / total_count\n",
    "    print(f\"Eval set average MSE: {avg_mse:.6f}\")\n",
    "\n",
    "        \n",
    "    # —— 可视化 —— \n",
    "    for sample in vis_data:\n",
    "        real = sample['real']   # [pred_len, D]\n",
    "        pred = sample['pred']   # [pred_len, D]\n",
    "        \n",
    "        pred_len, D = real.shape\n",
    "\n",
    "        # 每行最多显示 5 个图\n",
    "        max_cols = 5\n",
    "        ncols = int(min(pred_len, max_cols))\n",
    "        nrows = int(np.ceil(pred_len / max_cols))\n",
    "\n",
    "        fig, axes = plt.subplots(nrows, ncols, figsize=(ncols * 3, nrows * 3),\n",
    "                                sharex=True, sharey=True)\n",
    "\n",
    "        # 保证 axes 是一个平铺的数组\n",
    "        axes = axes.flatten() if pred_len > 1 else [axes]\n",
    "\n",
    "        for t in range(pred_len):\n",
    "            ax = axes[t]\n",
    "            ax.plot(real[t,:], label='Real', linestyle='-')\n",
    "            ax.plot(pred[t,:], label='Pred', linestyle='--')\n",
    "            ax.set_title(f\"Step {init_len + t}\")\n",
    "            # ax.legend()\n",
    "\n",
    "        # 隐藏多余的子图\n",
    "        for i in range(pred_len, len(axes)):\n",
    "            axes[i].axis('off')\n",
    "\n",
    "        plt.suptitle(f\"Seq idx {sample['idx']} Prediction vs True (Real part)\")\n",
    "        plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "        plt.show()\n",
    "\n",
    "    # —— 可视化 —— \n",
    "    # for sample in vis_data:\n",
    "    #     real = sample['real']   # [pred_len, D]\n",
    "    #     pred = sample['pred']   # [pred_len, D]\n",
    "        \n",
    "        \n",
    "    #     pred_len, D = real.shape\n",
    "\n",
    "    #     fig, axes = plt.subplots(1, pred_len, figsize=(pred_len*3, 3),\n",
    "    #                              sharex=True, sharey=True)\n",
    "    #     for t in range(pred_len):\n",
    "    #         ax = axes[t]\n",
    "    #         # 画实部（通道0）：\n",
    "    #         ax.plot(real[t,:], label='Real', linestyle='-')\n",
    "    #         ax.plot(pred[t,:], label='Pred', linestyle='--')\n",
    "    #         ax.set_title(f\"Step {init_len + t}\")\n",
    "\n",
    "    #     plt.suptitle(f\"Seq idx {sample['idx']} Prediction vs True (Real part)\")\n",
    "    #     plt.tight_layout(rect=[0,0,1,0.95])\n",
    "    #     plt.show()\n",
    "\n",
    "    # for item in vis_data:\n",
    "    #     real = item['real']   # [pred_len, D]\n",
    "    #     pred = item['pred']   # [pred_len, D]\n",
    "    #     idx  = item['idx']\n",
    "    #     pred_len, D = real.shape\n",
    "\n",
    "    #     fig, axes = plt.subplots(1, 2, figsize=(10, 4), \n",
    "    #                              sharex=True, sharey=True)\n",
    "    #     im0 = axes[0].imshow(real, aspect='auto', cmap='viridis')\n",
    "    #     axes[0].set_title(f\"Sample {idx} Real (init→T)\")\n",
    "    #     im1 = axes[1].imshow(pred, aspect='auto', cmap='viridis')\n",
    "    #     axes[1].set_title(f\"Sample {idx} Pred ({init_len}→T)\")\n",
    "    #     for ax in axes:\n",
    "    #         ax.set_xlabel(\"Freq Index\")\n",
    "    #         ax.set_ylabel(\"Time Step\")\n",
    "    #     fig.colorbar(im1, ax=axes.ravel().tolist(), shrink=0.6)\n",
    "    #     plt.tight_layout()\n",
    "    #     plt.show()\n",
    "\n",
    "    return avg_mse\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7281df0",
   "metadata": {},
   "source": [
    "### Running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "1cf741f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ===== Example Usage =====\n",
    "\n",
    "# Assuming you have:\n",
    "# - data_list_seq: list of torch.Tensor [T_i, C, D]\n",
    "# - eis2vec_model: pretrained Curve2VecVAE_Ver01\n",
    "# - definitions of EISSeqDataset & collate_seq from before\n",
    "\n",
    "# Prepare DataLoaders\n",
    "train_ds_tcn = EISDataset_SEQ(train_list)\n",
    "val_ds_tcn   = EISDataset_SEQ(val_list)\n",
    "train_loader_tcn  = DataLoader(train_ds_tcn, batch_size=16, shuffle=True,\n",
    "                           collate_fn=collate_seq)\n",
    "val_loader_tcn    = DataLoader(val_ds_tcn, batch_size=16, shuffle=False,\n",
    "                           collate_fn=collate_seq)\n",
    "\n",
    "# Instantiate TCN\n",
    "z_dim = 16\n",
    "tcn_model = TCNForecaster(z_dim=z_dim,\n",
    "                          num_channels=[32, 32, 32],\n",
    "                          kernel_size=3,\n",
    "                          dropout=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "cbc9ba01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100  Train MSE: 0.157689  Val MSE:   0.067138\n",
      "Epoch 2/100  Train MSE: 0.099325  Val MSE:   0.056973\n",
      "Epoch 3/100  Train MSE: 0.089993  Val MSE:   0.053014\n",
      "Epoch 4/100  Train MSE: 0.085545  Val MSE:   0.052763\n",
      "Epoch 5/100  Train MSE: 0.082646  Val MSE:   0.051614\n",
      "Epoch 6/100  Train MSE: 0.080332  Val MSE:   0.050302\n",
      "Epoch 7/100  Train MSE: 0.079096  Val MSE:   0.049890\n",
      "Epoch 8/100  Train MSE: 0.078180  Val MSE:   0.049420\n",
      "Epoch 9/100  Train MSE: 0.077682  Val MSE:   0.049870\n",
      "Epoch 10/100  Train MSE: 0.077307  Val MSE:   0.047972\n",
      "Epoch 11/100  Train MSE: 0.077051  Val MSE:   0.048089\n",
      "Epoch 12/100  Train MSE: 0.076701  Val MSE:   0.049274\n",
      "Epoch 13/100  Train MSE: 0.076719  Val MSE:   0.048564\n",
      "Epoch 14/100  Train MSE: 0.076591  Val MSE:   0.048796\n",
      "Epoch 15/100  Train MSE: 0.076276  Val MSE:   0.048145\n",
      "Epoch 16/100  Train MSE: 0.076296  Val MSE:   0.047130\n",
      "Epoch 17/100  Train MSE: 0.076220  Val MSE:   0.048154\n",
      "Epoch 18/100  Train MSE: 0.076174  Val MSE:   0.048750\n",
      "Epoch 19/100  Train MSE: 0.075934  Val MSE:   0.048630\n",
      "Epoch 20/100  Train MSE: 0.075989  Val MSE:   0.049237\n",
      "Epoch 21/100  Train MSE: 0.075919  Val MSE:   0.047912\n",
      "Epoch 22/100  Train MSE: 0.075783  Val MSE:   0.047714\n",
      "Epoch 23/100  Train MSE: 0.075900  Val MSE:   0.047860\n",
      "Epoch 24/100  Train MSE: 0.075772  Val MSE:   0.048297\n",
      "Epoch 25/100  Train MSE: 0.075617  Val MSE:   0.048114\n",
      "Epoch 26/100  Train MSE: 0.075701  Val MSE:   0.047140\n",
      "Epoch 27/100  Train MSE: 0.075550  Val MSE:   0.047891\n",
      "Epoch 28/100  Train MSE: 0.075523  Val MSE:   0.048255\n",
      "Epoch 29/100  Train MSE: 0.075492  Val MSE:   0.048173\n",
      "Epoch 30/100  Train MSE: 0.075526  Val MSE:   0.047583\n",
      "Epoch 31/100  Train MSE: 0.075473  Val MSE:   0.047390\n",
      "Epoch 32/100  Train MSE: 0.075397  Val MSE:   0.048039\n",
      "Epoch 33/100  Train MSE: 0.075573  Val MSE:   0.048524\n",
      "Epoch 34/100  Train MSE: 0.075433  Val MSE:   0.048486\n",
      "Epoch 35/100  Train MSE: 0.075387  Val MSE:   0.047499\n",
      "Epoch 36/100  Train MSE: 0.075383  Val MSE:   0.047741\n",
      "Epoch 37/100  Train MSE: 0.075421  Val MSE:   0.047707\n",
      "Epoch 38/100  Train MSE: 0.075430  Val MSE:   0.047948\n",
      "Epoch 39/100  Train MSE: 0.075302  Val MSE:   0.049190\n",
      "Epoch 40/100  Train MSE: 0.075340  Val MSE:   0.048019\n",
      "Epoch 41/100  Train MSE: 0.075268  Val MSE:   0.048219\n",
      "Epoch 42/100  Train MSE: 0.075217  Val MSE:   0.047739\n",
      "Epoch 43/100  Train MSE: 0.075269  Val MSE:   0.047348\n",
      "Epoch 44/100  Train MSE: 0.075326  Val MSE:   0.047746\n",
      "Epoch 45/100  Train MSE: 0.075203  Val MSE:   0.047900\n",
      "Epoch 46/100  Train MSE: 0.075058  Val MSE:   0.047110\n",
      "Epoch 47/100  Train MSE: 0.075077  Val MSE:   0.048054\n",
      "Epoch 48/100  Train MSE: 0.075089  Val MSE:   0.048662\n",
      "Epoch 49/100  Train MSE: 0.075247  Val MSE:   0.047312\n",
      "Epoch 50/100  Train MSE: 0.075133  Val MSE:   0.047693\n",
      "Epoch 51/100  Train MSE: 0.074885  Val MSE:   0.047124\n",
      "Epoch 52/100  Train MSE: 0.075150  Val MSE:   0.047260\n",
      "Epoch 53/100  Train MSE: 0.075258  Val MSE:   0.047548\n",
      "Epoch 54/100  Train MSE: 0.075259  Val MSE:   0.047750\n",
      "Epoch 55/100  Train MSE: 0.075062  Val MSE:   0.047972\n",
      "Epoch 56/100  Train MSE: 0.075024  Val MSE:   0.047211\n",
      "Epoch 57/100  Train MSE: 0.074965  Val MSE:   0.048724\n",
      "Epoch 58/100  Train MSE: 0.075075  Val MSE:   0.048069\n",
      "Epoch 59/100  Train MSE: 0.075059  Val MSE:   0.047230\n",
      "Epoch 60/100  Train MSE: 0.075082  Val MSE:   0.047927\n",
      "Epoch 61/100  Train MSE: 0.075010  Val MSE:   0.046860\n",
      "Epoch 62/100  Train MSE: 0.075010  Val MSE:   0.048197\n",
      "Epoch 63/100  Train MSE: 0.075023  Val MSE:   0.046838\n",
      "Epoch 64/100  Train MSE: 0.074879  Val MSE:   0.047369\n",
      "Epoch 65/100  Train MSE: 0.074882  Val MSE:   0.047938\n",
      "Epoch 66/100  Train MSE: 0.075051  Val MSE:   0.047908\n",
      "Epoch 67/100  Train MSE: 0.075057  Val MSE:   0.047318\n",
      "Epoch 68/100  Train MSE: 0.075032  Val MSE:   0.047056\n",
      "Epoch 69/100  Train MSE: 0.075102  Val MSE:   0.047645\n",
      "Epoch 70/100  Train MSE: 0.074849  Val MSE:   0.047078\n",
      "Epoch 71/100  Train MSE: 0.075110  Val MSE:   0.047579\n",
      "Epoch 72/100  Train MSE: 0.074893  Val MSE:   0.047892\n",
      "Epoch 73/100  Train MSE: 0.074864  Val MSE:   0.047446\n",
      "Epoch 74/100  Train MSE: 0.074960  Val MSE:   0.047058\n",
      "Epoch 75/100  Train MSE: 0.074803  Val MSE:   0.047310\n",
      "Epoch 76/100  Train MSE: 0.074826  Val MSE:   0.047346\n",
      "Epoch 77/100  Train MSE: 0.074832  Val MSE:   0.047381\n",
      "Epoch 78/100  Train MSE: 0.074971  Val MSE:   0.046845\n",
      "Epoch 79/100  Train MSE: 0.074993  Val MSE:   0.047147\n",
      "Epoch 80/100  Train MSE: 0.074772  Val MSE:   0.047965\n",
      "Epoch 81/100  Train MSE: 0.074777  Val MSE:   0.046767\n",
      "Epoch 82/100  Train MSE: 0.074857  Val MSE:   0.047655\n",
      "Epoch 83/100  Train MSE: 0.074973  Val MSE:   0.047191\n",
      "Epoch 84/100  Train MSE: 0.074941  Val MSE:   0.048047\n",
      "Epoch 85/100  Train MSE: 0.074747  Val MSE:   0.047204\n",
      "Epoch 86/100  Train MSE: 0.074870  Val MSE:   0.047417\n",
      "Epoch 87/100  Train MSE: 0.074935  Val MSE:   0.047776\n",
      "Epoch 88/100  Train MSE: 0.074824  Val MSE:   0.047225\n",
      "Epoch 89/100  Train MSE: 0.074602  Val MSE:   0.046587\n",
      "Epoch 90/100  Train MSE: 0.074679  Val MSE:   0.047328\n",
      "Epoch 91/100  Train MSE: 0.074905  Val MSE:   0.047242\n",
      "Epoch 92/100  Train MSE: 0.074837  Val MSE:   0.047378\n",
      "Epoch 93/100  Train MSE: 0.074713  Val MSE:   0.046952\n",
      "Epoch 94/100  Train MSE: 0.074831  Val MSE:   0.047127\n",
      "Epoch 95/100  Train MSE: 0.074646  Val MSE:   0.046597\n",
      "Epoch 96/100  Train MSE: 0.074921  Val MSE:   0.046942\n",
      "Epoch 97/100  Train MSE: 0.074596  Val MSE:   0.047495\n",
      "Epoch 98/100  Train MSE: 0.074731  Val MSE:   0.047823\n",
      "Epoch 99/100  Train MSE: 0.074775  Val MSE:   0.047450\n",
      "Epoch 100/100  Train MSE: 0.074803  Val MSE:   0.047398\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Train with freezing options\n",
    "train_losses, val_losses = train_tcn_forecasting(\n",
    "    eis2vec_model=model_ver01,\n",
    "    tcn_model=tcn_model,\n",
    "    train_loader=train_loader_tcn,\n",
    "    val_loader=val_loader_tcn,\n",
    "    num_epochs=100,\n",
    "    lr=1e-3,\n",
    "    device='cuda',\n",
    "    freeze_eis2vec=True,  # freeze encoder\n",
    "    freeze_tcn=False      # train TCN\n",
    ")\n",
    "\n",
    "# train_losses and val_losses now hold per-epoch MSE values for plotting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554d6ce5",
   "metadata": {},
   "source": [
    "### Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "c30c7ea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval set average MSE: 0.430405\n"
     ]
    }
   ],
   "source": [
    "val_seq_ds = EISDataset_SEQ(val_list)\n",
    "avg_mse = evaluate_tcn_and_plot(\n",
    "    eis2vec_model=model_ver01,\n",
    "    tcn_model=tcn_model,\n",
    "    eval_list=val_seq_ds,\n",
    "    init_len=3,\n",
    "    device='cuda',\n",
    "    num_samples=5\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EISNN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
