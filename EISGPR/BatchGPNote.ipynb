{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 816,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "%gui qt\n",
    "\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "from loguru import logger\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "\n",
    "import pyqtgraph as pg\n",
    "import pyqtgraph.opengl as gl\n",
    "\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "import gpytorch\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import scipy.interpolate as interp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 817,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gatherCSV(rootPath, outsuffix = 'Tracking'):\n",
    "    '''==================================================\n",
    "        Collect all EIS.csv files in the rootPath\n",
    "        Parameter: \n",
    "            rootPath: current search path\n",
    "            outsuffix: Saving path of EIS.csv files\n",
    "        Returen:\n",
    "            EISDict: a 2D-dict of EIS data\n",
    "            Storage Frame: EISDict[_sessionIndex][_channelIndex] = \"_filepath\"\n",
    "        ==================================================\n",
    "    '''\n",
    "    _filename       = None\n",
    "    _filepath       = None\n",
    "    _trackpath      = None\n",
    "    _csvpath        = None\n",
    "    _sessionIndex   = None\n",
    "    _channelIndex   = None\n",
    "    _processed      = None\n",
    "\n",
    "    EISDict = defaultdict(dict)\n",
    "\n",
    "    ## Iterate session\n",
    "    session_pattern = re.compile(r\"(.+?)_(\\d{8})_01\")\n",
    "    bank_pattern    = re.compile(r\"([1-4])\")\n",
    "    file_pattern    = re.compile(r\"EIS_ch(\\d{3})\\.csv\")\n",
    "\n",
    "    ## RootDir\n",
    "    for i in os.listdir(rootPath):\n",
    "        match_session = session_pattern.match(i)\n",
    "        ## SessionDir\n",
    "        if match_session:\n",
    "            logger.info(f\"Session Begin: {i}\")\n",
    "            _sessionIndex = match_session[2]\n",
    "            for j in os.listdir(f\"{rootPath}/{i}\"):\n",
    "                match_bank = bank_pattern.match(j)\n",
    "                ## BankDir\n",
    "                if match_bank:\n",
    "                    logger.info(f\"Bank Begin: {j}\")\n",
    "                    _trackpath = f\"{rootPath}/{i}/{j}/{outsuffix}\"\n",
    "                    if not os.path.exists(_trackpath):\n",
    "                        continue\n",
    "\n",
    "                    for k in os.listdir(f\"{rootPath}/{i}/{j}/{outsuffix}\"):\n",
    "                        match_file = file_pattern.match(k)\n",
    "                        ## File\n",
    "                        if match_file:\n",
    "                            _filename = k\n",
    "                            _filepath = f\"{rootPath}/{i}/{j}/{outsuffix}/{k}\"\n",
    "                            _channelIndex = (int(match_bank[1])-1)*32+int(match_file[1])\n",
    "                            \n",
    "                            EISDict[_sessionIndex][_channelIndex] = f\"{rootPath}/{i}/{j}/{outsuffix}/{k}\"\n",
    "                            \n",
    "    return EISDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 818,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Readout\n",
    "def readChannel(chID, fileDict):\n",
    "    '''==================================================\n",
    "        Read EIS.csv file by Channel\n",
    "        Parameter: \n",
    "            chID: channel index\n",
    "            fileDict: EISDict[_sessionIndex][_channelIndex] = \"_filepath\"\n",
    "        Returen:\n",
    "            freq: frequency\n",
    "            Zreal: real part of impedance\n",
    "            Zimag: imaginary part of impedance\n",
    "        ==================================================\n",
    "    '''\n",
    "    chData = []\n",
    "    for ssID in fileDict.keys():\n",
    "        _data   = np.loadtxt(fileDict[ssID][chID], delimiter=',')\n",
    "        _freq   = _data[:,0]\n",
    "        _Zreal  = _data[:,1] * np.cos(np.deg2rad(_data[:,2])) \n",
    "        _Zimag  = _data[:,1] * np.sin(np.deg2rad(_data[:,2])) \n",
    "        chData.append(np.stack((_freq, _Zreal, _Zimag),axis=0))\n",
    "\n",
    "    return np.stack(chData, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 819,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def EIS_recal_ver02(data, _phz_0 = None):\n",
    "    f_poi = data[0,:]\n",
    "    # Z_poi = data[1,:] * np.exp(1j*np.deg2rad(data[2,:]))\n",
    "    Z_poi = data[1,:] + 1j*data[2,:]\n",
    "    Y_poi = 1/Z_poi\n",
    "\n",
    "    Rg0 = 1.611e13\n",
    "    Cp0 = 1.4e-9\n",
    "    \n",
    "    _Rg0_rescale = 1/Rg0*np.power(f_poi,1.583)\n",
    "    _Cp0_rescale = Cp0*np.power(f_poi,0.911)\n",
    "    Y_org = Y_poi - _Rg0_rescale + 1j*_Cp0_rescale\n",
    "    # Y_org = Y_poi - _Rg0_rescale \n",
    "    # Y_org = Y_poi + 1j*_Cp0_rescale\n",
    "    # Y_org = Y_poi\n",
    "    Z_org = 1/Y_org\n",
    "\n",
    "    # Phz Calibration\n",
    "    if _phz_0 is None:\n",
    "        _phz_0 = np.loadtxt(\"./phz_Calib.txt\")\n",
    "    \n",
    "    Z_ampC = np.abs(Z_org)\n",
    "    # Z_phzC = np.angle(Z_org) - _phz_0\n",
    "    Z_phzC = np.angle(Z_org) - _phz_0\n",
    "\n",
    "    Z_rec = Z_ampC * np.exp(1j*Z_phzC)\n",
    "\n",
    "    # C = 5e-10\n",
    "    Rs0 = 100\n",
    "    Z_rec = Z_rec - Rs0\n",
    "\n",
    "\n",
    "\n",
    "    Cp0 = 5e-10\n",
    "    _Cp0_rescale = Cp0 * f_poi\n",
    "    Z_rec = 1/(1/Z_rec - 1j * _Cp0_rescale)\n",
    "\n",
    "    \n",
    "\n",
    "    # Ls0 = 1.7e-4\n",
    "    Ls0 = 5e-4\n",
    "    _Ls0_rescale = Ls0 * f_poi\n",
    "    Z_rec = Z_rec - 1j * _Ls0_rescale\n",
    "\n",
    "    # C = 5e-10\n",
    "    Rs0 = 566\n",
    "    Z_rec = Z_rec - Rs0\n",
    "    \n",
    "    return np.stack([f_poi, np.real(Z_rec), np.imag(Z_rec)], axis=1).T\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 820,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-04-04 21:42:18.457\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mSession Begin: 09290511_20241022_01\u001b[0m\n",
      "\u001b[32m2025-04-04 21:42:18.457\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 1\u001b[0m\n",
      "\u001b[32m2025-04-04 21:42:18.458\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 2\u001b[0m\n",
      "\u001b[32m2025-04-04 21:42:18.458\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 3\u001b[0m\n",
      "\u001b[32m2025-04-04 21:42:18.459\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 4\u001b[0m\n",
      "\u001b[32m2025-04-04 21:42:18.459\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mSession Begin: 09290511_20241024_01\u001b[0m\n",
      "\u001b[32m2025-04-04 21:42:18.459\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 1\u001b[0m\n",
      "\u001b[32m2025-04-04 21:42:18.460\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 2\u001b[0m\n",
      "\u001b[32m2025-04-04 21:42:18.460\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 3\u001b[0m\n",
      "\u001b[32m2025-04-04 21:42:18.460\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 4\u001b[0m\n",
      "\u001b[32m2025-04-04 21:42:18.461\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mSession Begin: 09290511_20241028_01\u001b[0m\n",
      "\u001b[32m2025-04-04 21:42:18.461\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 1\u001b[0m\n",
      "\u001b[32m2025-04-04 21:42:18.462\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 2\u001b[0m\n",
      "\u001b[32m2025-04-04 21:42:18.462\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 3\u001b[0m\n",
      "\u001b[32m2025-04-04 21:42:18.462\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 4\u001b[0m\n",
      "\u001b[32m2025-04-04 21:42:18.463\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mSession Begin: 09290511_20241029_01\u001b[0m\n",
      "\u001b[32m2025-04-04 21:42:18.463\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 1\u001b[0m\n",
      "\u001b[32m2025-04-04 21:42:18.463\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 2\u001b[0m\n",
      "\u001b[32m2025-04-04 21:42:18.464\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 3\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-04-04 21:42:18.464\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 4\u001b[0m\n",
      "\u001b[32m2025-04-04 21:42:18.465\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mSession Begin: 09290511_20241030_01\u001b[0m\n",
      "\u001b[32m2025-04-04 21:42:18.465\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 1\u001b[0m\n",
      "\u001b[32m2025-04-04 21:42:18.465\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 2\u001b[0m\n",
      "\u001b[32m2025-04-04 21:42:18.466\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 3\u001b[0m\n",
      "\u001b[32m2025-04-04 21:42:18.466\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 4\u001b[0m\n",
      "\u001b[32m2025-04-04 21:42:18.467\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mSession Begin: 09290511_20241031_01\u001b[0m\n",
      "\u001b[32m2025-04-04 21:42:18.467\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 1\u001b[0m\n",
      "\u001b[32m2025-04-04 21:42:18.467\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 2\u001b[0m\n",
      "\u001b[32m2025-04-04 21:42:18.468\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 3\u001b[0m\n",
      "\u001b[32m2025-04-04 21:42:18.468\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 4\u001b[0m\n",
      "\u001b[32m2025-04-04 21:42:18.469\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mSession Begin: 09290511_20241101_01\u001b[0m\n",
      "\u001b[32m2025-04-04 21:42:18.469\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 1\u001b[0m\n",
      "\u001b[32m2025-04-04 21:42:18.469\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 2\u001b[0m\n",
      "\u001b[32m2025-04-04 21:42:18.470\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 3\u001b[0m\n",
      "\u001b[32m2025-04-04 21:42:18.470\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 4\u001b[0m\n",
      "\u001b[32m2025-04-04 21:42:18.471\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mSession Begin: 09290511_20241103_01\u001b[0m\n",
      "\u001b[32m2025-04-04 21:42:18.471\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 1\u001b[0m\n",
      "\u001b[32m2025-04-04 21:42:18.472\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 2\u001b[0m\n",
      "\u001b[32m2025-04-04 21:42:18.472\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 3\u001b[0m\n",
      "\u001b[32m2025-04-04 21:42:18.473\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 4\u001b[0m\n",
      "\u001b[32m2025-04-04 21:42:18.473\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mSession Begin: 09290511_20241104_01\u001b[0m\n",
      "\u001b[32m2025-04-04 21:42:18.473\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 1\u001b[0m\n",
      "\u001b[32m2025-04-04 21:42:18.474\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 2\u001b[0m\n",
      "\u001b[32m2025-04-04 21:42:18.474\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 3\u001b[0m\n",
      "\u001b[32m2025-04-04 21:42:18.474\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 4\u001b[0m\n",
      "\u001b[32m2025-04-04 21:42:18.475\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mSession Begin: 09290511_20241105_01\u001b[0m\n",
      "\u001b[32m2025-04-04 21:42:18.475\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 1\u001b[0m\n",
      "\u001b[32m2025-04-04 21:42:18.475\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 2\u001b[0m\n",
      "\u001b[32m2025-04-04 21:42:18.476\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 3\u001b[0m\n",
      "\u001b[32m2025-04-04 21:42:18.476\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 4\u001b[0m\n",
      "\u001b[32m2025-04-04 21:42:18.476\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mSession Begin: 09290511_20241106_01\u001b[0m\n",
      "\u001b[32m2025-04-04 21:42:18.477\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 1\u001b[0m\n",
      "\u001b[32m2025-04-04 21:42:18.477\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 2\u001b[0m\n",
      "\u001b[32m2025-04-04 21:42:18.478\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 3\u001b[0m\n",
      "\u001b[32m2025-04-04 21:42:18.478\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 4\u001b[0m\n",
      "\u001b[32m2025-04-04 21:42:18.479\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mSession Begin: 09290511_20241107_01\u001b[0m\n",
      "\u001b[32m2025-04-04 21:42:18.479\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 1\u001b[0m\n",
      "\u001b[32m2025-04-04 21:42:18.479\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 2\u001b[0m\n",
      "\u001b[32m2025-04-04 21:42:18.480\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 3\u001b[0m\n",
      "\u001b[32m2025-04-04 21:42:18.480\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 4\u001b[0m\n",
      "\u001b[32m2025-04-04 21:42:18.481\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mSession Begin: 09290511_20241109_01\u001b[0m\n",
      "\u001b[32m2025-04-04 21:42:18.481\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 1\u001b[0m\n",
      "\u001b[32m2025-04-04 21:42:18.481\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 2\u001b[0m\n",
      "\u001b[32m2025-04-04 21:42:18.482\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 3\u001b[0m\n",
      "\u001b[32m2025-04-04 21:42:18.482\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 4\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(13, 3, 101)"
      ]
     },
     "execution_count": 820,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rootPath = \"D:/Baihm/EISNN/Dataset/01037160_归档\"\n",
    "# ch_id = 20  # Normal to Short, Same to GPR  \n",
    "# ch_id = 89  # Same to GPR  \n",
    "# ch_id = 7  # Normal Example\n",
    "\n",
    "# rootPath = \"D:/Baihm/EISNN/Dataset/05087163_归档\"\n",
    "# ch_id = 7   # one outlier\n",
    "# ch_id = 50  # No outlier but in two Phases\n",
    "# ch_id = 55  # One outlier &wired end point\n",
    "# ch_id = 114 # Open Circuit with on outpler\n",
    "\n",
    "# rootPath = \"D:/Baihm/EISNN/Archive/02067447_归档\"\n",
    "# ch_id = 68  # Short all the time\n",
    "\n",
    "# rootPath = \"D:/Baihm/EISNN/Archive/01067095_归档\"\n",
    "# ch_id = 19    # First Sample is outlier\n",
    "\n",
    "rootPath = \"D:/Baihm/EISNN/Archive/09290511_归档\"\n",
    "ch_id = 13    # Up & Down, 2 outliers\n",
    "# ch_id = 21    # Normal + 2 outlier\n",
    "# ch_id = 41    # Normal + 2 outlier - *(Hard To Tell)\n",
    "# ch_id = 79    # 3-class, What a mess\n",
    "\n",
    "# rootPath = \"D:/Baihm/EISNN/Archive/11057712_归档\"\n",
    "# ch_id = 106    # Very Good Electrode with 1 hidden outlier, and one phase shift\n",
    "\n",
    "# rootPath = \"D:\\Baihm\\EISNN\\Archive/10057084_归档\"\n",
    "# ch_id = 16    # Totaly Mess\n",
    "# ch_id = 18    # Totaly Mess\n",
    "\n",
    "# rootPath = \"D:\\Baihm\\EISNN\\Archive/11067223_归档\"\n",
    "# ch_id = 124     # Perfect with one outlier\n",
    "\n",
    "# rootPath = \"D:\\Baihm\\EISNN\\Archive/06017758_归档\"\n",
    "# ch_id = 96     # Perfect of Perfect\n",
    "\n",
    "# rootPath = \"D:\\Baihm\\EISNN\\Archive/15361101_归档\"\n",
    "# ch_id = 0     # Only One Sample - Run With Error\n",
    "\n",
    "\n",
    "# rootPath = \"D:\\Baihm\\EISNN\\Archive/11207147_归档\"\n",
    "# ch_id = 0     # Only Three Sample - Run With Error\n",
    "\n",
    "# freq_list = np.linspace(0,np.shape(chData)[2]-1,101,dtype=int)\n",
    "freq_list = np.linspace(0,5000-1,101,dtype=int, endpoint=True)\n",
    "EISDict = gatherCSV(rootPath)\n",
    "chData = readChannel(ch_id, EISDict)\n",
    "\n",
    "if False:\n",
    "    phz_calibration = np.loadtxt(\"./phz_Calib.txt\")\n",
    "    for i in range(np.shape(chData)[0]):\n",
    "        ch_eis = EIS_recal_ver02(chData[i,:,:], phz_calibration)\n",
    "        chData[i,:,:] = ch_eis\n",
    "chData = chData[:,:,freq_list]\n",
    "\n",
    "# chData = chData[:,:,91:100]\n",
    "\n",
    "\n",
    "np.shape(chData)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "from  Outlier import OutlierDetection\n",
    "\n",
    "CLEAN_FLAG = True\n",
    "if CLEAN_FLAG:\n",
    "    eis_seq, eis_cluster, eis_anomaly, leaf_anomaly = OutlierDetection.OutlierDetection(chData)\n",
    "else: \n",
    "    eis_seq = np.arange(np.shape(chData)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 822,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    fig= plt.figure(figsize=(15,8), constrained_layout=False)\n",
    "    axis = [0] * 8\n",
    "    axis[0] = fig.add_subplot(2,4,1, projection='3d')   \n",
    "    axis[1] = fig.add_subplot(2,4,2)            \n",
    "    axis[2] = fig.add_subplot(2,4,3)         \n",
    "    axis[3] = fig.add_subplot(2,4,4)      \n",
    "    axis[4] = fig.add_subplot(2,4,5, projection='3d')      \n",
    "    axis[5] = fig.add_subplot(2,4,6)         \n",
    "    axis[6] = fig.add_subplot(2,4,7)         \n",
    "    axis[7] = fig.add_subplot(2,4,8)    \n",
    "\n",
    "    init_elev = 21  # 仰角\n",
    "    init_azim = 55  # 方位角\n",
    "    axis[0].view_init(elev=init_elev, azim=init_azim)\n",
    "    axis[4].view_init(elev=init_elev, azim=init_azim)\n",
    "\n",
    "\n",
    "    num_samples = np.shape(chData)[0]\n",
    "\n",
    "    _x = np.arange(num_samples)[eis_seq]\n",
    "    _y = np.log10(chData[0,0,:]).flatten()\n",
    "    X, Y = np.meshgrid(_x, _y, indexing='ij')\n",
    "    axis[0].plot_surface(X, Y, np.log10(np.abs(chData[eis_seq,1,:]+1j*chData[eis_seq,2,:])), cmap='viridis_r', alpha=0.8)\n",
    "    axis[4].plot_surface(X, Y, np.rad2deg(np.angle(chData[eis_seq,1,:]+1j*chData[eis_seq,2,:])), cmap='viridis_r', alpha=0.8)\n",
    "\n",
    "\n",
    "\n",
    "    cmap = plt.colormaps.get_cmap('rainbow_r')\n",
    "    for i in range(len(eis_seq)):\n",
    "        _x = eis_seq[i]\n",
    "        ch_eis = chData[_x,:,:]\n",
    "        _color = cmap(_x/num_samples)\n",
    "        axis[1].loglog(ch_eis[0,:], np.abs(ch_eis[1,:]+1j*ch_eis[2,:]), color = _color, linewidth=2, label=f\"S{i:02d}\")\n",
    "        axis[5].semilogx(ch_eis[0,:], np.rad2deg(np.angle(ch_eis[1,:]+1j*ch_eis[2,:])), color = _color, linewidth=2, label=f\"S{i:02d}\")\n",
    "\n",
    "\n",
    "    cmap = plt.colormaps.get_cmap('Set1')\n",
    "    for i in range(len(eis_seq)):\n",
    "        _x = eis_seq[i]\n",
    "        ch_eis = chData[_x,:,:]\n",
    "        _color = cmap(eis_cluster[i])\n",
    "        axis[2].loglog(ch_eis[0,:], np.abs(ch_eis[1,:]+1j*ch_eis[2,:]), color = _color, linewidth=2, label=f\"{chr(ord('A')+eis_cluster[i])}\")\n",
    "        axis[6].semilogx(ch_eis[0,:], np.rad2deg(np.angle(ch_eis[1,:]+1j*ch_eis[2,:])), color = _color, linewidth=2, label=f\"{chr(ord('A')+eis_cluster[i])}\")\n",
    "\n",
    "    _legend_handle = []\n",
    "    for i in range(len(np.unique(eis_cluster))):\n",
    "        _legend_handle.append(mpatches.Patch(color = cmap(i), label = f\"{chr(ord('A')+i)}:{len(eis_cluster[eis_cluster==i])}\"))\n",
    "    axis[2].legend(handles=_legend_handle)\n",
    "\n",
    "    axis[2].sharex(axis[1])\n",
    "    axis[6].sharex(axis[5])\n",
    "\n",
    "\n",
    "    cmap = plt.colormaps.get_cmap('rainbow_r')\n",
    "    for i in range(len(eis_anomaly)):\n",
    "        _x = eis_anomaly[i]\n",
    "        ch_eis = chData[_x,:,:]\n",
    "        _color = cmap(_x/num_samples)\n",
    "        axis[3].loglog(ch_eis[0,:], np.abs(ch_eis[1,:]+1j*ch_eis[2,:]), color = _color, linewidth=2, label=f\"S{_x:02d}\")\n",
    "        axis[7].semilogx(ch_eis[0,:], np.rad2deg(np.angle(ch_eis[1,:]+1j*ch_eis[2,:])), color = _color, linewidth=2, label=f\"S{_x:02d}\")\n",
    "    axis[3].legend()\n",
    "    axis[3].sharex(axis[1])\n",
    "    axis[7].sharex(axis[5])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EIS-GP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 809,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multitask 共享一个kernel和lengthscale，这不符合我们的任务情况\n",
    "# 但是每个点都是复数，所以每个点需要MultitaskGPR(task=2, rank=2)\n",
    "# 一个一个迭代太慢了，我想把任务打包放一起迭代，这里使用BatchGPR\n",
    "from gpytorch.kernels import MultitaskKernel, RQKernel, RBFKernel, MaternKernel\n",
    "\n",
    "class BatchEISGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood, batch_size):\n",
    "        super().__init__(train_x, train_y, likelihood)\n",
    "\n",
    "        # 每个任务独立一个 Kernel\n",
    "        self.mean_module = gpytorch.means.MultitaskMean(\n",
    "            gpytorch.means.ConstantMean(batch_shape=torch.Size([batch_size])),\n",
    "            num_tasks=2\n",
    "        )\n",
    "        self.covar_module = gpytorch.kernels.MultitaskKernel(\n",
    "            # gpytorch.kernels.RQKernel(batch_shape=torch.Size([batch_size])),  # 100 个独立的 RQ Kernel\n",
    "            # gpytorch.kernels.RBFKernel(batch_shape=torch.Size([batch_size])),\n",
    "            gpytorch.kernels.MaternKernel(batch_shape=torch.Size([batch_size]), nu=0.5, lengthscale_bounds=(1e-2, 10)),\n",
    "\n",
    "            num_tasks=2,  # 复数 (实部+虚部)\n",
    "            rank=2\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultitaskMultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "\n",
    "def BatchGPTrain(x_train, y_train, x_eval, cluster_id, device, training_iter = 200, lr = 0.05):\n",
    "    batch_size = y_train.shape[0]\n",
    "    # Initialize likelihood and model\n",
    "    likelihood = gpytorch.likelihoods.MultitaskGaussianLikelihood(\n",
    "        num_tasks=2, rank =2, batch_shape=torch.Size([batch_size])).to(device)\n",
    "    model = BatchEISGPModel(x_train, y_train, likelihood, batch_size=batch_size).to(device)\n",
    "\n",
    "    # Find optimal model hyperparameters\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)  # Includes GaussianLikelihood parameters\n",
    "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "    # logger.info(f\"Training for {training_iter} iterations...\")\n",
    "    loss_inst       = []\n",
    "    length_inst     = []\n",
    "    noise_inst      = []\n",
    "    for i in range(training_iter):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(x_train)\n",
    "        loss = -mll(output, y_train).sum()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        poi_noise   = model.likelihood.noise.detach().cpu().numpy()\n",
    "        poi_length  = model.covar_module.data_covar_module.lengthscale.detach().cpu().numpy()\n",
    "        \n",
    "        loss_inst.append(loss.item())\n",
    "        noise_inst.append(poi_noise)\n",
    "        length_inst.append(poi_length)\n",
    "        if not (i+1)%100:\n",
    "            logger.info(f\"C{cluster_id} - Iter {i+1}/{training_iter}\\tLoss: {loss.item()}\")\n",
    "            \n",
    "    # logger.info(\"Model Training Finished.\")\n",
    "\n",
    "    # Make predictions\n",
    "    model.eval()\n",
    "    likelihood.eval()\n",
    "    with torch.no_grad(), gpytorch.settings.cholesky_jitter(1e-4):\n",
    "        pred = likelihood(model(x_eval))\n",
    "    # logger.info(\"Model Evaluation Finished.\")\n",
    "\n",
    "    return [pred, np.array(loss_inst), np.array(length_inst), np.array(noise_inst)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 728,
   "metadata": {},
   "outputs": [],
   "source": [
    "def piecewise_interp(chData, eis_seq, run_list, eis_cluster = None, SPEED_RATE = 1, LOG_FLAG = True):\n",
    "    # Init xy according to the datetime\n",
    "    x_day = [datetime.strptime(date, '%Y%m%d') for date in EISDict.keys()]\n",
    "    x_day = [x_day[i] for i in eis_seq]\n",
    "\n",
    "    x_train_full = np.array([(poi - x_day[0]).days for poi in x_day])\n",
    "    x_eval_full = np.linspace(0,max(x_train_full),max(x_train_full)*SPEED_RATE+1)\n",
    "\n",
    "    y_train_full = np.stack([chData[eis_seq,1,:].T,chData[eis_seq,2,:].T], axis=2)\n",
    "    y_train_full = y_train_full.take(run_list, axis=0)\n",
    "\n",
    "\n",
    "    if LOG_FLAG:\n",
    "        y_train_log = np.log(y_train_full[:,:,0] + 1j*y_train_full[:,:,1])\n",
    "        y_train_full = np.stack([y_train_log.real, y_train_log.imag], axis=2)\n",
    "\n",
    "\n",
    "    # Segmentation of clusters\n",
    "    if eis_cluster is None:\n",
    "        eis_cluster = np.zeros_like(eis_seq)\n",
    "    unique_clusters = np.unique(eis_cluster)\n",
    "    n_clusters = len(unique_clusters)\n",
    "\n",
    "    train_mask_list = []\n",
    "    eval_mask_list = []\n",
    "\n",
    "    for i in range(n_clusters):\n",
    "        # 取当前状态和下一个状态的数据\n",
    "        train_mask = (eis_cluster == unique_clusters[i])\n",
    "        if i == n_clusters - 1:\n",
    "            x_eval_end = x_eval_full.max() + 1\n",
    "        else:\n",
    "            x_eval_end = x_train_full[(eis_cluster == unique_clusters[i+1])].min()\n",
    "        # x_state = x_train[state_mask]\n",
    "        # y_state = y_train[:,state_mask]\n",
    "\n",
    "        eval_mask = (x_eval_full >= x_train_full[train_mask].min()) & (x_eval_full < x_eval_end)\n",
    "        \n",
    "        train_mask_list.append(train_mask)\n",
    "        eval_mask_list.append(eval_mask)\n",
    "    \n",
    "    return x_train_full, y_train_full, x_eval_full, n_clusters, train_mask_list, eval_mask_list\n",
    "    \n",
    "\n",
    "def GPDataLoader(x_train, y_train, x_eval, NORM_X_FLAG = True, NORM_Y_FLAG = True):\n",
    "\n",
    "    Scaler_X        = StandardScaler()\n",
    "    Scaler_Y_real   = StandardScaler()\n",
    "    Scaler_Y_imag   = StandardScaler()\n",
    "\n",
    "    if NORM_Y_FLAG:\n",
    "        y_train[:,:,0] = Scaler_Y_real.fit_transform(y_train[:,:,0].T).T\n",
    "        y_train[:,:,1] = Scaler_Y_imag.fit_transform(y_train[:,:,1].T).T\n",
    "    if NORM_X_FLAG:\n",
    "        x_train = Scaler_X.fit_transform(x_train.reshape(-1, 1)).flatten()\n",
    "        x_eval = Scaler_X.transform(x_eval.reshape(-1, 1)).flatten()\n",
    "\n",
    "    logger.info(f\"\\nx: {np.shape(x_train)} \\ny: {np.shape(y_train)} \\nx_pred{np.shape(x_eval)}\")\n",
    "\n",
    "    return x_train, y_train, x_eval, [Scaler_X, Scaler_Y_real, Scaler_Y_imag]\n",
    "\n",
    "\n",
    "def GPDataExporter(x_train, y_train, x_eval, y_eval_mean, y_eval_var, ScalerSet, NORM_X_FLAG, NORM_Y_FLAG):\n",
    "    # Export Data\n",
    "    if NORM_X_FLAG:\n",
    "        x_train = ScalerSet[0].inverse_transform(x_train.reshape(-1, 1)).flatten()\n",
    "        x_eval = ScalerSet[0].inverse_transform(x_eval.reshape(-1, 1)).flatten()\n",
    "    \n",
    "    if NORM_Y_FLAG:\n",
    "        y_train_real = ScalerSet[1].inverse_transform(y_train[:,:,0].T).T\n",
    "        y_train_imag = ScalerSet[2].inverse_transform(y_train[:,:,1].T).T\n",
    "        \n",
    "        y_eval_mean_real = ScalerSet[1].inverse_transform(y_eval_mean[:,:,0].T).T\n",
    "        y_eval_mean_imag = ScalerSet[2].inverse_transform(y_eval_mean[:,:,1].T).T\n",
    "\n",
    "        y_eval_var_real = y_eval_var[:,:,0] * ScalerSet[1].var_.reshape(-1,1)\n",
    "        y_eval_var_imag = y_eval_var[:,:,1] * ScalerSet[2].var_.reshape(-1,1)\n",
    "    else:\n",
    "        y_train_real = y_train[:,:,0]\n",
    "        y_train_imag = y_train[:,:,1]\n",
    "\n",
    "        y_eval_mean_real = y_eval_mean[:,:,0]\n",
    "        y_eval_mean_imag = y_eval_mean[:,:,1]\n",
    "\n",
    "        y_eval_var_real = y_eval_var[:,:,0]\n",
    "        y_eval_var_imag = y_eval_var[:,:,1]\n",
    "\n",
    "    y_train = np.stack([y_train_real, y_train_imag], axis=2)\n",
    "    y_eval = np.stack([y_eval_mean_real, y_eval_mean_imag], axis=2)\n",
    "    y_eval_err = np.stack([y_eval_var_real, y_eval_var_imag], axis=2)\n",
    "    \n",
    "    logger.info(f\"\\nx: {np.shape(x_train)} \\ny: {np.shape(y_train)} \\nx_pred{np.shape(x_eval)} \\ny_pred{np.shape(y_eval)} \\ny_pred{np.shape(y_eval_err)}\")\n",
    "\n",
    "    return x_train, y_train, x_eval, y_eval, y_eval_err\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 752,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-04-04 18:37:58.702\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mGPDataLoader\u001b[0m:\u001b[36m58\u001b[0m - \u001b[1m\n",
      "x: (11,) \n",
      "y: (101, 11, 2) \n",
      "x_pred(19,)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "SPEED_RATE=1\n",
    "LOG_FLAG=False\n",
    "NORM_X_FLAG=True\n",
    "NORM_Y_FLAG=True\n",
    "\n",
    "\n",
    "# run_list = range(60,80)\n",
    "run_list = range(np.shape(chData)[2])\n",
    "\n",
    "x_train_full, y_train_full, x_eval_full,  n_clusters, train_mask_list, eval_mask_list = \\\n",
    "    piecewise_interp(chData, eis_seq, run_list, \n",
    "                     eis_cluster = None, SPEED_RATE = SPEED_RATE, LOG_FLAG=LOG_FLAG)\n",
    "\n",
    "x_train = x_train_full[train_mask_list[0]]\n",
    "y_train = y_train_full[:,train_mask_list[0],:]\n",
    "x_eval = x_eval_full[eval_mask_list[0]]\n",
    "\n",
    "x_train, y_train, x_eval, ScalerSet = \\\n",
    "    GPDataLoader(x_train, y_train, x_eval, \n",
    "        NORM_X_FLAG=NORM_X_FLAG, NORM_Y_FLAG=NORM_Y_FLAG)\n",
    "\n",
    "batch_size = np.shape(y_train)[0]\n",
    "x_train_batch = x_train.reshape(1,-1,1).repeat(batch_size, axis=0)\n",
    "x_eval_batch  = x_eval.reshape(1,-1,1).repeat(batch_size, axis=0)\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "x_train_tensor = torch.from_numpy(x_train_batch).float().to(device)\n",
    "x_eval_tensor = torch.from_numpy(x_eval_batch).float().to(device)\n",
    "y_train_tensor = torch.from_numpy(y_train).float().to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 753,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-04-04 18:37:59.330\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mBatchGPTrain\u001b[0m:\u001b[36m60\u001b[0m - \u001b[1mC0 - Iter 100/1000\tLoss: 128.69215393066406\u001b[0m\n",
      "\u001b[32m2025-04-04 18:37:59.809\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mBatchGPTrain\u001b[0m:\u001b[36m60\u001b[0m - \u001b[1mC0 - Iter 200/1000\tLoss: 128.20394897460938\u001b[0m\n",
      "\u001b[32m2025-04-04 18:38:00.292\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mBatchGPTrain\u001b[0m:\u001b[36m60\u001b[0m - \u001b[1mC0 - Iter 300/1000\tLoss: 128.08999633789062\u001b[0m\n",
      "\u001b[32m2025-04-04 18:38:00.775\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mBatchGPTrain\u001b[0m:\u001b[36m60\u001b[0m - \u001b[1mC0 - Iter 400/1000\tLoss: 128.04051208496094\u001b[0m\n",
      "\u001b[32m2025-04-04 18:38:01.245\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mBatchGPTrain\u001b[0m:\u001b[36m60\u001b[0m - \u001b[1mC0 - Iter 500/1000\tLoss: 128.01341247558594\u001b[0m\n",
      "\u001b[32m2025-04-04 18:38:01.720\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mBatchGPTrain\u001b[0m:\u001b[36m60\u001b[0m - \u001b[1mC0 - Iter 600/1000\tLoss: 127.99659729003906\u001b[0m\n",
      "\u001b[32m2025-04-04 18:38:02.204\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mBatchGPTrain\u001b[0m:\u001b[36m60\u001b[0m - \u001b[1mC0 - Iter 700/1000\tLoss: 127.98529052734375\u001b[0m\n",
      "\u001b[32m2025-04-04 18:38:02.685\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mBatchGPTrain\u001b[0m:\u001b[36m60\u001b[0m - \u001b[1mC0 - Iter 800/1000\tLoss: 127.97721099853516\u001b[0m\n",
      "\u001b[32m2025-04-04 18:38:03.157\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mBatchGPTrain\u001b[0m:\u001b[36m60\u001b[0m - \u001b[1mC0 - Iter 900/1000\tLoss: 127.97122192382812\u001b[0m\n",
      "\u001b[32m2025-04-04 18:38:03.643\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mBatchGPTrain\u001b[0m:\u001b[36m60\u001b[0m - \u001b[1mC0 - Iter 1000/1000\tLoss: 127.96661376953125\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "y_eval_tensor, loss_inst, length_inst, noise_inst = \\\n",
    "    BatchGPTrain(x_train_tensor, y_train_tensor, x_eval_tensor, 0, device, training_iter=1000)\n",
    "\n",
    "\n",
    "y_eval_mean = y_eval_tensor.mean.cpu().numpy()\n",
    "y_eval_cov = y_eval_tensor.covariance_matrix.cpu().detach().numpy()\n",
    "y_eval_var = y_eval_tensor.variance.detach().cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 754,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-04-04 18:38:03.658\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mGPDataExporter\u001b[0m:\u001b[36m92\u001b[0m - \u001b[1m\n",
      "x: (11,) \n",
      "y: (101, 11, 2) \n",
      "x_pred(19,) \n",
      "y_pred(101, 19, 2) \n",
      "y_pred(101, 19, 2)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train, x_eval, y_eval, y_eval_err = \\\n",
    "        GPDataExporter(x_train, y_train, x_eval, y_eval_mean, y_eval_var, ScalerSet,\n",
    "                       NORM_X_FLAG=NORM_X_FLAG, NORM_Y_FLAG=NORM_Y_FLAG)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 755,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    fig = plt.figure()\n",
    "    ax1 = fig.add_subplot(311)\n",
    "    ax2 = fig.add_subplot(312)\n",
    "    ax3 = fig.add_subplot(313)\n",
    "    ax1.plot(loss_inst)\n",
    "    ax1.set_title(\"Loss\")\n",
    "\n",
    "\n",
    "    cmap = plt.colormaps.get_cmap('viridis_r')\n",
    "    for i in range(np.shape(length_inst)[1]):\n",
    "        ax2.plot(noise_inst[:,i].flatten(), color = cmap(i/np.shape(length_inst)[1]), linewidth=2, label=f\"LengthScale {i+1}\")\n",
    "        ax3.plot(length_inst[:,i].flatten(), color = cmap(i/np.shape(length_inst)[1]), linewidth=2, label=f\"LengthScale {i+1}\")\n",
    "    ax2.set_yscale('log')\n",
    "    ax3.set_title(\"LengthScale\")\n",
    "\n",
    "    fig.set_tight_layout(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPR Result Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 756,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    n_freq = np.shape(y_eval)[0]\n",
    "    fig = plt.figure(figsize=(8,8))\n",
    "    cmap = plt.colormaps.get_cmap('viridis_r')\n",
    "    axis0 = fig.add_subplot(2,2,1)\n",
    "    axis1 = fig.add_subplot(2,2,2)\n",
    "    axis2 = fig.add_subplot(2,2,3)\n",
    "    axis3 = fig.add_subplot(2,2,4)\n",
    "\n",
    "\n",
    "    # for i in range(n_freq):\n",
    "    # for i in range(70,80):\n",
    "    for i in range(np.shape(y_eval)[0]):\n",
    "        axis0.fill_between(x_eval, y_eval[i,:,0] - 2*np.sqrt(y_eval_err[i,:,0]), y_eval[i,:,0] + 2*np.sqrt(y_eval_err[i,:,0]), \n",
    "                        alpha=0.2, color = cmap(run_list[i]/n_freq))\n",
    "        \n",
    "        axis1.plot(x_eval, y_eval[i,:,0], color = cmap(i/n_freq))\n",
    "        axis1.plot(x_train, y_train[i,:,0], color = cmap(i/n_freq), linestyle = ' ', marker = 'o')\n",
    "\n",
    "        axis2.fill_between(x_eval, y_eval[i,:,1] - 2*np.sqrt(y_eval_err[i,:,1]), y_eval[i,:,1] + 2*np.sqrt(y_eval_err[i,:,1]), \n",
    "                        alpha=0.2, color = cmap(run_list[i]/n_freq))\n",
    "        \n",
    "        axis3.plot(x_eval, y_eval[i,:,1], color = cmap(i/n_freq))\n",
    "        axis3.plot(x_train, y_train[i,:,1], color = cmap(i/n_freq), linestyle = ' ', marker = 'o')\n",
    "\n",
    "    # axis1.sharex(axis0)\n",
    "    # axis1.sharey(axis0)\n",
    "    # axis3.sharex(axis2)\n",
    "    # axis3.sharey(axis2)\n",
    "    axis0.set_xlabel('x')\n",
    "    axis0.set_ylabel('y')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPR EIS Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 757,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    \n",
    "    n_freq = np.shape(y_eval)[0]\n",
    "    fig = plt.figure(figsize=(8,8))\n",
    "    cmap = plt.colormaps.get_cmap('rainbow_r')\n",
    "    axis0 = fig.add_subplot(2,3,1)\n",
    "    axis1 = fig.add_subplot(2,3,2)\n",
    "    axis2 = fig.add_subplot(2,3,3, projection = '3d')\n",
    "    axis3 = fig.add_subplot(2,3,4)\n",
    "    axis4 = fig.add_subplot(2,3,5)\n",
    "    axis5 = fig.add_subplot(2,3,6, projection = '3d')\n",
    "\n",
    "\n",
    "    init_elev = 30  # 仰角\n",
    "    init_azim = -40  # 方位角\n",
    "    axis2.view_init(elev=init_elev, azim=init_azim)\n",
    "    axis5.view_init(elev=init_elev, azim=init_azim)\n",
    "\n",
    "\n",
    "    new_f = chData[0,0,:].take(run_list, axis=0)\n",
    "\n",
    "    if LOG_FLAG:\n",
    "        \n",
    "        y_EIS_train =   np.exp(y_train[:,:,0] + 1j * y_train[:,:,1])\n",
    "        y_EIS_eval = np.exp(y_eval[:,:,0] + 1j * y_eval[:,:,1])\n",
    "\n",
    "        # f_plot = range(40,80)\n",
    "        f_plot = range(np.shape(new_f)[0])\n",
    "        y_EIS_train = y_EIS_train.take(f_plot, axis=0)\n",
    "        y_EIS_eval = y_EIS_eval.take(f_plot, axis=0)\n",
    "        new_f = new_f.take(f_plot, axis=0)\n",
    "\n",
    "    else:\n",
    "        y_EIS_train = y_train[:,:,0] + 1j*y_train[:,:,1]\n",
    "        y_EIS_eval = y_eval[:,:,0] + 1j*y_eval[:,:,1]\n",
    "\n",
    "        # f_plot = range(40,60)\n",
    "        f_plot = range(np.shape(new_f)[0])\n",
    "        y_EIS_train = y_EIS_train.take(f_plot, axis=0)\n",
    "        y_EIS_eval = y_EIS_eval.take(f_plot, axis=0)\n",
    "        new_f = new_f.take(f_plot, axis=0)\n",
    "\n",
    "\n",
    "\n",
    "    for i in range(np.shape(x_eval)[0]):\n",
    "        axis1.semilogx(new_f, np.log10(np.abs(y_EIS_eval[:,i])), color = cmap(i/np.shape(x_eval)[0]))\n",
    "        axis4.semilogx(new_f, np.rad2deg(np.angle(y_EIS_eval[:,i])), color = cmap(i/np.shape(x_eval)[0]))\n",
    "        \n",
    "        \n",
    "    for i in range(np.shape(x_train)[0]):\n",
    "        axis1.semilogx(new_f, np.log10(np.abs(y_EIS_train[:,i])), 'black', alpha = 0.3)\n",
    "        axis4.semilogx(new_f, np.rad2deg(np.angle(y_EIS_train[:,i])), 'black', alpha = 0.3)\n",
    "    \n",
    "\n",
    "\n",
    "    _y = np.arange(np.shape(x_eval)[0])\n",
    "    _x = np.log10(new_f).flatten()\n",
    "    X, Y = np.meshgrid(_x, _y, indexing='ij')\n",
    "    axis2.plot_surface(X, Y, np.log10(np.abs(y_EIS_eval[:,:])), cmap='viridis_r', alpha=0.8)\n",
    "    axis5.plot_surface(X, Y, np.rad2deg(np.angle(y_EIS_eval[:,:])) * 180 / np.pi, cmap='viridis_r', alpha=0.8)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 758,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(101, 11)"
      ]
     },
     "execution_count": 758,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(chData[eis_seq,1,:].take(run_list, axis=1).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segment "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 810,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPEED_RATE=1\n",
    "LOG_FLAG=True\n",
    "NORM_X_FLAG=True\n",
    "NORM_Y_FLAG=True\n",
    "\n",
    "\n",
    "# run_list = range(60,80)\n",
    "run_list = range(np.shape(chData)[2])\n",
    "\n",
    "x_train_full, y_train_full, x_eval_full,  n_clusters, train_mask_list, eval_mask_list = \\\n",
    "    piecewise_interp(chData, eis_seq, run_list, \n",
    "                     eis_cluster, SPEED_RATE = SPEED_RATE, LOG_FLAG=LOG_FLAG)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 811,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-04-04 20:50:57.619\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mGPDataLoader\u001b[0m:\u001b[36m58\u001b[0m - \u001b[1m\n",
      "x: (6,) \n",
      "y: (101, 6, 2) \n",
      "x_pred(10,)\u001b[0m\n",
      "\u001b[32m2025-04-04 20:50:58.966\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mBatchGPTrain\u001b[0m:\u001b[36m61\u001b[0m - \u001b[1mC0 - Iter 100/1000\tLoss: 94.79246520996094\u001b[0m\n",
      "\u001b[32m2025-04-04 20:51:00.248\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mBatchGPTrain\u001b[0m:\u001b[36m61\u001b[0m - \u001b[1mC0 - Iter 200/1000\tLoss: 93.10476684570312\u001b[0m\n",
      "\u001b[32m2025-04-04 20:51:01.547\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mBatchGPTrain\u001b[0m:\u001b[36m61\u001b[0m - \u001b[1mC0 - Iter 300/1000\tLoss: 92.74947357177734\u001b[0m\n",
      "\u001b[32m2025-04-04 20:51:02.805\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mBatchGPTrain\u001b[0m:\u001b[36m61\u001b[0m - \u001b[1mC0 - Iter 400/1000\tLoss: 92.62405395507812\u001b[0m\n",
      "\u001b[32m2025-04-04 20:51:04.053\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mBatchGPTrain\u001b[0m:\u001b[36m61\u001b[0m - \u001b[1mC0 - Iter 500/1000\tLoss: 92.51010131835938\u001b[0m\n",
      "\u001b[32m2025-04-04 20:51:05.318\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mBatchGPTrain\u001b[0m:\u001b[36m61\u001b[0m - \u001b[1mC0 - Iter 600/1000\tLoss: 92.4442367553711\u001b[0m\n",
      "\u001b[32m2025-04-04 20:51:06.579\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mBatchGPTrain\u001b[0m:\u001b[36m61\u001b[0m - \u001b[1mC0 - Iter 700/1000\tLoss: 92.41567993164062\u001b[0m\n",
      "\u001b[32m2025-04-04 20:51:07.844\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mBatchGPTrain\u001b[0m:\u001b[36m61\u001b[0m - \u001b[1mC0 - Iter 800/1000\tLoss: 92.4105224609375\u001b[0m\n",
      "\u001b[32m2025-04-04 20:51:09.051\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mBatchGPTrain\u001b[0m:\u001b[36m61\u001b[0m - \u001b[1mC0 - Iter 900/1000\tLoss: 92.3897476196289\u001b[0m\n",
      "\u001b[32m2025-04-04 20:51:10.297\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mBatchGPTrain\u001b[0m:\u001b[36m61\u001b[0m - \u001b[1mC0 - Iter 1000/1000\tLoss: 92.38273620605469\u001b[0m\n",
      "\u001b[32m2025-04-04 20:51:10.312\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mGPDataExporter\u001b[0m:\u001b[36m92\u001b[0m - \u001b[1m\n",
      "x: (6,) \n",
      "y: (101, 6, 2) \n",
      "x_pred(10,) \n",
      "y_pred(101, 10, 2) \n",
      "y_pred(101, 10, 2)\u001b[0m\n",
      "\u001b[32m2025-04-04 20:51:10.314\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mGPDataLoader\u001b[0m:\u001b[36m58\u001b[0m - \u001b[1m\n",
      "x: (5,) \n",
      "y: (101, 5, 2) \n",
      "x_pred(9,)\u001b[0m\n",
      "\u001b[32m2025-04-04 20:51:11.571\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mBatchGPTrain\u001b[0m:\u001b[36m61\u001b[0m - \u001b[1mC1 - Iter 100/1000\tLoss: 119.68312072753906\u001b[0m\n",
      "\u001b[32m2025-04-04 20:51:12.800\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mBatchGPTrain\u001b[0m:\u001b[36m61\u001b[0m - \u001b[1mC1 - Iter 200/1000\tLoss: 118.73779296875\u001b[0m\n",
      "\u001b[32m2025-04-04 20:51:14.018\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mBatchGPTrain\u001b[0m:\u001b[36m61\u001b[0m - \u001b[1mC1 - Iter 300/1000\tLoss: 118.61933135986328\u001b[0m\n",
      "\u001b[32m2025-04-04 20:51:15.265\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mBatchGPTrain\u001b[0m:\u001b[36m61\u001b[0m - \u001b[1mC1 - Iter 400/1000\tLoss: 118.5846176147461\u001b[0m\n",
      "\u001b[32m2025-04-04 20:51:16.519\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mBatchGPTrain\u001b[0m:\u001b[36m61\u001b[0m - \u001b[1mC1 - Iter 500/1000\tLoss: 118.56652069091797\u001b[0m\n",
      "\u001b[32m2025-04-04 20:51:17.764\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mBatchGPTrain\u001b[0m:\u001b[36m61\u001b[0m - \u001b[1mC1 - Iter 600/1000\tLoss: 118.55610656738281\u001b[0m\n",
      "\u001b[32m2025-04-04 20:51:19.001\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mBatchGPTrain\u001b[0m:\u001b[36m61\u001b[0m - \u001b[1mC1 - Iter 700/1000\tLoss: 118.54780578613281\u001b[0m\n",
      "\u001b[32m2025-04-04 20:51:20.224\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mBatchGPTrain\u001b[0m:\u001b[36m61\u001b[0m - \u001b[1mC1 - Iter 800/1000\tLoss: 118.5428466796875\u001b[0m\n",
      "\u001b[32m2025-04-04 20:51:21.490\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mBatchGPTrain\u001b[0m:\u001b[36m61\u001b[0m - \u001b[1mC1 - Iter 900/1000\tLoss: 118.53874206542969\u001b[0m\n",
      "\u001b[32m2025-04-04 20:51:22.740\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mBatchGPTrain\u001b[0m:\u001b[36m61\u001b[0m - \u001b[1mC1 - Iter 1000/1000\tLoss: 118.5366439819336\u001b[0m\n",
      "\u001b[32m2025-04-04 20:51:22.755\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mGPDataExporter\u001b[0m:\u001b[36m92\u001b[0m - \u001b[1m\n",
      "x: (5,) \n",
      "y: (101, 5, 2) \n",
      "x_pred(9,) \n",
      "y_pred(101, 9, 2) \n",
      "y_pred(101, 9, 2)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "training_iter = 1000\n",
    "\n",
    "\n",
    "y_eval_full = np.zeros((np.shape(y_train_full)[0], np.shape(x_eval_full)[0], 2))\n",
    "y_eval_err_full = np.zeros((np.shape(y_train_full)[0], np.shape(x_eval_full)[0], 2))\n",
    "\n",
    "loss_list = []\n",
    "noise_list = []\n",
    "length_list = []\n",
    "\n",
    "for i in range(n_clusters):\n",
    "# for i in [1]:\n",
    "\n",
    "    x_train = x_train_full[train_mask_list[i]]\n",
    "    y_train = y_train_full[:,train_mask_list[i],:]\n",
    "    x_eval = x_eval_full[eval_mask_list[i]]\n",
    "\n",
    "    x_train, y_train, x_eval, ScalerSet = \\\n",
    "        GPDataLoader(x_train, y_train, x_eval, \n",
    "            NORM_X_FLAG=NORM_X_FLAG, NORM_Y_FLAG=NORM_Y_FLAG)\n",
    "\n",
    "    batch_size = np.shape(y_train)[0]\n",
    "    x_train_batch = x_train.reshape(1,-1,1).repeat(batch_size, axis=0)\n",
    "    x_eval_batch  = x_eval.reshape(1,-1,1).repeat(batch_size, axis=0)\n",
    "\n",
    "\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    x_train_tensor = torch.from_numpy(x_train_batch).float().to(device)\n",
    "    x_eval_tensor = torch.from_numpy(x_eval_batch).float().to(device)\n",
    "    y_train_tensor = torch.from_numpy(y_train).float().to(device)\n",
    "\n",
    "    y_eval_tensor, loss_inst, length_inst, noise_inst = \\\n",
    "    BatchGPTrain(x_train_tensor, y_train_tensor, x_eval_tensor, i, device, training_iter=training_iter)\n",
    "\n",
    "    loss_list.append(loss_inst)\n",
    "    noise_list.append(noise_inst)\n",
    "    length_list.append(length_inst)\n",
    "\n",
    "    y_eval_mean = y_eval_tensor.mean.cpu().numpy()\n",
    "    y_eval_cov = y_eval_tensor.covariance_matrix.cpu().detach().numpy()\n",
    "    y_eval_var = y_eval_tensor.variance.detach().cpu().numpy()\n",
    "\n",
    "\n",
    "    x_train, y_train, x_eval, y_eval, y_eval_err = \\\n",
    "            GPDataExporter(x_train, y_train, x_eval, y_eval_mean, y_eval_var, ScalerSet,\n",
    "                        NORM_X_FLAG=NORM_X_FLAG, NORM_Y_FLAG=NORM_Y_FLAG)\n",
    "        \n",
    "    y_eval_full[:,eval_mask_list[i],:] = y_eval\n",
    "    y_eval_err_full[:,eval_mask_list[i],:] = y_eval_err\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 812,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    fig, axis = plt.subplots(1+2*n_clusters,1,figsize=(6,6))\n",
    "\n",
    "\n",
    "    for j in range(n_clusters):\n",
    "        loss_inst = loss_list[j]\n",
    "        noise_inst = noise_list[j]\n",
    "        length_inst = length_list[j]\n",
    "\n",
    "        axis[0].plot(loss_inst, label = f\"Cluster {j}\")\n",
    "        axis[0].set_title(\"Loss\")\n",
    "\n",
    "\n",
    "        cmap = plt.colormaps.get_cmap('viridis_r')\n",
    "        for i in range(np.shape(length_inst)[1]):\n",
    "        # for i in range(60,70):\n",
    "        # for i in range(0,20):\n",
    "            axis[1+2*j].plot(noise_inst[:,i].flatten(), color = cmap(i/np.shape(length_inst)[1]), linewidth=2, label=f\"LengthScale {i+1}\")\n",
    "            axis[1+2*j+1].plot(length_inst[:,i].flatten(), color = cmap(i/np.shape(length_inst)[1]), linewidth=2, label=f\"LengthScale {i+1}\")\n",
    "        \n",
    "        axis[1+2*j].set_yscale('log')\n",
    "        axis[1+2*j].set_title(f\"Noise C{j}\")\n",
    "        axis[1+2*j+1].set_yscale('log')\n",
    "        axis[1+2*j+1].set_title(f\"LengthScale C{j}\")\n",
    "\n",
    "    axis[0].legend(frameon=False, loc='upper right')\n",
    "    fig.set_tight_layout(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPR Result Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 813,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    \n",
    "    cluster_id = 1\n",
    "    if cluster_id is None:\n",
    "        x_train = x_train_full\n",
    "        y_train = y_train_full\n",
    "        x_eval = x_eval_full\n",
    "        y_eval = y_eval_full\n",
    "        y_eval_err = y_eval_err_full\n",
    "\n",
    "    else:\n",
    "        x_train = x_train_full[train_mask_list[cluster_id]]\n",
    "        y_train = y_train_full[:,train_mask_list[cluster_id],:]\n",
    "        x_eval = x_eval_full[eval_mask_list[cluster_id]]\n",
    "        y_eval = y_eval_full[:,eval_mask_list[cluster_id],:]\n",
    "        y_eval_err = y_eval_err_full[:,eval_mask_list[cluster_id],:]\n",
    "\n",
    "\n",
    "        Scaler_Y_real   = StandardScaler()\n",
    "        Scaler_Y_imag   = StandardScaler()\n",
    "\n",
    "        if NORM_Y_FLAG:\n",
    "            y_train[:,:,0] = Scaler_Y_real.fit_transform(y_train[:,:,0].T).T\n",
    "            y_train[:,:,1] = Scaler_Y_imag.fit_transform(y_train[:,:,1].T).T\n",
    "            y_eval[:,:,0] = Scaler_Y_real.transform(y_eval[:,:,0].T).T\n",
    "            y_eval[:,:,1] = Scaler_Y_imag.transform(y_eval[:,:,1].T).T\n",
    "            \n",
    "\n",
    "\n",
    "    n_freq = np.shape(y_eval)[0]\n",
    "    fig = plt.figure(figsize=(8,8))\n",
    "    cmap = plt.colormaps.get_cmap('viridis_r')\n",
    "    axis0 = fig.add_subplot(2,2,1)\n",
    "    axis1 = fig.add_subplot(2,2,2)\n",
    "    axis2 = fig.add_subplot(2,2,3)\n",
    "    axis3 = fig.add_subplot(2,2,4)\n",
    "\n",
    "\n",
    "    # for i in range(n_freq):\n",
    "    for i in range(70,80):\n",
    "    # for i in range(00,90):\n",
    "    # for i in range(np.shape(y_eval)[0]):\n",
    "        axis0.fill_between(x_eval, y_eval[i,:,0] - 2*np.sqrt(y_eval_err[i,:,0]), y_eval[i,:,0] + 2*np.sqrt(y_eval_err[i,:,0]), \n",
    "                        alpha=0.2, color = cmap(run_list[i]/n_freq))\n",
    "        \n",
    "        axis1.plot(x_eval, y_eval[i,:,0], color = cmap(i/n_freq))\n",
    "        axis1.plot(x_train, y_train[i,:,0], color = cmap(i/n_freq), linestyle = ' ', marker = 'o')\n",
    "\n",
    "        axis2.fill_between(x_eval, -(y_eval[i,:,1] - 2*np.sqrt(y_eval_err[i,:,1])), -(y_eval[i,:,1] + 2*np.sqrt(y_eval_err[i,:,1])), \n",
    "                        alpha=0.2, color = cmap(run_list[i]/n_freq))\n",
    "        \n",
    "        axis3.plot(x_eval, -y_eval[i,:,1], color = cmap(i/n_freq))\n",
    "        axis3.plot(x_train, -y_train[i,:,1], color = cmap(i/n_freq), linestyle = ' ', marker = 'o')\n",
    "    if not LOG_FLAG:\n",
    "        axis0.set_yscale('log')\n",
    "        axis1.set_yscale('log')\n",
    "        axis2.set_yscale('log')\n",
    "        axis3.set_yscale('log')\n",
    "\n",
    "    axis1.sharey(axis0)\n",
    "    axis3.sharey(axis2)\n",
    "    axis0.set_xlabel('x')\n",
    "    axis0.set_ylabel('y')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPR EIS Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 814,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:   \n",
    "    cluster_id = None\n",
    "    if cluster_id is None:\n",
    "        x_train = x_train_full\n",
    "        y_train = y_train_full\n",
    "        x_eval = x_eval_full\n",
    "        y_eval = y_eval_full\n",
    "        y_eval_err = y_eval_err_full\n",
    "\n",
    "    else:\n",
    "        x_train = x_train_full[train_mask_list[cluster_id]]\n",
    "        y_train = y_train_full[:,train_mask_list[cluster_id],:]\n",
    "        x_eval = x_eval_full[eval_mask_list[cluster_id]]\n",
    "        y_eval = y_eval_full[:,eval_mask_list[cluster_id],:]\n",
    "        y_eval_err = y_eval_err_full[:,eval_mask_list[cluster_id],:]\n",
    "\n",
    "\n",
    "        # Scaler_Y_real   = StandardScaler()\n",
    "        # Scaler_Y_imag   = StandardScaler()\n",
    "\n",
    "        # if NORM_Y_FLAG:\n",
    "        #     y_train[:,:,0] = Scaler_Y_real.fit_transform(y_train[:,:,0].T).T\n",
    "        #     y_train[:,:,1] = Scaler_Y_imag.fit_transform(y_train[:,:,1].T).T\n",
    "        #     y_eval[:,:,0] = Scaler_Y_real.transform(y_eval[:,:,0].T).T\n",
    "        #     y_eval[:,:,1] = Scaler_Y_imag.transform(y_eval[:,:,1].T).T\n",
    "            \n",
    "\n",
    "\n",
    "    n_freq = np.shape(y_eval)[0]\n",
    "    fig = plt.figure(figsize=(8,8))\n",
    "    cmap = plt.colormaps.get_cmap('rainbow_r')\n",
    "    axis0 = fig.add_subplot(2,3,1)\n",
    "    axis1 = fig.add_subplot(2,3,2)\n",
    "    axis2 = fig.add_subplot(2,3,3, projection = '3d')\n",
    "    axis3 = fig.add_subplot(2,3,4)\n",
    "    axis4 = fig.add_subplot(2,3,5)\n",
    "    axis5 = fig.add_subplot(2,3,6, projection = '3d')\n",
    "\n",
    "\n",
    "    init_elev = 30  # 仰角\n",
    "    init_azim = -40  # 方位角\n",
    "    axis2.view_init(elev=init_elev, azim=init_azim)\n",
    "    axis5.view_init(elev=init_elev, azim=init_azim)\n",
    "\n",
    "\n",
    "    new_f = chData[0,0,:].take(run_list, axis=0)\n",
    "\n",
    "    if LOG_FLAG:\n",
    "        \n",
    "        y_EIS_train =   np.exp(y_train[:,:,0] + 1j * y_train[:,:,1])\n",
    "        y_EIS_eval = np.exp(y_eval[:,:,0] + 1j * y_eval[:,:,1])\n",
    "\n",
    "        # f_plot = range(40,80)\n",
    "        f_plot = range(np.shape(new_f)[0])\n",
    "        y_EIS_train = y_EIS_train.take(f_plot, axis=0)\n",
    "        y_EIS_eval = y_EIS_eval.take(f_plot, axis=0)\n",
    "        new_f = new_f.take(f_plot, axis=0)\n",
    "\n",
    "    else:\n",
    "        y_EIS_train = y_train[:,:,0] + 1j*y_train[:,:,1]\n",
    "        y_EIS_eval = y_eval[:,:,0] + 1j*y_eval[:,:,1]\n",
    "\n",
    "        # f_plot = range(40,60)\n",
    "        f_plot = range(np.shape(new_f)[0])\n",
    "        y_EIS_train = y_EIS_train.take(f_plot, axis=0)\n",
    "        y_EIS_eval = y_EIS_eval.take(f_plot, axis=0)\n",
    "        new_f = new_f.take(f_plot, axis=0)\n",
    "\n",
    "\n",
    "\n",
    "    for i in range(np.shape(x_eval)[0]):\n",
    "        \n",
    "        # axis0.semilogx(new_f, np.log10(np.real(y_EIS_eval[:,i])), color = cmap(i/np.shape(x_eval)[0]))\n",
    "        # axis3.semilogx(new_f, np.log10(-np.imag(y_EIS_eval[:,i])), color = cmap(i/np.shape(x_eval)[0]))\n",
    "        axis0.loglog(new_f, (np.real(y_EIS_eval[:,i])), color = cmap(i/np.shape(x_eval)[0]))\n",
    "        axis3.loglog(new_f, (-np.imag(y_EIS_eval[:,i])), color = cmap(i/np.shape(x_eval)[0]))\n",
    "        axis1.semilogx(new_f, np.log10(np.abs(y_EIS_eval[:,i])), color = cmap(i/np.shape(x_eval)[0]))\n",
    "        axis4.semilogx(new_f, np.rad2deg(np.angle(y_EIS_eval[:,i])), color = cmap(i/np.shape(x_eval)[0]))\n",
    "        \n",
    "        \n",
    "    for i in range(np.shape(x_train)[0]):\n",
    "        axis1.semilogx(new_f, np.log10(np.abs(y_EIS_train[:,i])), 'black', alpha = 0.3)\n",
    "        axis4.semilogx(new_f, np.rad2deg(np.angle(y_EIS_train[:,i])), 'black', alpha = 0.3)\n",
    "    \n",
    "\n",
    "\n",
    "    _y = np.arange(np.shape(x_eval)[0])\n",
    "    _x = np.log10(new_f).flatten()\n",
    "    X, Y = np.meshgrid(_x, _y, indexing='ij')\n",
    "    axis2.plot_surface(X, Y, np.log10(np.abs(y_EIS_eval[:,:])), cmap='viridis_r', alpha=0.8)\n",
    "    axis5.plot_surface(X, Y, np.rad2deg(np.angle(y_EIS_eval[:,:])) * 180 / np.pi, cmap='viridis_r', alpha=0.8)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EISNN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
