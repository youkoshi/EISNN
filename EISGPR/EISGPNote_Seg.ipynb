{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "%gui qt\n",
    "\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "from loguru import logger\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "\n",
    "import pyqtgraph as pg\n",
    "import pyqtgraph.opengl as gl\n",
    "\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "import gpytorch\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import scipy.interpolate as interp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gatherCSV(rootPath, outsuffix = 'Tracking'):\n",
    "    '''==================================================\n",
    "        Collect all EIS.csv files in the rootPath\n",
    "        Parameter: \n",
    "            rootPath: current search path\n",
    "            outsuffix: Saving path of EIS.csv files\n",
    "        Returen:\n",
    "            EISDict: a 2D-dict of EIS data\n",
    "            Storage Frame: EISDict[_sessionIndex][_channelIndex] = \"_filepath\"\n",
    "        ==================================================\n",
    "    '''\n",
    "    _filename       = None\n",
    "    _filepath       = None\n",
    "    _trackpath      = None\n",
    "    _csvpath        = None\n",
    "    _sessionIndex   = None\n",
    "    _channelIndex   = None\n",
    "    _processed      = None\n",
    "\n",
    "    EISDict = defaultdict(dict)\n",
    "\n",
    "    ## Iterate session\n",
    "    session_pattern = re.compile(r\"(.+?)_(\\d{8})_01\")\n",
    "    bank_pattern    = re.compile(r\"([1-4])\")\n",
    "    file_pattern    = re.compile(r\"EIS_ch(\\d{3})\\.csv\")\n",
    "\n",
    "    ## RootDir\n",
    "    for i in os.listdir(rootPath):\n",
    "        match_session = session_pattern.match(i)\n",
    "        ## SessionDir\n",
    "        if match_session:\n",
    "            logger.info(f\"Session Begin: {i}\")\n",
    "            _sessionIndex = match_session[2]\n",
    "            for j in os.listdir(f\"{rootPath}/{i}\"):\n",
    "                match_bank = bank_pattern.match(j)\n",
    "                ## BankDir\n",
    "                if match_bank:\n",
    "                    logger.info(f\"Bank Begin: {j}\")\n",
    "                    _trackpath = f\"{rootPath}/{i}/{j}/{outsuffix}\"\n",
    "                    if not os.path.exists(_trackpath):\n",
    "                        continue\n",
    "\n",
    "                    for k in os.listdir(f\"{rootPath}/{i}/{j}/{outsuffix}\"):\n",
    "                        match_file = file_pattern.match(k)\n",
    "                        ## File\n",
    "                        if match_file:\n",
    "                            _filename = k\n",
    "                            _filepath = f\"{rootPath}/{i}/{j}/{outsuffix}/{k}\"\n",
    "                            _channelIndex = (int(match_bank[1])-1)*32+int(match_file[1])\n",
    "                            \n",
    "                            EISDict[_sessionIndex][_channelIndex] = f\"{rootPath}/{i}/{j}/{outsuffix}/{k}\"\n",
    "                            \n",
    "    return EISDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Readout\n",
    "def readChannel(chID, fileDict):\n",
    "    '''==================================================\n",
    "        Read EIS.csv file by Channel\n",
    "        Parameter: \n",
    "            chID: channel index\n",
    "            fileDict: EISDict[_sessionIndex][_channelIndex] = \"_filepath\"\n",
    "        Returen:\n",
    "            freq: frequency\n",
    "            Zreal: real part of impedance\n",
    "            Zimag: imaginary part of impedance\n",
    "        ==================================================\n",
    "    '''\n",
    "    chData = []\n",
    "    for ssID in fileDict.keys():\n",
    "        _data   = np.loadtxt(fileDict[ssID][chID], delimiter=',')\n",
    "        _freq   = _data[:,0]\n",
    "        _Zreal  = _data[:,1] * np.cos(np.deg2rad(_data[:,2])) \n",
    "        _Zimag  = _data[:,1] * np.sin(np.deg2rad(_data[:,2])) \n",
    "        chData.append(np.stack((_freq, _Zreal, _Zimag),axis=0))\n",
    "\n",
    "    return np.stack(chData, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def EIS_recal_ver02(data, _phz_0 = None):\n",
    "    f_poi = data[0,:]\n",
    "    # Z_poi = data[1,:] * np.exp(1j*np.deg2rad(data[2,:]))\n",
    "    Z_poi = data[1,:] + 1j*data[2,:]\n",
    "    Y_poi = 1/Z_poi\n",
    "\n",
    "    Rg0 = 1.611e13\n",
    "    Cp0 = 1.4e-9\n",
    "    \n",
    "    _Rg0_rescale = 1/Rg0*np.power(f_poi,1.583)\n",
    "    _Cp0_rescale = Cp0*np.power(f_poi,0.911)\n",
    "    Y_org = Y_poi - _Rg0_rescale + 1j*_Cp0_rescale\n",
    "    # Y_org = Y_poi - _Rg0_rescale \n",
    "    # Y_org = Y_poi + 1j*_Cp0_rescale\n",
    "    # Y_org = Y_poi\n",
    "    Z_org = 1/Y_org\n",
    "\n",
    "    # Phz Calibration\n",
    "    if _phz_0 is None:\n",
    "        _phz_0 = np.loadtxt(\"./phz_Calib.txt\")\n",
    "    \n",
    "    Z_ampC = np.abs(Z_org)\n",
    "    # Z_phzC = np.angle(Z_org) - _phz_0\n",
    "    Z_phzC = np.angle(Z_org) - _phz_0\n",
    "\n",
    "    Z_rec = Z_ampC * np.exp(1j*Z_phzC)\n",
    "\n",
    "    # C = 5e-10\n",
    "    Rs0 = 100\n",
    "    Z_rec = Z_rec - Rs0\n",
    "\n",
    "\n",
    "\n",
    "    Cp0 = 5e-10\n",
    "    _Cp0_rescale = Cp0 * f_poi\n",
    "    Z_rec = 1/(1/Z_rec - 1j * _Cp0_rescale)\n",
    "\n",
    "    \n",
    "\n",
    "    # Ls0 = 1.7e-4\n",
    "    Ls0 = 5e-4\n",
    "    _Ls0_rescale = Ls0 * f_poi\n",
    "    Z_rec = Z_rec - 1j * _Ls0_rescale\n",
    "\n",
    "    # C = 5e-10\n",
    "    Rs0 = 566\n",
    "    Z_rec = Z_rec - Rs0\n",
    "    \n",
    "    return np.stack([f_poi, np.real(Z_rec), np.imag(Z_rec)], axis=1).T\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-08 16:51:29.763\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mSession Begin: 09290511_20241022_01\u001b[0m\n",
      "\u001b[32m2025-05-08 16:51:29.764\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 1\u001b[0m\n",
      "\u001b[32m2025-05-08 16:51:29.765\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 2\u001b[0m\n",
      "\u001b[32m2025-05-08 16:51:29.766\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 3\u001b[0m\n",
      "\u001b[32m2025-05-08 16:51:29.766\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 4\u001b[0m\n",
      "\u001b[32m2025-05-08 16:51:29.767\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mSession Begin: 09290511_20241024_01\u001b[0m\n",
      "\u001b[32m2025-05-08 16:51:29.768\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 1\u001b[0m\n",
      "\u001b[32m2025-05-08 16:51:29.769\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 2\u001b[0m\n",
      "\u001b[32m2025-05-08 16:51:29.769\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 3\u001b[0m\n",
      "\u001b[32m2025-05-08 16:51:29.770\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 4\u001b[0m\n",
      "\u001b[32m2025-05-08 16:51:29.771\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mSession Begin: 09290511_20241028_01\u001b[0m\n",
      "\u001b[32m2025-05-08 16:51:29.771\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 1\u001b[0m\n",
      "\u001b[32m2025-05-08 16:51:29.772\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 2\u001b[0m\n",
      "\u001b[32m2025-05-08 16:51:29.773\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 3\u001b[0m\n",
      "\u001b[32m2025-05-08 16:51:29.773\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 4\u001b[0m\n",
      "\u001b[32m2025-05-08 16:51:29.774\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mSession Begin: 09290511_20241029_01\u001b[0m\n",
      "\u001b[32m2025-05-08 16:51:29.774\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 1\u001b[0m\n",
      "\u001b[32m2025-05-08 16:51:29.775\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 2\u001b[0m\n",
      "\u001b[32m2025-05-08 16:51:29.775\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 3\u001b[0m\n",
      "\u001b[32m2025-05-08 16:51:29.776\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 4\u001b[0m\n",
      "\u001b[32m2025-05-08 16:51:29.776\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mSession Begin: 09290511_20241030_01\u001b[0m\n",
      "\u001b[32m2025-05-08 16:51:29.777\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 1\u001b[0m\n",
      "\u001b[32m2025-05-08 16:51:29.777\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 2\u001b[0m\n",
      "\u001b[32m2025-05-08 16:51:29.777\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 3\u001b[0m\n",
      "\u001b[32m2025-05-08 16:51:29.778\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 4\u001b[0m\n",
      "\u001b[32m2025-05-08 16:51:29.778\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mSession Begin: 09290511_20241031_01\u001b[0m\n",
      "\u001b[32m2025-05-08 16:51:29.779\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 1\u001b[0m\n",
      "\u001b[32m2025-05-08 16:51:29.779\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 2\u001b[0m\n",
      "\u001b[32m2025-05-08 16:51:29.779\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 3\u001b[0m\n",
      "\u001b[32m2025-05-08 16:51:29.780\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 4\u001b[0m\n",
      "\u001b[32m2025-05-08 16:51:29.780\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mSession Begin: 09290511_20241101_01\u001b[0m\n",
      "\u001b[32m2025-05-08 16:51:29.781\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 1\u001b[0m\n",
      "\u001b[32m2025-05-08 16:51:29.781\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 2\u001b[0m\n",
      "\u001b[32m2025-05-08 16:51:29.782\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 3\u001b[0m\n",
      "\u001b[32m2025-05-08 16:51:29.782\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 4\u001b[0m\n",
      "\u001b[32m2025-05-08 16:51:29.782\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mSession Begin: 09290511_20241103_01\u001b[0m\n",
      "\u001b[32m2025-05-08 16:51:29.783\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 1\u001b[0m\n",
      "\u001b[32m2025-05-08 16:51:29.783\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 2\u001b[0m\n",
      "\u001b[32m2025-05-08 16:51:29.783\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 3\u001b[0m\n",
      "\u001b[32m2025-05-08 16:51:29.783\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 4\u001b[0m\n",
      "\u001b[32m2025-05-08 16:51:29.783\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mSession Begin: 09290511_20241104_01\u001b[0m\n",
      "\u001b[32m2025-05-08 16:51:29.783\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 1\u001b[0m\n",
      "\u001b[32m2025-05-08 16:51:29.783\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 2\u001b[0m\n",
      "\u001b[32m2025-05-08 16:51:29.784\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 3\u001b[0m\n",
      "\u001b[32m2025-05-08 16:51:29.784\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 4\u001b[0m\n",
      "\u001b[32m2025-05-08 16:51:29.784\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mSession Begin: 09290511_20241105_01\u001b[0m\n",
      "\u001b[32m2025-05-08 16:51:29.785\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 1\u001b[0m\n",
      "\u001b[32m2025-05-08 16:51:29.785\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 2\u001b[0m\n",
      "\u001b[32m2025-05-08 16:51:29.785\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 3\u001b[0m\n",
      "\u001b[32m2025-05-08 16:51:29.786\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 4\u001b[0m\n",
      "\u001b[32m2025-05-08 16:51:29.786\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mSession Begin: 09290511_20241106_01\u001b[0m\n",
      "\u001b[32m2025-05-08 16:51:29.787\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 1\u001b[0m\n",
      "\u001b[32m2025-05-08 16:51:29.788\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 2\u001b[0m\n",
      "\u001b[32m2025-05-08 16:51:29.788\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 3\u001b[0m\n",
      "\u001b[32m2025-05-08 16:51:29.789\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 4\u001b[0m\n",
      "\u001b[32m2025-05-08 16:51:29.790\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mSession Begin: 09290511_20241107_01\u001b[0m\n",
      "\u001b[32m2025-05-08 16:51:29.790\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 1\u001b[0m\n",
      "\u001b[32m2025-05-08 16:51:29.791\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 2\u001b[0m\n",
      "\u001b[32m2025-05-08 16:51:29.791\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 3\u001b[0m\n",
      "\u001b[32m2025-05-08 16:51:29.792\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 4\u001b[0m\n",
      "\u001b[32m2025-05-08 16:51:29.792\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mSession Begin: 09290511_20241109_01\u001b[0m\n",
      "\u001b[32m2025-05-08 16:51:29.793\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 1\u001b[0m\n",
      "\u001b[32m2025-05-08 16:51:29.793\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 2\u001b[0m\n",
      "\u001b[32m2025-05-08 16:51:29.794\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 3\u001b[0m\n",
      "\u001b[32m2025-05-08 16:51:29.794\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 4\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(13, 3, 101)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rootPath = \"D:/Baihm/EISNN/Dataset/01037160_归档\"\n",
    "# ch_id = 20  # Normal to Short, Same to GPR  \n",
    "# ch_id = 89  # Same to GPR  \n",
    "# ch_id = 7  # Normal Example\n",
    "\n",
    "# rootPath = \"D:/Baihm/EISNN/Dataset/05087163_归档\"\n",
    "# ch_id = 7   # one outlier\n",
    "# ch_id = 50  # No outlier but in two Phases\n",
    "# ch_id = 55  # One outlier &wired end point\n",
    "# ch_id = 114 # Open Circuit with on outpler\n",
    "\n",
    "# rootPath = \"D:/Baihm/EISNN/Archive/02067447_归档\"\n",
    "# ch_id = 68  # Short all the time\n",
    "\n",
    "# rootPath = \"D:/Baihm/EISNN/Archive/01067095_归档\"\n",
    "# ch_id = 19    # First Sample is outlier\n",
    "\n",
    "rootPath = \"D:/Baihm/EISNN/Archive/09290511_归档\"\n",
    "ch_id = 13    # Up & Down, 2 outliers\n",
    "# ch_id = 21    # Normal + 2 outlier\n",
    "# ch_id = 41    # Normal + 2 outlier - *(Hard To Tell)\n",
    "# ch_id = 79    # 3-class, What a mess\n",
    "\n",
    "# rootPath = \"D:/Baihm/EISNN/Archive/11057712_归档\"\n",
    "# ch_id = 106    # Very Good Electrode with 1 hidden outlier, and one phase shift\n",
    "\n",
    "# rootPath = \"D:\\Baihm\\EISNN\\Archive/10057084_归档\"\n",
    "# ch_id = 16    # Totaly Mess\n",
    "# ch_id = 18    # Totaly Mess\n",
    "\n",
    "# rootPath = \"D:\\Baihm\\EISNN\\Archive/11067223_归档\"\n",
    "# ch_id = 124     # Perfect with one outlier\n",
    "\n",
    "# rootPath = \"D:\\Baihm\\EISNN\\Archive/06017758_归档\"\n",
    "# ch_id = 96     # Perfect of Perfect\n",
    "\n",
    "# rootPath = \"D:\\Baihm\\EISNN\\Archive/15361101_归档\"\n",
    "# ch_id = 0     # Only One Sample - Run With Error\n",
    "\n",
    "\n",
    "# rootPath = \"D:\\Baihm\\EISNN\\Archive/11207147_归档\"\n",
    "# ch_id = 0     # Only Three Sample - Run Without Error\n",
    "\n",
    "# rootPath = \"D:\\Baihm\\EISNN\\Archive/22037380_归档\"\n",
    "# ch_id = 20     # Connection Error\n",
    "\n",
    "\n",
    "# freq_list = np.linspace(0,np.shape(chData)[2]-1,101,dtype=int)\n",
    "freq_list = np.linspace(0,5000-1,101,dtype=int, endpoint=True)\n",
    "# freq_list_DTW = np.linspace(1000,5000-1,101,dtype=int, endpoint=True)\n",
    "EISDict = gatherCSV(rootPath)\n",
    "chData_full = readChannel(ch_id, EISDict)\n",
    "\n",
    "# chData_DTW = chData_full[:,:,freq_list_DTW]\n",
    "\n",
    "if True:\n",
    "    phz_calibration = np.loadtxt(\"./phz_Calib.txt\")\n",
    "    for i in range(np.shape(chData_full)[0]):\n",
    "        ch_eis = EIS_recal_ver02(chData_full[i,:,:], phz_calibration)\n",
    "        chData_full[i,:,:] = ch_eis\n",
    "\n",
    "# chData = chData[:,:,91:100]\n",
    "chData = chData_full[:,:,freq_list]\n",
    "\n",
    "\n",
    "np.shape(chData)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "from  Outlier import OutlierDetection\n",
    "\n",
    "CLEAN_FLAG = True\n",
    "if CLEAN_FLAG:\n",
    "    import joblib\n",
    "    weirdSVMmodel = joblib.load(\"../Outlier/weirdSVMmodel.pkl\")\n",
    "    # eis_seq, eis_cluster, eis_anomaly, leaf_anomaly= OutlierDetection.OutlierDetection(chData_full)\n",
    "    eis_seq, eis_cluster, eis_anomaly, leaf_anomaly, seq_weird = OutlierDetection.OutlierDetection_Ver02(chData_full, weirdSVMmodel)\n",
    "else: \n",
    "    eis_seq = np.arange(np.shape(chData)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    fig= plt.figure(figsize=(15,8), constrained_layout=False)\n",
    "    axis = [0] * 8\n",
    "    axis[0] = fig.add_subplot(2,4,1, projection='3d')   \n",
    "    axis[1] = fig.add_subplot(2,4,2)            \n",
    "    axis[2] = fig.add_subplot(2,4,3)         \n",
    "    axis[3] = fig.add_subplot(2,4,4)      \n",
    "    axis[4] = fig.add_subplot(2,4,5, projection='3d')      \n",
    "    axis[5] = fig.add_subplot(2,4,6)         \n",
    "    axis[6] = fig.add_subplot(2,4,7)         \n",
    "    axis[7] = fig.add_subplot(2,4,8)    \n",
    "\n",
    "    init_elev = 21  # 仰角\n",
    "    init_azim = 55  # 方位角\n",
    "    axis[0].view_init(elev=init_elev, azim=init_azim)\n",
    "    axis[4].view_init(elev=init_elev, azim=init_azim)\n",
    "\n",
    "\n",
    "    num_samples = np.shape(chData)[0]\n",
    "\n",
    "    _x = np.arange(num_samples)[eis_seq]\n",
    "    _y = np.log10(chData[0,0,:]).flatten()\n",
    "    X, Y = np.meshgrid(_x, _y, indexing='ij')\n",
    "    axis[0].plot_surface(X, Y, np.log10(np.abs(chData[eis_seq,1,:]+1j*chData[eis_seq,2,:])), cmap='viridis_r', alpha=0.8)\n",
    "    axis[4].plot_surface(X, Y, np.rad2deg(np.angle(chData[eis_seq,1,:]+1j*chData[eis_seq,2,:])), cmap='viridis_r', alpha=0.8)\n",
    "\n",
    "\n",
    "\n",
    "    cmap = plt.colormaps.get_cmap('rainbow_r')\n",
    "    for i in range(len(eis_seq)):\n",
    "        _x = eis_seq[i]\n",
    "        ch_eis = chData[_x,:,:]\n",
    "        _color = cmap(_x/num_samples)\n",
    "        axis[1].loglog(ch_eis[0,:], np.abs(ch_eis[1,:]+1j*ch_eis[2,:]), color = _color, linewidth=2, label=f\"S{i:02d}\")\n",
    "        axis[5].semilogx(ch_eis[0,:], np.rad2deg(np.angle(ch_eis[1,:]+1j*ch_eis[2,:])), color = _color, linewidth=2, label=f\"S{i:02d}\")\n",
    "\n",
    "\n",
    "    cmap = plt.colormaps.get_cmap('Set1')\n",
    "    for i in range(len(eis_seq)):\n",
    "        _x = eis_seq[i]\n",
    "        ch_eis = chData[_x,:,:]\n",
    "        _color = cmap(eis_cluster[i])\n",
    "        axis[2].loglog(ch_eis[0,:], np.abs(ch_eis[1,:]+1j*ch_eis[2,:]), color = _color, linewidth=2, label=f\"{chr(ord('A')+eis_cluster[i])}\")\n",
    "        axis[6].semilogx(ch_eis[0,:], np.rad2deg(np.angle(ch_eis[1,:]+1j*ch_eis[2,:])), color = _color, linewidth=2, label=f\"{chr(ord('A')+eis_cluster[i])}\")\n",
    "\n",
    "    _legend_handle = []\n",
    "    for i in range(len(np.unique(eis_cluster))):\n",
    "        _legend_handle.append(mpatches.Patch(color = cmap(i), label = f\"{chr(ord('A')+i)}:{len(eis_cluster[eis_cluster==i])}\"))\n",
    "    axis[2].legend(handles=_legend_handle)\n",
    "\n",
    "    axis[2].sharex(axis[1])\n",
    "    axis[6].sharex(axis[5])\n",
    "\n",
    "\n",
    "    cmap = plt.colormaps.get_cmap('rainbow_r')\n",
    "    for i in range(len(eis_anomaly)):\n",
    "        _x = eis_anomaly[i]\n",
    "        ch_eis = chData[_x,:,:]\n",
    "        _color = cmap(_x/num_samples)\n",
    "        axis[3].loglog(ch_eis[0,:], np.abs(ch_eis[1,:]+1j*ch_eis[2,:]), color = _color, linewidth=2, label=f\"S{_x:02d}\")\n",
    "        axis[7].semilogx(ch_eis[0,:], np.rad2deg(np.angle(ch_eis[1,:]+1j*ch_eis[2,:])), color = _color, linewidth=2, label=f\"S{_x:02d}\")\n",
    "    axis[3].legend()\n",
    "    axis[3].sharex(axis[1])\n",
    "    axis[7].sharex(axis[5])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EIS-GP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 之前发现 likelihood声明的时候没加rank，可能会显著影响结果，这里\n",
    "# from gpytorch.kernels import MultitaskKernel, RQKernel, RBFKernel, MaternKernel\n",
    "\n",
    "class EISGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood, num_tasks):\n",
    "        super().__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.MultitaskMean(\n",
    "            gpytorch.means.ConstantMean(), num_tasks=num_tasks\n",
    "        )\n",
    "        self.covar_module = gpytorch.kernels.MultitaskKernel(\n",
    "            # gpytorch.kernels.RBFKernel(),\n",
    "            # gpytorch.kernels.RQKernel(),\n",
    "            # gpytorch.kernels.LinearKernel(),\n",
    "            # gpytorch.kernels.PolynomialKernel(power=3.0),\n",
    "            # gpytorch.kernels.PiecewisePolynomialKernel(),\n",
    "            # gpytorch.kernels.SpectralMixtureKernel(num_mixtures=3),\n",
    "            # gpytorch.kernels.CosineKernel(),\n",
    "            \n",
    "            gpytorch.kernels.MaternKernel(nu=0.5), \n",
    "            \n",
    "            num_tasks=num_tasks, \n",
    "            rank=2\n",
    "        )\n",
    "        # self.covar_module.data_covar_module.lengthscale = 1\n",
    "        # self.covar_module.data_covar_module.alpha = 0.001\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultitaskMultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "\n",
    "def EISGPTrain(x_train, y_train, x_eval, cluster_id, device, training_iter = 200, lr = 0.05):\n",
    "    num_tasks = y_train.shape[1]\n",
    "    # Initialize likelihood and model\n",
    "    likelihood = gpytorch.likelihoods.MultitaskGaussianLikelihood(\n",
    "        num_tasks=num_tasks, rank = 0).to(device)\n",
    "    model = EISGPModel(x_train, y_train, likelihood, num_tasks).to(device)\n",
    "\n",
    "    # Find optimal model hyperparameters\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)  # Includes GaussianLikelihood parameters\n",
    "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "    # logger.info(f\"Training for {training_iter} iterations...\")\n",
    "    loss_inst       = []\n",
    "    length_inst     = []\n",
    "    noise_inst      = []\n",
    "    for i in range(training_iter):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(x_train)\n",
    "        loss = -mll(output, y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        poi_noise   = model.likelihood.noise.detach().cpu().numpy()\n",
    "        poi_length  = model.covar_module.data_covar_module.lengthscale.detach().cpu().numpy()\n",
    "        # poi_length  = 0\n",
    "        \n",
    "        loss_inst.append(loss.item())\n",
    "        noise_inst.append(poi_noise)\n",
    "        length_inst.append(poi_length)\n",
    "        if not (i+1)%100:\n",
    "            logger.info(f\"C{cluster_id} - Iter {i+1}/{training_iter}\\tLoss: {loss.item()}\")\n",
    "            \n",
    "    # logger.info(\"Model Training Finished.\")\n",
    "\n",
    "    # Make predictions\n",
    "    model.eval()\n",
    "    likelihood.eval()\n",
    "    with torch.no_grad(), gpytorch.settings.cholesky_jitter(1e-4):\n",
    "        pred = likelihood(model(x_eval))\n",
    "    # logger.info(\"Model Evaluation Finished.\")\n",
    "\n",
    "    return [pred, np.array(loss_inst), np.array(length_inst), np.array(noise_inst)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def piecewise_interp(chData, eis_seq, run_list, eis_cluster = None, SPEED_RATE = 1, LOG_FLAG = True):\n",
    "    # Init xy according to the datetime\n",
    "    x_day = [datetime.strptime(date, '%Y%m%d') for date in EISDict.keys()]\n",
    "    x_day = [x_day[i] for i in eis_seq]\n",
    "\n",
    "    x_train_full = np.array([(poi - x_day[0]).days for poi in x_day])\n",
    "    x_eval_full = np.linspace(0,max(x_train_full),max(x_train_full)*SPEED_RATE+1)\n",
    "\n",
    "    y_train_full = np.stack([chData[eis_seq,1,:],chData[eis_seq,2,:]], axis=2)\n",
    "    y_train_full = y_train_full.take(run_list, axis=1)\n",
    "\n",
    "\n",
    "    if LOG_FLAG:\n",
    "        y_train_log = np.log(y_train_full[:,:,0] + 1j*y_train_full[:,:,1])\n",
    "        y_train_full = np.stack([y_train_log.real, y_train_log.imag], axis=2)\n",
    "\n",
    "\n",
    "    # Segmentation of clusters\n",
    "    if eis_cluster is None:\n",
    "        eis_cluster = np.zeros_like(eis_seq)\n",
    "    unique_clusters = np.unique(eis_cluster)\n",
    "    n_clusters = len(unique_clusters)\n",
    "\n",
    "    train_mask_list = []\n",
    "    eval_mask_list = []\n",
    "\n",
    "    for i in range(n_clusters):\n",
    "        # 取当前状态和下一个状态的数据\n",
    "        train_mask = (eis_cluster == unique_clusters[i])\n",
    "        if i == n_clusters - 1:\n",
    "            x_eval_end = x_eval_full.max() + 1\n",
    "        else:\n",
    "            x_eval_end = x_train_full[(eis_cluster == unique_clusters[i+1])].min()\n",
    "        # x_state = x_train[state_mask]\n",
    "        # y_state = y_train[:,state_mask]\n",
    "\n",
    "        eval_mask = (x_eval_full >= x_train_full[train_mask].min()) & (x_eval_full < x_eval_end)\n",
    "        \n",
    "        train_mask_list.append(train_mask)\n",
    "        eval_mask_list.append(eval_mask)\n",
    "    \n",
    "    return x_train_full, y_train_full, x_eval_full, n_clusters, train_mask_list, eval_mask_list\n",
    "    \n",
    "\n",
    "def GPDataLoader(x_train, y_train, x_eval, NORM_X_FLAG = True, NORM_Y_FLAG = True):\n",
    "\n",
    "    Scaler_X        = StandardScaler()\n",
    "    Scaler_Y_real   = StandardScaler()\n",
    "    Scaler_Y_imag   = StandardScaler()\n",
    "\n",
    "    if NORM_Y_FLAG:\n",
    "        y_train[:,:,0] = Scaler_Y_real.fit_transform(y_train[:,:,0])\n",
    "        y_train[:,:,1] = Scaler_Y_imag.fit_transform(y_train[:,:,1])\n",
    "    if NORM_X_FLAG:\n",
    "        x_train = Scaler_X.fit_transform(x_train.reshape(-1, 1)).flatten()\n",
    "        x_eval = Scaler_X.transform(x_eval.reshape(-1, 1)).flatten()\n",
    "\n",
    "    \n",
    "    y_train = np.hstack((y_train[:,:,0], y_train[:,:,1]))\n",
    "\n",
    "    logger.info(f\"\\nx: {np.shape(x_train)} \\ny: {np.shape(y_train)} \\nx_pred{np.shape(x_eval)}\")\n",
    "\n",
    "    return x_train, y_train, x_eval, [Scaler_X, Scaler_Y_real, Scaler_Y_imag]\n",
    "\n",
    "\n",
    "def GPDataExporter(x_train, y_train, x_eval, y_eval_mean, y_eval_var, ScalerSet, NORM_X_FLAG, NORM_Y_FLAG):\n",
    "    # Export Data\n",
    "    n_freq = np.shape(y_train)[1]//2 \n",
    "    y_train = np.stack((y_train[:,:n_freq], y_train[:,n_freq:]), axis=2)\n",
    "    y_eval_mean = np.stack((y_eval_mean[:,:n_freq], y_eval_mean[:,n_freq:]), axis=2)\n",
    "    y_eval_var = np.stack((y_eval_var[:,:n_freq], y_eval_var[:,n_freq:]), axis=2)\n",
    "    if NORM_X_FLAG:\n",
    "        x_train = ScalerSet[0].inverse_transform(x_train.reshape(-1, 1)).flatten()\n",
    "        x_eval = ScalerSet[0].inverse_transform(x_eval.reshape(-1, 1)).flatten()\n",
    "    \n",
    "    if NORM_Y_FLAG:\n",
    "        y_train_real = ScalerSet[1].inverse_transform(y_train[:,:,0])\n",
    "        y_train_imag = ScalerSet[2].inverse_transform(y_train[:,:,1])\n",
    "        \n",
    "        y_eval_mean_real = ScalerSet[1].inverse_transform(y_eval_mean[:,:,0])\n",
    "        y_eval_mean_imag = ScalerSet[2].inverse_transform(y_eval_mean[:,:,1])\n",
    "\n",
    "        y_eval_var_real = y_eval_var[:,:,0] * ScalerSet[1].var_\n",
    "        y_eval_var_imag = y_eval_var[:,:,1] * ScalerSet[2].var_\n",
    "    else:\n",
    "        y_train_real = y_train[:,:,0]\n",
    "        y_train_imag = y_train[:,:,1]\n",
    "\n",
    "        y_eval_mean_real = y_eval_mean[:,:,0]\n",
    "        y_eval_mean_imag = y_eval_mean[:,:,1]\n",
    "\n",
    "        y_eval_var_real = y_eval_var[:,:,0]\n",
    "        y_eval_var_imag = y_eval_var[:,:,1]\n",
    "\n",
    "    y_train = np.stack([y_train_real, y_train_imag], axis=2)\n",
    "    y_eval = np.stack([y_eval_mean_real, y_eval_mean_imag], axis=2)\n",
    "    y_eval_err = np.stack([y_eval_var_real, y_eval_var_imag], axis=2)\n",
    "    \n",
    "    logger.info(f\"\\nx: {np.shape(x_train)} \\ny: {np.shape(y_train)} \\nx_pred{np.shape(x_eval)} \\ny_pred{np.shape(y_eval)} \\ny_pred{np.shape(y_eval_err)}\")\n",
    "\n",
    "    return x_train, y_train, x_eval, y_eval, y_eval_err\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "SPEED_RATE=2\n",
    "LOG_FLAG=False\n",
    "NORM_X_FLAG=True\n",
    "NORM_Y_FLAG=True\n",
    "\n",
    "\n",
    "# run_list = range(60,80)\n",
    "run_list = range(np.shape(chData)[2])\n",
    "\n",
    "x_train_full, y_train_full, x_eval_full,  n_clusters, train_mask_list, eval_mask_list = \\\n",
    "    piecewise_interp(chData, eis_seq, run_list, \n",
    "                     eis_cluster = None, SPEED_RATE = SPEED_RATE, LOG_FLAG=LOG_FLAG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-08 16:54:31.008\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mGPDataLoader\u001b[0m:\u001b[36m61\u001b[0m - \u001b[1m\n",
      "x: (10,) \n",
      "y: (10, 202) \n",
      "x_pred(37,)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "x_train = x_train_full[train_mask_list[0]]\n",
    "y_train = y_train_full[train_mask_list[0],:,:]\n",
    "x_eval = x_eval_full[eval_mask_list[0]]\n",
    "\n",
    "x_train, y_train, x_eval, ScalerSet = \\\n",
    "    GPDataLoader(x_train, y_train, x_eval, \n",
    "        NORM_X_FLAG=NORM_X_FLAG, NORM_Y_FLAG=NORM_Y_FLAG)\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "x_train_tensor = torch.from_numpy(x_train).float().to(device)\n",
    "x_eval_tensor = torch.from_numpy(x_eval).float().to(device)\n",
    "y_train_tensor = torch.from_numpy(y_train).float().to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-08 16:54:33.502\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mEISGPTrain\u001b[0m:\u001b[36m66\u001b[0m - \u001b[1mC0 - Iter 100/500\tLoss: -0.2840431332588196\u001b[0m\n",
      "\u001b[32m2025-05-08 16:54:35.753\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mEISGPTrain\u001b[0m:\u001b[36m66\u001b[0m - \u001b[1mC0 - Iter 200/500\tLoss: -0.7687401175498962\u001b[0m\n",
      "\u001b[32m2025-05-08 16:54:38.008\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mEISGPTrain\u001b[0m:\u001b[36m66\u001b[0m - \u001b[1mC0 - Iter 300/500\tLoss: -0.8300940990447998\u001b[0m\n",
      "\u001b[32m2025-05-08 16:54:40.243\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mEISGPTrain\u001b[0m:\u001b[36m66\u001b[0m - \u001b[1mC0 - Iter 400/500\tLoss: -0.8434693813323975\u001b[0m\n",
      "\u001b[32m2025-05-08 16:54:42.483\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mEISGPTrain\u001b[0m:\u001b[36m66\u001b[0m - \u001b[1mC0 - Iter 500/500\tLoss: -0.8493403792381287\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "y_eval_tensor, loss_inst, length_inst, noise_inst = \\\n",
    "    EISGPTrain(x_train_tensor, y_train_tensor, x_eval_tensor, 0, device, training_iter=500)\n",
    "\n",
    "\n",
    "y_eval_mean = y_eval_tensor.mean.cpu().numpy()\n",
    "y_eval_cov = y_eval_tensor.covariance_matrix.cpu().detach().numpy()\n",
    "y_eval_var = y_eval_tensor.variance.detach().cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-08 16:54:42.589\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mGPDataExporter\u001b[0m:\u001b[36m99\u001b[0m - \u001b[1m\n",
      "x: (10,) \n",
      "y: (10, 101, 2) \n",
      "x_pred(37,) \n",
      "y_pred(37, 101, 2) \n",
      "y_pred(37, 101, 2)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train, x_eval, y_eval, y_eval_err = \\\n",
    "        GPDataExporter(x_train, y_train, x_eval, y_eval_mean, y_eval_var, ScalerSet,\n",
    "                       NORM_X_FLAG=NORM_X_FLAG, NORM_Y_FLAG=NORM_Y_FLAG)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    fig = plt.figure()\n",
    "    ax1 = fig.add_subplot(311)\n",
    "    ax2 = fig.add_subplot(312)\n",
    "    ax3 = fig.add_subplot(313)\n",
    "    ax1.plot(loss_inst)\n",
    "    ax1.set_title(\"Loss\")\n",
    "\n",
    "\n",
    "    cmap = plt.colormaps.get_cmap('viridis_r')\n",
    "    for i in range(np.shape(length_inst)[1]):\n",
    "        ax2.plot(noise_inst[:,i].flatten(), color = cmap(i/np.shape(length_inst)[1]), linewidth=2, label=f\"LengthScale {i+1}\")\n",
    "        ax3.plot(length_inst[:,i].flatten(), color = cmap(i/np.shape(length_inst)[1]), linewidth=2, label=f\"LengthScale {i+1}\")\n",
    "    ax2.set_yscale('log')\n",
    "    ax3.set_title(\"LengthScale\")\n",
    "\n",
    "    fig.set_tight_layout(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPR Result Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    n_freq = np.shape(y_eval)[0]\n",
    "    fig = plt.figure(figsize=(8,8))\n",
    "    cmap = plt.colormaps.get_cmap('viridis_r')\n",
    "    axis0 = fig.add_subplot(2,2,1)\n",
    "    axis1 = fig.add_subplot(2,2,2)\n",
    "    axis2 = fig.add_subplot(2,2,3)\n",
    "    axis3 = fig.add_subplot(2,2,4)\n",
    "\n",
    "\n",
    "    # for i in range(n_freq):\n",
    "    # for i in range(70,80):\n",
    "    for i in range(np.shape(y_eval)[1]):\n",
    "        axis0.fill_between(x_eval, y_eval[:,i,0] - 2*np.sqrt(y_eval_err[:,i,0]), y_eval[:,i,0] + 2*np.sqrt(y_eval_err[:,i,0]), \n",
    "                        alpha=0.2, color = cmap(run_list[i]/n_freq))\n",
    "        \n",
    "        axis1.plot(x_eval, y_eval[:,i,0], color = cmap(i/n_freq))\n",
    "        axis1.plot(x_train, y_train[:,i,0], color = cmap(i/n_freq), linestyle = ' ', marker = 'o')\n",
    "\n",
    "        axis2.fill_between(x_eval, y_eval[:,i,1] - 2*np.sqrt(y_eval_err[:,i,1]), y_eval[:,i,1] + 2*np.sqrt(y_eval_err[:,i,1]), \n",
    "                        alpha=0.2, color = cmap(run_list[i]/n_freq))\n",
    "        \n",
    "        axis3.plot(x_eval, y_eval[:,i,1], color = cmap(i/n_freq))\n",
    "        axis3.plot(x_train, y_train[:,i,1], color = cmap(i/n_freq), linestyle = ' ', marker = 'o')\n",
    "\n",
    "    # axis1.sharex(axis0)\n",
    "    # axis1.sharey(axis0)\n",
    "    # axis3.sharex(axis2)\n",
    "    # axis3.sharey(axis2)\n",
    "    axis0.set_xlabel('x')\n",
    "    axis0.set_ylabel('y')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPR EIS Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    \n",
    "    n_freq = np.shape(y_eval)[0]\n",
    "    fig = plt.figure(figsize=(8,8))\n",
    "    cmap = plt.colormaps.get_cmap('rainbow_r')\n",
    "    axis0 = fig.add_subplot(2,3,1)\n",
    "    axis1 = fig.add_subplot(2,3,2)\n",
    "    axis2 = fig.add_subplot(2,3,3, projection = '3d')\n",
    "    axis3 = fig.add_subplot(2,3,4)\n",
    "    axis4 = fig.add_subplot(2,3,5)\n",
    "    axis5 = fig.add_subplot(2,3,6, projection = '3d')\n",
    "\n",
    "\n",
    "    init_elev = 40  # 仰角\n",
    "    init_azim = 30  # 方位角\n",
    "    axis2.view_init(elev=init_elev, azim=init_azim)\n",
    "    axis5.view_init(elev=init_elev, azim=init_azim)\n",
    "\n",
    "\n",
    "    new_f = chData[0,0,:].take(run_list, axis=0)\n",
    "\n",
    "    if LOG_FLAG:\n",
    "        \n",
    "        y_EIS_train =   np.exp(y_train[:,:,0] + 1j * y_train[:,:,1])\n",
    "        y_EIS_eval = np.exp(y_eval[:,:,0] + 1j * y_eval[:,:,1])\n",
    "\n",
    "        # f_plot = range(40,80)\n",
    "        f_plot = range(np.shape(new_f)[0])\n",
    "        y_EIS_train = y_EIS_train.take(f_plot, axis=1)\n",
    "        y_EIS_eval = y_EIS_eval.take(f_plot, axis=1)\n",
    "        new_f = new_f.take(f_plot, axis=0)\n",
    "\n",
    "    else:\n",
    "        y_EIS_train = y_train[:,:,0] + 1j*y_train[:,:,1]\n",
    "        y_EIS_eval = y_eval[:,:,0] + 1j*y_eval[:,:,1]\n",
    "\n",
    "        # f_plot = range(40,60)\n",
    "        f_plot = range(np.shape(new_f)[0])\n",
    "        y_EIS_train = y_EIS_train.take(f_plot, axis=1)\n",
    "        y_EIS_eval = y_EIS_eval.take(f_plot, axis=1)\n",
    "        new_f = new_f.take(f_plot, axis=0)\n",
    "\n",
    "\n",
    "\n",
    "    for i in range(np.shape(x_eval)[0]):\n",
    "        axis1.semilogx(new_f, np.log10(np.abs(y_EIS_eval[i,:])), color = cmap(i/np.shape(x_eval)[0]))\n",
    "        axis4.semilogx(new_f, np.rad2deg(np.angle(y_EIS_eval[i,:])), color = cmap(i/np.shape(x_eval)[0]))\n",
    "        \n",
    "        \n",
    "    for i in range(np.shape(x_train)[0]):\n",
    "        axis1.semilogx(new_f, np.log10(np.abs(y_EIS_train[i,:])), 'black', alpha = 0.3)\n",
    "        axis4.semilogx(new_f, np.rad2deg(np.angle(y_EIS_train[i,:])), 'black', alpha = 0.3)\n",
    "    \n",
    "\n",
    "\n",
    "    _x = np.arange(np.shape(x_eval)[0])\n",
    "    _y = np.log10(new_f).flatten()\n",
    "    X, Y = np.meshgrid(_x, _y, indexing='ij')\n",
    "    axis2.plot_surface(X, Y, np.log10(np.abs(y_EIS_eval[:,:])), cmap='viridis_r', alpha=0.8)\n",
    "    axis5.plot_surface(X, Y, np.rad2deg(np.angle(y_EIS_eval[:,:])) * 180 / np.pi, cmap='viridis_r', alpha=0.8)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segment "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPEED_RATE=2\n",
    "LOG_FLAG=True\n",
    "NORM_X_FLAG=True\n",
    "NORM_Y_FLAG=True\n",
    "\n",
    "\n",
    "# run_list = range(60,80)\n",
    "run_list = range(np.shape(chData)[2])\n",
    "\n",
    "x_train_full, y_train_full, x_eval_full,  n_clusters, train_mask_list, eval_mask_list = \\\n",
    "    piecewise_interp(chData, eis_seq, run_list, \n",
    "                     eis_cluster, SPEED_RATE = SPEED_RATE, LOG_FLAG=LOG_FLAG)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-08 16:55:31.875\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mGPDataLoader\u001b[0m:\u001b[36m61\u001b[0m - \u001b[1m\n",
      "x: (3,) \n",
      "y: (3, 202) \n",
      "x_pred(14,)\u001b[0m\n",
      "\u001b[32m2025-05-08 16:55:33.026\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mEISGPTrain\u001b[0m:\u001b[36m66\u001b[0m - \u001b[1mC0 - Iter 100/200\tLoss: -1.2300845384597778\u001b[0m\n",
      "\u001b[32m2025-05-08 16:55:34.034\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mEISGPTrain\u001b[0m:\u001b[36m66\u001b[0m - \u001b[1mC0 - Iter 200/200\tLoss: -3.013937473297119\u001b[0m\n",
      "\u001b[32m2025-05-08 16:55:34.054\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mGPDataExporter\u001b[0m:\u001b[36m99\u001b[0m - \u001b[1m\n",
      "x: (3,) \n",
      "y: (3, 101, 2) \n",
      "x_pred(14,) \n",
      "y_pred(14, 101, 2) \n",
      "y_pred(14, 101, 2)\u001b[0m\n",
      "\u001b[32m2025-05-08 16:55:34.055\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mGPDataLoader\u001b[0m:\u001b[36m61\u001b[0m - \u001b[1m\n",
      "x: (3,) \n",
      "y: (3, 202) \n",
      "x_pred(14,)\u001b[0m\n",
      "\u001b[32m2025-05-08 16:55:35.091\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mEISGPTrain\u001b[0m:\u001b[36m66\u001b[0m - \u001b[1mC1 - Iter 100/200\tLoss: -1.2432068586349487\u001b[0m\n",
      "\u001b[32m2025-05-08 16:55:36.115\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mEISGPTrain\u001b[0m:\u001b[36m66\u001b[0m - \u001b[1mC1 - Iter 200/200\tLoss: -3.0036368370056152\u001b[0m\n",
      "\u001b[32m2025-05-08 16:55:36.164\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mGPDataExporter\u001b[0m:\u001b[36m99\u001b[0m - \u001b[1m\n",
      "x: (3,) \n",
      "y: (3, 101, 2) \n",
      "x_pred(14,) \n",
      "y_pred(14, 101, 2) \n",
      "y_pred(14, 101, 2)\u001b[0m\n",
      "\u001b[32m2025-05-08 16:55:36.165\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mGPDataLoader\u001b[0m:\u001b[36m61\u001b[0m - \u001b[1m\n",
      "x: (4,) \n",
      "y: (4, 202) \n",
      "x_pred(9,)\u001b[0m\n",
      "\u001b[32m2025-05-08 16:55:38.408\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mEISGPTrain\u001b[0m:\u001b[36m66\u001b[0m - \u001b[1mC2 - Iter 100/200\tLoss: -0.6378360986709595\u001b[0m\n",
      "\u001b[32m2025-05-08 16:55:40.604\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mEISGPTrain\u001b[0m:\u001b[36m66\u001b[0m - \u001b[1mC2 - Iter 200/200\tLoss: -1.161294937133789\u001b[0m\n",
      "\u001b[32m2025-05-08 16:55:40.631\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mGPDataExporter\u001b[0m:\u001b[36m99\u001b[0m - \u001b[1m\n",
      "x: (4,) \n",
      "y: (4, 101, 2) \n",
      "x_pred(9,) \n",
      "y_pred(9, 101, 2) \n",
      "y_pred(9, 101, 2)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "training_iter = 200\n",
    "\n",
    "\n",
    "y_eval_full = np.zeros((np.shape(x_eval_full)[0], np.shape(y_train_full)[1], 2))\n",
    "y_eval_err_full = np.zeros((np.shape(x_eval_full)[0], np.shape(y_train_full)[1], 2))\n",
    "\n",
    "loss_list = []\n",
    "noise_list = []\n",
    "length_list = []\n",
    "\n",
    "\n",
    "for i in range(n_clusters):\n",
    "# for i in [1]:\n",
    "\n",
    "    x_train = x_train_full[train_mask_list[i]]\n",
    "    y_train = y_train_full[train_mask_list[i],:,:]\n",
    "    x_eval = x_eval_full[eval_mask_list[i]]\n",
    "\n",
    "    x_train, y_train, x_eval, ScalerSet = \\\n",
    "        GPDataLoader(x_train, y_train, x_eval, \n",
    "            NORM_X_FLAG=NORM_X_FLAG, NORM_Y_FLAG=NORM_Y_FLAG)\n",
    "\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    x_train_tensor = torch.from_numpy(x_train).float().to(device)\n",
    "    x_eval_tensor = torch.from_numpy(x_eval).float().to(device)\n",
    "    y_train_tensor = torch.from_numpy(y_train).float().to(device)\n",
    "\n",
    "\n",
    "\n",
    "    y_eval_tensor, loss_inst, length_inst, noise_inst = \\\n",
    "    EISGPTrain(x_train_tensor, y_train_tensor, x_eval_tensor, i, device, training_iter=training_iter)\n",
    "\n",
    "    loss_list.append(loss_inst)\n",
    "    noise_list.append(noise_inst)\n",
    "    length_list.append(length_inst)\n",
    "\n",
    "    y_eval_mean = y_eval_tensor.mean.cpu().numpy()\n",
    "    y_eval_cov = y_eval_tensor.covariance_matrix.cpu().detach().numpy()\n",
    "    y_eval_var = y_eval_tensor.variance.detach().cpu().numpy()\n",
    "\n",
    "\n",
    "    x_train, y_train, x_eval, y_eval, y_eval_err = \\\n",
    "            GPDataExporter(x_train, y_train, x_eval, y_eval_mean, y_eval_var, ScalerSet,\n",
    "                        NORM_X_FLAG=NORM_X_FLAG, NORM_Y_FLAG=NORM_Y_FLAG)\n",
    "        \n",
    "    y_eval_full[eval_mask_list[i],:,:] = y_eval\n",
    "    y_eval_err_full[eval_mask_list[i],:,:] = y_eval_err\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    fig, axis = plt.subplots(1+2*n_clusters,1,figsize=(6,6))\n",
    "\n",
    "\n",
    "    for j in range(n_clusters):\n",
    "        loss_inst = loss_list[j]\n",
    "        noise_inst = noise_list[j]\n",
    "        length_inst = length_list[j]\n",
    "\n",
    "        axis[0].plot(loss_inst+10, label = f\"Cluster {j}\")\n",
    "        axis[0].set_yscale('log')\n",
    "        axis[0].set_title(\"Loss\")\n",
    "\n",
    "\n",
    "        cmap = plt.colormaps.get_cmap('viridis')\n",
    "        for i in range(np.shape(length_inst)[1]):\n",
    "        # for i in range(60,70):\n",
    "        # for i in range(0,20):\n",
    "            axis[1+2*j].plot(noise_inst[:,i].flatten(), color = cmap(i/np.shape(length_inst)[1]), linewidth=2, label=f\"LengthScale {i+1}\")\n",
    "            axis[1+2*j+1].plot(length_inst[:,i].flatten(), color = cmap(i/np.shape(length_inst)[1]), linewidth=2, label=f\"LengthScale {i+1}\")\n",
    "        \n",
    "        # axis[1+2*j].set_yscale('log')\n",
    "        axis[1+2*j].set_title(f\"Noise C{j}\")\n",
    "        # axis[1+2*j+1].set_yscale('log')\n",
    "        axis[1+2*j+1].set_title(f\"LengthScale C{j}\")\n",
    "\n",
    "    # axis[0].legend(frameon=False, loc='upper right')\n",
    "    fig.set_tight_layout(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPR Result Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    \n",
    "    cluster_id = None\n",
    "    if cluster_id is None:\n",
    "        x_train = x_train_full\n",
    "        y_train = y_train_full\n",
    "        x_eval = x_eval_full\n",
    "        y_eval = y_eval_full\n",
    "        y_eval_err = y_eval_err_full\n",
    "\n",
    "    else:\n",
    "        x_train = x_train_full[train_mask_list[cluster_id]]\n",
    "        y_train = y_train_full[train_mask_list[cluster_id],:,:]\n",
    "        x_eval = x_eval_full[eval_mask_list[cluster_id]]\n",
    "        y_eval = y_eval_full[eval_mask_list[cluster_id],:,:]\n",
    "        y_eval_err = y_eval_err_full[eval_mask_list[cluster_id],:,:]\n",
    "\n",
    "\n",
    "        # Scaler_Y_real   = StandardScaler()\n",
    "        # Scaler_Y_imag   = StandardScaler()\n",
    "\n",
    "        # if NORM_Y_FLAG:\n",
    "        #     y_train[:,:,0] = Scaler_Y_real.fit_transform(y_train[:,:,0])\n",
    "        #     y_train[:,:,1] = Scaler_Y_imag.fit_transform(y_train[:,:,1])\n",
    "        #     y_eval[:,:,0] = Scaler_Y_real.transform(y_eval[:,:,0])\n",
    "        #     y_eval[:,:,1] = Scaler_Y_imag.transform(y_eval[:,:,1])\n",
    "            \n",
    "\n",
    "\n",
    "    n_freq = np.shape(y_eval)[0]\n",
    "    fig = plt.figure(figsize=(8,8))\n",
    "    cmap = plt.colormaps.get_cmap('viridis_r')\n",
    "    axis0 = fig.add_subplot(2,2,1)\n",
    "    axis1 = fig.add_subplot(2,2,2)\n",
    "    axis2 = fig.add_subplot(2,2,3)\n",
    "    axis3 = fig.add_subplot(2,2,4)\n",
    "\n",
    "\n",
    "    # for i in range(n_freq):\n",
    "    # for i in range(30,40):\n",
    "    for i in range(np.shape(y_eval)[1]):\n",
    "        axis0.fill_between(x_eval, y_eval[:,i,0] - 2*np.sqrt(y_eval_err[:,i,0]), y_eval[:,i,0] + 2*np.sqrt(y_eval_err[:,i,0]), \n",
    "                        alpha=0.2, color = cmap(i/n_freq))\n",
    "        \n",
    "        axis1.plot(x_eval, y_eval[:,i,0], color = cmap(i/n_freq))\n",
    "        axis1.plot(x_train, y_train[:,i,0], color = cmap(i/n_freq), linestyle = ' ', marker = 'o')\n",
    "        axis0.plot(x_train, y_train[:,i,0], color = cmap(i/n_freq), linestyle = ' ', marker = 'o')\n",
    "\n",
    "        axis2.fill_between(x_eval, -(y_eval[:,i,1] - 2*np.sqrt(y_eval_err[:,i,1])), -(y_eval[:,i,1] + 2*np.sqrt(y_eval_err[:,i,1])), \n",
    "                        alpha=0.2, color = cmap(i/n_freq))\n",
    "        \n",
    "        axis3.plot(x_eval, -y_eval[:,i,1], color = cmap(i/n_freq))\n",
    "        axis3.plot(x_train, -y_train[:,i,1], color = cmap(i/n_freq), linestyle = ' ', marker = 'o')\n",
    "        axis2.plot(x_train, -y_train[:,i,1], color = cmap(i/n_freq), linestyle = ' ', marker = 'o')\n",
    "    if not LOG_FLAG:\n",
    "        axis0.set_yscale('log')\n",
    "        axis1.set_yscale('log')\n",
    "        axis2.set_yscale('log')\n",
    "        axis3.set_yscale('log')\n",
    "\n",
    "    axis1.sharey(axis0)\n",
    "    axis3.sharey(axis2)\n",
    "    axis0.set_xlabel('x')\n",
    "    axis0.set_ylabel('y')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPR EIS Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:   \n",
    "    cluster_id = None\n",
    "    if cluster_id is None:\n",
    "        x_train = x_train_full\n",
    "        y_train = y_train_full\n",
    "        x_eval = x_eval_full\n",
    "        y_eval = y_eval_full\n",
    "        y_eval_err = y_eval_err_full\n",
    "\n",
    "    else:\n",
    "        x_train = x_train_full[train_mask_list[cluster_id]]\n",
    "        y_train = y_train_full[train_mask_list[cluster_id],:,:]\n",
    "        x_eval = x_eval_full[eval_mask_list[cluster_id]]\n",
    "        y_eval = y_eval_full[eval_mask_list[cluster_id],:,:]\n",
    "        y_eval_err = y_eval_err_full[eval_mask_list[cluster_id],:,:]\n",
    "\n",
    "        # Scaler_Y_real   = StandardScaler()\n",
    "        # Scaler_Y_imag   = StandardScaler()\n",
    "\n",
    "        # if NORM_Y_FLAG:\n",
    "        #     y_train[:,:,0] = Scaler_Y_real.fit_transform(y_train[:,:,0])\n",
    "        #     y_train[:,:,1] = Scaler_Y_imag.fit_transform(y_train[:,:,1])\n",
    "        #     y_eval[:,:,0] = Scaler_Y_real.transform(y_eval[:,:,0])\n",
    "        #     y_eval[:,:,1] = Scaler_Y_imag.transform(y_eval[:,:,1])\n",
    "            \n",
    "\n",
    "\n",
    "    n_freq = np.shape(y_eval)[0]\n",
    "    fig = plt.figure(figsize=(8,8))\n",
    "    cmap = plt.colormaps.get_cmap('rainbow_r')\n",
    "    axis0 = fig.add_subplot(2,3,1)\n",
    "    axis1 = fig.add_subplot(2,3,2)\n",
    "    axis2 = fig.add_subplot(2,3,3, projection = '3d')\n",
    "    axis3 = fig.add_subplot(2,3,4)\n",
    "    axis4 = fig.add_subplot(2,3,5)\n",
    "    axis5 = fig.add_subplot(2,3,6, projection = '3d')\n",
    "\n",
    "\n",
    "    init_elev = 40  # 仰角\n",
    "    init_azim = 45  # 方位角\n",
    "    init_roll = 0\n",
    "    axis2.view_init(elev=init_elev, azim=init_azim, roll=init_roll)\n",
    "    axis5.view_init(elev=init_elev, azim=init_azim, roll=init_roll)\n",
    "\n",
    "\n",
    "    new_f = chData[0,0,:].take(run_list, axis=0)\n",
    "\n",
    "    if LOG_FLAG:\n",
    "        \n",
    "        y_EIS_train =   np.exp(y_train[:,:,0] + 1j * y_train[:,:,1])\n",
    "        y_EIS_eval = np.exp(y_eval[:,:,0] + 1j * y_eval[:,:,1])\n",
    "\n",
    "        # f_plot = range(40,80)\n",
    "        f_plot = range(np.shape(new_f)[0])\n",
    "        y_EIS_train = y_EIS_train.take(f_plot, axis=1)\n",
    "        y_EIS_eval = y_EIS_eval.take(f_plot, axis=1)\n",
    "        new_f = new_f.take(f_plot, axis=0)\n",
    "\n",
    "        \n",
    "        for i in range(np.shape(x_eval)[0]):\n",
    "            axis0.fill_between(new_f, y_eval[i,:,0] - 2*np.sqrt(y_eval_err[i,:,0]), y_eval[i,:,0] + 2*np.sqrt(y_eval_err[i,:,0]), \n",
    "                    alpha=0.2, color = cmap(i/np.shape(x_eval)[0]))\n",
    "            axis3.fill_between(new_f, y_eval[i,:,1] - 2*np.sqrt(y_eval_err[i,:,1]), y_eval[i,:,1] + 2*np.sqrt(y_eval_err[i,:,1]), \n",
    "                    alpha=0.2, color = cmap(i/np.shape(x_eval)[0]))\n",
    "        axis0.set_xscale('log')\n",
    "        axis0.set_yscale('log')\n",
    "        axis3.set_xscale('log')\n",
    "        # axis3.set_yscale('log')\n",
    "        \n",
    "\n",
    "    else:\n",
    "        y_EIS_train = y_train[:,:,0] + 1j*y_train[:,:,1]\n",
    "        y_EIS_eval = y_eval[:,:,0] + 1j*y_eval[:,:,1]\n",
    "\n",
    "        # f_plot = range(40,60)\n",
    "        f_plot = range(np.shape(new_f)[0])\n",
    "        y_EIS_train = y_EIS_train.take(f_plot, axis=1)\n",
    "        y_EIS_eval = y_EIS_eval.take(f_plot, axis=1)\n",
    "        new_f = new_f.take(f_plot, axis=0)\n",
    "\n",
    "        for i in range(np.shape(x_eval)[0]):\n",
    "            axis0.loglog(new_f, (np.real(y_EIS_eval[i,:])), color = cmap(i/np.shape(x_eval)[0]))\n",
    "            axis3.loglog(new_f, (-np.imag(y_EIS_eval[i,:])), color = cmap(i/np.shape(x_eval)[0]))\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "    for i in range(np.shape(x_eval)[0]):\n",
    "        axis1.semilogx(new_f, np.log10(np.abs(y_EIS_eval[i,:])), color = cmap(i/np.shape(x_eval)[0]))\n",
    "        axis4.semilogx(new_f, np.rad2deg(np.angle(y_EIS_eval[i,:])), color = cmap(i/np.shape(x_eval)[0]))\n",
    "        \n",
    "        \n",
    "    for i in range(np.shape(x_train)[0]):\n",
    "        axis1.semilogx(new_f, np.log10(np.abs(y_EIS_train[i,:])), 'black', alpha = 0.3)\n",
    "        axis4.semilogx(new_f, np.rad2deg(np.angle(y_EIS_train[i,:])), 'black', alpha = 0.3)\n",
    "    \n",
    "\n",
    "\n",
    "    _x = np.arange(np.shape(x_eval)[0])\n",
    "    _y = np.log10(new_f).flatten()\n",
    "    X, Y = np.meshgrid(_x, _y, indexing='ij')\n",
    "    axis2.plot_surface(X, Y, np.log10(np.abs(y_EIS_eval[:,:])), cmap='viridis_r', alpha=0.8)\n",
    "    axis5.plot_surface(X, Y, -np.rad2deg(np.angle(y_EIS_eval[:,:])), cmap='viridis_r', alpha=0.8)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EISNN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
