{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import Interpolation\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "\n",
    "from loguru import logger\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gatherCSV(rootPath, outsuffix = 'Tracking'):\n",
    "    '''==================================================\n",
    "        Collect all EIS.csv files in the rootPath\n",
    "        Parameter: \n",
    "            rootPath: current search path\n",
    "            outsuffix: Saving path of EIS.csv files\n",
    "        Returen:\n",
    "            EISDict: a 2D-dict of EIS data\n",
    "            Storage Frame: EISDict[_sessionIndex][_channelIndex] = \"_filepath\"\n",
    "        ==================================================\n",
    "    '''\n",
    "    _filename       = None\n",
    "    _filepath       = None\n",
    "    _trackpath      = None\n",
    "    _csvpath        = None\n",
    "    _sessionIndex   = None\n",
    "    _channelIndex   = None\n",
    "    _processed      = None\n",
    "\n",
    "    EISDict = defaultdict(dict)\n",
    "\n",
    "    ## Iterate session\n",
    "    session_pattern = re.compile(r\"(.+?)_(\\d{8})_01\")\n",
    "    bank_pattern    = re.compile(r\"([1-4])\")\n",
    "    file_pattern    = re.compile(r\"EIS_ch(\\d{3})\\.csv\")\n",
    "\n",
    "    ## RootDir\n",
    "    for i in os.listdir(rootPath):\n",
    "        match_session = session_pattern.match(i)\n",
    "        ## SessionDir\n",
    "        if match_session:\n",
    "            logger.info(f\"Session Begin: {i}\")\n",
    "            _sessionIndex = match_session[2]\n",
    "            for j in os.listdir(f\"{rootPath}/{i}\"):\n",
    "                match_bank = bank_pattern.match(j)\n",
    "                ## BankDir\n",
    "                if match_bank:\n",
    "                    logger.info(f\"Bank Begin: {j}\")\n",
    "                    _trackpath = f\"{rootPath}/{i}/{j}/{outsuffix}\"\n",
    "                    if not os.path.exists(_trackpath):\n",
    "                        continue\n",
    "\n",
    "                    for k in os.listdir(f\"{rootPath}/{i}/{j}/{outsuffix}\"):\n",
    "                        match_file = file_pattern.match(k)\n",
    "                        ## File\n",
    "                        if match_file:\n",
    "                            _filename = k\n",
    "                            _filepath = f\"{rootPath}/{i}/{j}/{outsuffix}/{k}\"\n",
    "                            _channelIndex = (int(match_bank[1])-1)*32+int(match_file[1])\n",
    "                            \n",
    "                            EISDict[_sessionIndex][_channelIndex] = f\"{rootPath}/{i}/{j}/{outsuffix}/{k}\"\n",
    "                            \n",
    "    return EISDict\n",
    "\n",
    "# Data Readout\n",
    "def readChannel(chID, fileDict):\n",
    "    '''==================================================\n",
    "        Read EIS.csv file by Channel\n",
    "        Parameter: \n",
    "            chID: channel index\n",
    "            fileDict: EISDict[_sessionIndex][_channelIndex] = \"_filepath\"\n",
    "        Returen:\n",
    "            freq: frequency\n",
    "            Zreal: real part of impedance\n",
    "            Zimag: imaginary part of impedance\n",
    "        ==================================================\n",
    "    '''\n",
    "    chData = []\n",
    "    for ssID in fileDict.keys():\n",
    "        _data   = np.loadtxt(fileDict[ssID][chID], delimiter=',')\n",
    "        _freq   = _data[:,0]\n",
    "        _Zreal  = _data[:,3]\n",
    "        _Zimag  = _data[:,4]\n",
    "        chData.append(np.stack((_freq, _Zreal, _Zimag),axis=0))\n",
    "\n",
    "    return np.stack(chData, axis=0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def EIS_recal_ver02(data, _phz_0 = None):\n",
    "    f_poi = data[0,:]\n",
    "    # Z_poi = data[1,:] * np.exp(1j*np.deg2rad(data[2,:]))\n",
    "    Z_poi = data[1,:] + 1j*data[2,:]\n",
    "    Y_poi = 1/Z_poi\n",
    "\n",
    "    Rg0 = 1.611e13\n",
    "    Cp0 = 1.4e-9\n",
    "    \n",
    "    _Rg0_rescale = 1/Rg0*np.power(f_poi,1.583)\n",
    "    _Cp0_rescale = Cp0*np.power(f_poi,0.911)\n",
    "    Y_org = Y_poi - _Rg0_rescale + 1j*_Cp0_rescale\n",
    "    # Y_org = Y_poi - _Rg0_rescale \n",
    "    # Y_org = Y_poi + 1j*_Cp0_rescale\n",
    "    # Y_org = Y_poi\n",
    "    Z_org = 1/Y_org\n",
    "\n",
    "    # Phz Calibration\n",
    "    if _phz_0 is None:\n",
    "        _phz_0 = np.loadtxt(\"./phz_Calib.txt\")\n",
    "    \n",
    "    Z_ampC = np.abs(Z_org)\n",
    "    # Z_phzC = np.angle(Z_org) - _phz_0\n",
    "    Z_phzC = np.angle(Z_org) - _phz_0\n",
    "\n",
    "    Z_rec = Z_ampC * np.exp(1j*Z_phzC)\n",
    "\n",
    "    # C = 5e-10\n",
    "    Rs0 = 100\n",
    "    Z_rec = Z_rec - Rs0\n",
    "\n",
    "\n",
    "\n",
    "    Cp0 = 5e-10\n",
    "    _Cp0_rescale = Cp0 * f_poi\n",
    "    Z_rec = 1/(1/Z_rec - 1j * _Cp0_rescale)\n",
    "\n",
    "    \n",
    "\n",
    "    # Ls0 = 1.7e-4\n",
    "    Ls0 = 5e-4\n",
    "    _Ls0_rescale = Ls0 * f_poi\n",
    "    Z_rec = Z_rec - 1j * _Ls0_rescale\n",
    "\n",
    "    # C = 5e-10\n",
    "    Rs0 = 566\n",
    "    Z_rec = Z_rec - Rs0\n",
    "    \n",
    "    return np.stack([f_poi, np.real(Z_rec), np.imag(Z_rec)], axis=1).T\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loader Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-04-05 19:00:03.842\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mSession Begin: 02017384_20241211_01\u001b[0m\n",
      "\u001b[32m2025-04-05 19:00:03.843\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 1\u001b[0m\n",
      "\u001b[32m2025-04-05 19:00:03.844\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 2\u001b[0m\n",
      "\u001b[32m2025-04-05 19:00:03.846\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 3\u001b[0m\n",
      "\u001b[32m2025-04-05 19:00:03.847\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 4\u001b[0m\n",
      "\u001b[32m2025-04-05 19:00:03.848\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mSession Begin: 02017384_20241212_01\u001b[0m\n",
      "\u001b[32m2025-04-05 19:00:03.849\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 1\u001b[0m\n",
      "\u001b[32m2025-04-05 19:00:03.849\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 2\u001b[0m\n",
      "\u001b[32m2025-04-05 19:00:03.850\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 3\u001b[0m\n",
      "\u001b[32m2025-04-05 19:00:03.852\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 4\u001b[0m\n",
      "\u001b[32m2025-04-05 19:00:03.853\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mSession Begin: 02017384_20241213_01\u001b[0m\n",
      "\u001b[32m2025-04-05 19:00:03.854\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 1\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-04-05 19:00:03.855\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 2\u001b[0m\n",
      "\u001b[32m2025-04-05 19:00:03.856\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 3\u001b[0m\n",
      "\u001b[32m2025-04-05 19:00:03.857\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 4\u001b[0m\n",
      "\u001b[32m2025-04-05 19:00:03.858\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mSession Begin: 02017384_20241214_01\u001b[0m\n",
      "\u001b[32m2025-04-05 19:00:03.859\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 1\u001b[0m\n",
      "\u001b[32m2025-04-05 19:00:03.860\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 2\u001b[0m\n",
      "\u001b[32m2025-04-05 19:00:03.861\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 3\u001b[0m\n",
      "\u001b[32m2025-04-05 19:00:03.862\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 4\u001b[0m\n",
      "\u001b[32m2025-04-05 19:00:03.863\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mSession Begin: 02017384_20241215_01\u001b[0m\n",
      "\u001b[32m2025-04-05 19:00:03.864\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 1\u001b[0m\n",
      "\u001b[32m2025-04-05 19:00:03.865\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 2\u001b[0m\n",
      "\u001b[32m2025-04-05 19:00:03.866\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 3\u001b[0m\n",
      "\u001b[32m2025-04-05 19:00:03.867\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 4\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(5, 3, 101)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rootPath = \"D:/Baihm/EISNN/Archive/01037160_归档\"   # Default\n",
    "# ch_id = 20  # Normal to Short, Same to GPR  \n",
    "\n",
    "# rootPath = \"D:/Baihm/EISNN/Archive/01037161_归档\"   # 1\n",
    "# ch_id = 10  # Error AP\n",
    "\n",
    "# rootPath = \"D:/Baihm/EISNN/Archive/01067094_归档\"   # #4\n",
    "# ch_id = 89  # Error\n",
    "\n",
    "\n",
    "rootPath = \"D:/Baihm/EISNN/Archive/02017384_归档\"   # #6\n",
    "ch_id = 2  # Error length\n",
    "\n",
    "\n",
    "# rootPath = \"D:\\Baihm\\EISNN\\Archive/11207147_归档\"   # 3 Point\n",
    "# ch_id = 0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# \n",
    "freq_list = np.linspace(0,5000-1,101,dtype=int, endpoint=True)\n",
    "freq_list_DTW = np.linspace(1000,5000-1,101,dtype=int, endpoint=True)\n",
    "EISDict = gatherCSV(rootPath)\n",
    "chData = readChannel(ch_id, EISDict)\n",
    "\n",
    "chData_DTW = chData[:,:,freq_list_DTW]\n",
    "\n",
    "if True:\n",
    "    phz_calibration = np.loadtxt(\"./phz_Calib.txt\")\n",
    "    for i in range(np.shape(chData)[0]):\n",
    "        ch_eis = EIS_recal_ver02(chData[i,:,:], phz_calibration)\n",
    "        chData[i,:,:] = ch_eis\n",
    "\n",
    "# chData = chData[:,:,91:100]\n",
    "chData = chData[:,:,freq_list]\n",
    "\n",
    "\n",
    "np.shape(chData)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DTW Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "from  Outlier import OutlierDetection\n",
    "\n",
    "CLEAN_FLAG = True\n",
    "if CLEAN_FLAG:\n",
    "    eis_seq, eis_cluster, eis_anomaly, leaf_anomaly = OutlierDetection.OutlierDetection(chData_DTW)\n",
    "else: \n",
    "    eis_seq = np.arange(np.shape(chData)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPR - Interpolation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "from loguru import logger\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "import gpytorch\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def piecewise_interp(x_day_all, chData, eis_seq, run_list = None, eis_cluster = None, SPEED_RATE = 1, LOG_FLAG = True):\n",
    "    '''==================================================\n",
    "        Seperate the EIS data into clusters for interpolation\n",
    "        Parameter: \n",
    "            chData: EIS data #sample x #freq x2\n",
    "            eis_seq: Index of valid data #valid_sample x 1\n",
    "            run_list: Index of valid freq #valid_freq x 1, default None for all freq valid\n",
    "            eis_cluster: cluster label for each data in eis_seq #valid_sample x 1, default None for all data in one cluster\n",
    "            SPEED_RATE: Speed of interpolation, default 1 for 1x interpolation\n",
    "            LOG_FLAG: Logarithm flag, default True for log scale\n",
    "        Returen:\n",
    "            x_train_full: x_train data #train x 1\n",
    "            y_train_full: y_train data #train x #freq x2\n",
    "            x_eval_full: x_eval data #eval x 1\n",
    "            n_clusters: number of clusters #uniq_clusters x 1\n",
    "            train_mask_list: mask for each cluster in train data\n",
    "            eval_mask_list: mask for each cluster in eval data\n",
    "            eis_cluster_eval: cluster label for each data in x_eval_full\n",
    "        ==================================================\n",
    "    '''\n",
    "    # Init xy according to the datetime\n",
    "\n",
    "    x_day = [x_day_all[i] for i in eis_seq]\n",
    "\n",
    "    x_train_full = np.array([(poi - x_day[0]).days for poi in x_day])\n",
    "    x_eval_full = np.linspace(0,max(x_train_full),max(x_train_full)*SPEED_RATE+1)\n",
    "\n",
    "    y_train_full = np.stack([chData[eis_seq,1,:],chData[eis_seq,2,:]], axis=2)\n",
    "    \n",
    "    if run_list is not None:\n",
    "        y_train_full = y_train_full.take(run_list, axis=1)\n",
    "\n",
    "\n",
    "    if LOG_FLAG:\n",
    "        y_train_log = np.log(y_train_full[:,:,0] + 1j*y_train_full[:,:,1])\n",
    "        y_train_full = np.stack([y_train_log.real, y_train_log.imag], axis=2)\n",
    "\n",
    "\n",
    "    # Segmentation of clusters\n",
    "    if eis_cluster is None:\n",
    "        eis_cluster = np.zeros_like(eis_seq)\n",
    "    unique_clusters = np.unique(eis_cluster)\n",
    "    n_clusters = len(unique_clusters)\n",
    "\n",
    "    train_mask_list = []\n",
    "    eval_mask_list = []\n",
    "    eis_cluster_eval = np.zeros_like(x_eval_full)\n",
    "    for i in range(n_clusters):\n",
    "        # 取当前状态和下一个状态的数据\n",
    "        train_mask = (eis_cluster == unique_clusters[i])\n",
    "        if i == n_clusters - 1:\n",
    "            x_eval_end = x_eval_full.max() + 1\n",
    "        else:\n",
    "            x_eval_end = x_train_full[(eis_cluster == unique_clusters[i+1])].min()\n",
    "        # x_state = x_train[state_mask]\n",
    "        # y_state = y_train[:,state_mask]\n",
    "\n",
    "        eval_mask = (x_eval_full >= x_train_full[train_mask].min()) & (x_eval_full < x_eval_end)\n",
    "        \n",
    "        train_mask_list.append(train_mask)\n",
    "        eval_mask_list.append(eval_mask)\n",
    "        eis_cluster_eval[eval_mask] = unique_clusters[i]\n",
    "\n",
    "    return x_train_full, y_train_full, x_eval_full, n_clusters, train_mask_list, eval_mask_list, eis_cluster_eval\n",
    "    \n",
    "\n",
    "def GPDataLoader(x_train, y_train, x_eval, NORM_X_FLAG = True, NORM_Y_FLAG = True):\n",
    "    '''==================================================\n",
    "        Regularize the data for GP\n",
    "        Parameter: \n",
    "            x_train: x_train data #train x 1\n",
    "            y_train: y_train data #train x #freq x2\n",
    "            x_eval: x_eval data #eval x 1\n",
    "            NORM_X_FLAG: Normalization flag for x_train, default True\n",
    "            NORM_Y_FLAG: Normalization flag for y_train, default True\n",
    "        Returen:\n",
    "            x_train: regularized x_train data\n",
    "            y_train: regularized y_train data\n",
    "            x_eval: regularized x_eval data\n",
    "            ScalerSet: ScalerSet[0]: x_train scaler, ScalerSet[1]: y_train real scaler, ScalerSet[2]: y_train imag scaler\n",
    "        ==================================================\n",
    "    '''\n",
    "    Scaler_X        = StandardScaler()\n",
    "    Scaler_Y_real   = StandardScaler()\n",
    "    Scaler_Y_imag   = StandardScaler()\n",
    "\n",
    "    if NORM_Y_FLAG:\n",
    "        y_train[:,:,0] = Scaler_Y_real.fit_transform(y_train[:,:,0])\n",
    "        y_train[:,:,1] = Scaler_Y_imag.fit_transform(y_train[:,:,1])\n",
    "    if NORM_X_FLAG:\n",
    "        x_train = Scaler_X.fit_transform(x_train.reshape(-1, 1)).flatten()\n",
    "        x_eval = Scaler_X.transform(x_eval.reshape(-1, 1)).flatten()\n",
    "\n",
    "    \n",
    "    y_train = np.hstack((y_train[:,:,0], y_train[:,:,1]))\n",
    "\n",
    "    logger.info(f\"\\nx: {np.shape(x_train)} \\ny: {np.shape(y_train)} \\nx_pred{np.shape(x_eval)}\")\n",
    "\n",
    "    return x_train, y_train, x_eval, [Scaler_X, Scaler_Y_real, Scaler_Y_imag]\n",
    "\n",
    "\n",
    "def GPDataExporter(x_train, y_train, x_eval, y_eval_mean, y_eval_var, ScalerSet, NORM_X_FLAG, NORM_Y_FLAG):\n",
    "    '''==================================================\n",
    "        Deregularize the data for Saving\n",
    "        Parameter: \n",
    "            x_train: regularized x_train data\n",
    "            y_train: regularized y_train data\n",
    "            x_eval: regularized x_eval data\n",
    "            y_eval_mean: mean from GPR #eval x #freq x2\n",
    "            y_eval_var: var from GPR #eval x #freq x2\n",
    "            ScalerSet: ScalerSet[0]: x_train scaler, ScalerSet[1]: y_train real scaler, ScalerSet[2]: y_train imag scaler\n",
    "            NORM_X_FLAG: Normalization flag for x_train, default True\n",
    "            NORM_Y_FLAG: Normalization flag for y_train, default True\n",
    "        Returen:\n",
    "            x_train: deregularized x_train data\n",
    "            y_train: dederegularized y_train data\n",
    "            x_eval: dederegularized x_eval data\n",
    "            y_eval: dederegularized y_eval_mean\n",
    "            y_eval_err: dederegularized y_eval_var\n",
    "        ==================================================\n",
    "    '''\n",
    "    n_freq = np.shape(y_train)[1]//2 \n",
    "    y_train = np.stack((y_train[:,:n_freq], y_train[:,n_freq:]), axis=2)\n",
    "    y_eval_mean = np.stack((y_eval_mean[:,:n_freq], y_eval_mean[:,n_freq:]), axis=2)\n",
    "    y_eval_var = np.stack((y_eval_var[:,:n_freq], y_eval_var[:,n_freq:]), axis=2)\n",
    "    if NORM_X_FLAG:\n",
    "        x_train = ScalerSet[0].inverse_transform(x_train.reshape(-1, 1)).flatten()\n",
    "        x_eval = ScalerSet[0].inverse_transform(x_eval.reshape(-1, 1)).flatten()\n",
    "    \n",
    "    if NORM_Y_FLAG:\n",
    "        y_train_real = ScalerSet[1].inverse_transform(y_train[:,:,0])\n",
    "        y_train_imag = ScalerSet[2].inverse_transform(y_train[:,:,1])\n",
    "        \n",
    "        y_eval_mean_real = ScalerSet[1].inverse_transform(y_eval_mean[:,:,0])\n",
    "        y_eval_mean_imag = ScalerSet[2].inverse_transform(y_eval_mean[:,:,1])\n",
    "\n",
    "        y_eval_var_real = y_eval_var[:,:,0] * ScalerSet[1].var_\n",
    "        y_eval_var_imag = y_eval_var[:,:,1] * ScalerSet[2].var_\n",
    "    else:\n",
    "        y_train_real = y_train[:,:,0]\n",
    "        y_train_imag = y_train[:,:,1]\n",
    "\n",
    "        y_eval_mean_real = y_eval_mean[:,:,0]\n",
    "        y_eval_mean_imag = y_eval_mean[:,:,1]\n",
    "\n",
    "        y_eval_var_real = y_eval_var[:,:,0]\n",
    "        y_eval_var_imag = y_eval_var[:,:,1]\n",
    "\n",
    "    y_train = np.stack([y_train_real, y_train_imag], axis=2)\n",
    "    y_eval = np.stack([y_eval_mean_real, y_eval_mean_imag], axis=2)\n",
    "    y_eval_err = np.stack([y_eval_var_real, y_eval_var_imag], axis=2)\n",
    "    \n",
    "    logger.info(f\"\\nx: {np.shape(x_train)} \\ny: {np.shape(y_train)} \\nx_pred{np.shape(x_eval)} \\ny_pred{np.shape(y_eval)} \\ny_pred{np.shape(y_eval_err)}\")\n",
    "\n",
    "    return x_train, y_train, x_eval, y_eval, y_eval_err\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EISGPR Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EISGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood, num_tasks):\n",
    "        super().__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.MultitaskMean(\n",
    "            gpytorch.means.ConstantMean(), num_tasks=num_tasks\n",
    "        )\n",
    "        self.covar_module = gpytorch.kernels.MultitaskKernel(\n",
    "            # gpytorch.kernels.RBFKernel(),\n",
    "            # gpytorch.kernels.RQKernel(),\n",
    "            # gpytorch.kernels.LinearKernel(),\n",
    "            # gpytorch.kernels.PolynomialKernel(power=3.0),\n",
    "            # gpytorch.kernels.PiecewisePolynomialKernel(),\n",
    "            # gpytorch.kernels.SpectralMixtureKernel(num_mixtures=3),\n",
    "            # gpytorch.kernels.CosineKernel(),\n",
    "            \n",
    "            gpytorch.kernels.MaternKernel(nu=0.5), \n",
    "            \n",
    "            num_tasks=num_tasks, \n",
    "            rank=2\n",
    "        )\n",
    "        # self.covar_module.data_covar_module.lengthscale = 1\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultitaskMultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "\n",
    "def EISGPTrain(x_train, y_train, x_eval, cluster_id, device, training_iter = 200, lr = 0.05):\n",
    "    num_tasks = y_train.shape[1]\n",
    "    # Initialize likelihood and model\n",
    "    likelihood = gpytorch.likelihoods.MultitaskGaussianLikelihood(\n",
    "        num_tasks=num_tasks, rank = 0).to(device)\n",
    "    model = EISGPModel(x_train, y_train, likelihood, num_tasks).to(device)\n",
    "\n",
    "    # Find optimal model hyperparameters\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)  # Includes GaussianLikelihood parameters\n",
    "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "    # Iteration begin\n",
    "    loss_inst       = []\n",
    "    length_inst     = []\n",
    "    noise_inst      = []\n",
    "    for i in range(training_iter):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(x_train)\n",
    "        loss = -mll(output, y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        poi_noise   = model.likelihood.noise.detach().cpu().numpy()\n",
    "        poi_length  = model.covar_module.data_covar_module.lengthscale.detach().cpu().numpy()\n",
    "        # poi_length  = 0\n",
    "        \n",
    "        loss_inst.append(loss.item())\n",
    "        noise_inst.append(poi_noise)\n",
    "        length_inst.append(poi_length)\n",
    "        if not (i+1)%100:\n",
    "            logger.info(f\"C{cluster_id} - Iter {i+1}/{training_iter}\\tLoss: {loss.item()}\")\n",
    "            \n",
    "    # logger.info(\"Model Training Finished.\")\n",
    "\n",
    "    # Make predictions\n",
    "    model.eval()\n",
    "    likelihood.eval()\n",
    "    with torch.no_grad(), gpytorch.settings.cholesky_jitter(1e-4):\n",
    "        pred = likelihood(model(x_eval))\n",
    "\n",
    "    return [pred, np.array(loss_inst), np.array(length_inst), np.array(noise_inst)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Piecewise GPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PiecewiseGPR(x_day_full, chData, eis_seq, eis_cluster = None, SPEED_RATE = 1, training_iter = 200, lr = 0.05):\n",
    "    '''==================================================\n",
    "        Piecewise GPR for EIS data\n",
    "        Parameter: \n",
    "            chData: EIS data #sample x #freq x2\n",
    "            eis_seq: Index of valid data #valid_sample x 1\n",
    "            eis_cluster: cluster label for each data in eis_seq #valid_sample x 1, default None for all data in one cluster\n",
    "            SPEED_RATE: Speed of interpolation, default 1 for 1x interpolation\n",
    "            training_iter: Number of training iterations, default 200\n",
    "            lr: Learning rate, default 0.05\n",
    "        Returen:\n",
    "            y_eval_mean: mean from GPR #eval x #freq x2\n",
    "            y_eval_var: var from GPR #eval x #freq x2\n",
    "        ==================================================\n",
    "    '''\n",
    "    # Regularization Type\n",
    "    LOG_FLAG=True\n",
    "    NORM_X_FLAG=True\n",
    "    NORM_Y_FLAG=True\n",
    "\n",
    "    x_train_full, y_train_full, x_eval_full,  n_clusters, train_mask_list, eval_mask_list, eis_cluster_eval = \\\n",
    "        piecewise_interp(x_day_full, chData, eis_seq, None, \n",
    "                     eis_cluster = eis_cluster, \n",
    "                     SPEED_RATE = SPEED_RATE, \n",
    "                     LOG_FLAG=LOG_FLAG)\n",
    "\n",
    "\n",
    "    y_eval_full = np.zeros((np.shape(x_eval_full)[0], np.shape(y_train_full)[1], 2))\n",
    "    y_eval_err_full = np.zeros((np.shape(x_eval_full)[0], np.shape(y_train_full)[1], 2))\n",
    "\n",
    "\n",
    "\n",
    "    for i in range(n_clusters):\n",
    "    # for i in [1]:\n",
    "\n",
    "        x_train = x_train_full[train_mask_list[i]]\n",
    "        y_train = y_train_full[train_mask_list[i],:,:]\n",
    "        x_eval = x_eval_full[eval_mask_list[i]]\n",
    "\n",
    "        x_train, y_train, x_eval, ScalerSet = \\\n",
    "            GPDataLoader(x_train, y_train, x_eval, \n",
    "                NORM_X_FLAG=NORM_X_FLAG, NORM_Y_FLAG=NORM_Y_FLAG)\n",
    "\n",
    "\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        x_train_tensor = torch.from_numpy(x_train).float().to(device)\n",
    "        x_eval_tensor = torch.from_numpy(x_eval).float().to(device)\n",
    "        y_train_tensor = torch.from_numpy(y_train).float().to(device)\n",
    "\n",
    "\n",
    "        y_eval_tensor, _, _, _ = EISGPTrain(x_train_tensor, y_train_tensor, x_eval_tensor, \n",
    "                cluster_id = i, device = device, \n",
    "                training_iter=training_iter, lr=lr)\n",
    "        y_eval_mean = y_eval_tensor.mean.cpu().numpy()\n",
    "        y_eval_var = y_eval_tensor.variance.detach().cpu().numpy()\n",
    "\n",
    "\n",
    "        x_train, y_train, x_eval, y_eval, y_eval_err = \\\n",
    "                GPDataExporter(x_train, y_train, x_eval, y_eval_mean, y_eval_var, ScalerSet,\n",
    "                            NORM_X_FLAG=NORM_X_FLAG, NORM_Y_FLAG=NORM_Y_FLAG)\n",
    "            \n",
    "        y_eval_full[eval_mask_list[i],:,:] = y_eval\n",
    "        y_eval_err_full[eval_mask_list[i],:,:] = y_eval_err\n",
    "\n",
    "\n",
    "    return x_train_full, y_train_full, x_eval_full, y_eval_full, y_eval_err_full, eis_cluster_eval\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPR Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EISPreprocessPlot(fig, chData, x_train, y_train, x_eval, y_eval, y_eval_err, eis_seq, eis_cluster, eis_anomaly):\n",
    "    '''==================================================\n",
    "        Plot summary for EIS data preprocessing\n",
    "        Parameter: \n",
    "            fig: figure object\n",
    "            chData: EIS data of one channel\n",
    "            x_train: x_train data\n",
    "            y_train: y_train data\n",
    "            x_eval: x_eval data\n",
    "            y_eval: mean from GPR\n",
    "            y_eval_err: var from GPR \n",
    "            eis_seq: Index of valid data\n",
    "            eis_cluster: cluster label for each data in eis_seq\n",
    "            eis_anomaly: Index of outlier data\n",
    "        ==================================================\n",
    "    '''\n",
    "    axis = [0] * 12\n",
    "    axis[0] = fig.add_subplot(3,4,1)   \n",
    "    axis[1] = fig.add_subplot(3,4,2)            \n",
    "    axis[2] = fig.add_subplot(3,4,3)         \n",
    "    axis[3] = fig.add_subplot(3,4,4, projection='3d')      \n",
    "    axis[4] = fig.add_subplot(3,4,5)      \n",
    "    axis[5] = fig.add_subplot(3,4,6)         \n",
    "    axis[6] = fig.add_subplot(3,4,7)         \n",
    "    axis[7] = fig.add_subplot(3,4,8, projection='3d')         \n",
    "    axis[8] = fig.add_subplot(3,4,9)         \n",
    "    axis[9] = fig.add_subplot(3,4,10)    \n",
    "    axis[10] = fig.add_subplot(3,4,11)    \n",
    "    # axis[9] = fig.add_subplot(2,5,12)    \n",
    "\n",
    "\n",
    "    axis[0].set_title(\"Origin Amp\")\n",
    "    axis[1].set_title(\"Interpolated mean\")\n",
    "    axis[2].set_title(\"Interpolated var\")\n",
    "    # axis[3].set_title(\"Overview\")\n",
    "    \n",
    "    axis[4].set_title(\"Origin Phase\")\n",
    "    axis[5].set_title(\"Interpolated mean\")\n",
    "    axis[6].set_title(\"Interpolated var\")\n",
    "    # axis[7].set_title(\"Overview\")\n",
    "\n",
    "\n",
    "    axis[8].set_title(\"Cluster\")\n",
    "    axis[9].set_title(\"Outlier\")\n",
    "    axis[10].set_title(\"Trace\")\n",
    "    # axis[11].set_title(\"Nope\")\n",
    "\n",
    "\n",
    "    init_elev = 40  # 仰角\n",
    "    init_azim = 45  # 方位角\n",
    "    axis[3].view_init(elev=init_elev, azim=init_azim)\n",
    "    axis[7].view_init(elev=init_elev, azim=init_azim)\n",
    "\n",
    "    axis[0].set_ylim((5e2, 2e8))\n",
    "    axis[4].set_ylim((-95, 0))\n",
    "\n",
    "    # Origin Amp & Phase\n",
    "    cmap = plt.colormaps.get_cmap('rainbow_r')\n",
    "    for i in range(len(eis_seq)):\n",
    "        _x = eis_seq[i]\n",
    "        ch_eis = chData[_x,:,:]\n",
    "        _color = cmap(_x/np.shape(chData)[0])\n",
    "        axis[0].loglog(ch_eis[0,:], np.abs(ch_eis[1,:]+1j*ch_eis[2,:]), color = _color, linewidth=2, label=f\"S{i:02d}\")\n",
    "        axis[4].semilogx(ch_eis[0,:], np.rad2deg(np.angle(ch_eis[1,:]+1j*ch_eis[2,:])), color = _color, linewidth=2, label=f\"S{i:02d}\")\n",
    "\n",
    "    \n",
    "    # Interpolation Mean\n",
    "    y_EIS_train =   np.exp(y_train[:,:,0] + 1j * y_train[:,:,1])\n",
    "    y_EIS_eval = np.exp(y_eval[:,:,0] + 1j * y_eval[:,:,1])\n",
    "\n",
    "    _freq_poi = chData[0,0,:]\n",
    "    cmap = plt.colormaps.get_cmap('rainbow_r')\n",
    "    for i in range(np.shape(x_eval)[0]):\n",
    "        axis[1].loglog(_freq_poi, (np.abs(y_EIS_eval[i,:])), color = cmap(i/np.shape(x_eval)[0]))\n",
    "        axis[5].semilogx(_freq_poi, np.rad2deg(np.angle(y_EIS_eval[i,:])), color = cmap(i/np.shape(x_eval)[0]))\n",
    "        \n",
    "    for i in range(np.shape(x_train)[0]):\n",
    "        axis[1].semilogx(_freq_poi, (np.abs(y_EIS_train[i,:])), 'black', alpha = 0.3)\n",
    "        axis[5].semilogx(_freq_poi, np.rad2deg(np.angle(y_EIS_train[i,:])), 'black', alpha = 0.3)\n",
    "    \n",
    "    axis[1].sharex(axis[0])\n",
    "    axis[1].sharey(axis[0])\n",
    "    axis[5].sharex(axis[4])\n",
    "    axis[5].sharey(axis[4])\n",
    "\n",
    "    # Interpolation Var\n",
    "    \n",
    "    for i in range(np.shape(x_eval)[0]):\n",
    "        axis[2].fill_between(_freq_poi, np.exp(y_eval[i,:,0] - 2*np.sqrt(y_eval_err[i,:,0])), np.exp(y_eval[i,:,0] + 2*np.sqrt(y_eval_err[i,:,0])), \n",
    "                alpha=0.2, color = cmap(i/np.shape(x_eval)[0]))\n",
    "        axis[6].fill_between(_freq_poi, np.rad2deg(y_eval[i,:,1] - 2*np.sqrt(y_eval_err[i,:,1])), np.rad2deg(y_eval[i,:,1] + 2*np.sqrt(y_eval_err[i,:,1])), \n",
    "                alpha=0.2, color = cmap(i/np.shape(x_eval)[0]))\n",
    "    axis[2].set_xscale('log')\n",
    "    axis[2].set_yscale('log')\n",
    "    axis[6].set_xscale('log')\n",
    "    axis[2].sharex(axis[0])\n",
    "    axis[2].sharey(axis[0])\n",
    "    axis[6].sharex(axis[4])\n",
    "    axis[6].sharey(axis[4])\n",
    "\n",
    "\n",
    "    # Interpolate Plain\n",
    "    _x = np.arange(np.shape(x_eval)[0])\n",
    "    _y = np.log10(_freq_poi).flatten()\n",
    "    X, Y = np.meshgrid(_x, _y, indexing='ij')\n",
    "    axis[3].plot_surface(X, Y, np.log10(np.abs(y_EIS_eval[:,:])), cmap='viridis_r', alpha=0.8)\n",
    "    axis[7].plot_surface(X, Y, -np.rad2deg(np.angle(y_EIS_eval[:,:])), cmap='viridis_r', alpha=0.8)\n",
    "\n",
    "\n",
    "\n",
    "    # Cluster\n",
    "    cmap = plt.colormaps.get_cmap('Set1')\n",
    "    for i in range(len(eis_seq)):\n",
    "        _x = eis_seq[i]\n",
    "        ch_eis = chData[_x,:,:]\n",
    "        _color = cmap(eis_cluster[i])\n",
    "        axis[8].loglog(ch_eis[0,:], np.abs(ch_eis[1,:]+1j*ch_eis[2,:]), color = _color, linewidth=2, label=f\"{chr(ord('A')+eis_cluster[i])}\")\n",
    "        \n",
    "    _legend_handle = []\n",
    "    for i in range(len(np.unique(eis_cluster))):\n",
    "        _legend_handle.append(mpatches.Patch(color = cmap(i), label = f\"{chr(ord('A')+i)}:{len(eis_cluster[eis_cluster==i])}\"))\n",
    "    axis[8].legend(handles=_legend_handle)\n",
    "    axis[8].sharex(axis[0])\n",
    "    axis[8].sharey(axis[0])\n",
    "\n",
    "\n",
    "    # Outlier\n",
    "    cmap = plt.colormaps.get_cmap('rainbow_r')\n",
    "    for i in range(len(eis_anomaly)):\n",
    "        _x = eis_anomaly[i]\n",
    "        ch_eis = chData[_x,:,:]\n",
    "        _color = cmap(_x/np.shape(chData)[0])\n",
    "        axis[9].loglog(ch_eis[0,:], np.abs(ch_eis[1,:]+1j*ch_eis[2,:]), color = _color, linewidth=2, label=f\"S{_x:02d}\")\n",
    "    axis[9].legend()\n",
    "    axis[9].sharex(axis[0])\n",
    "    axis[9].sharey(axis[0])\n",
    "\n",
    "    # Interpolation Trace\n",
    "    cmap = plt.colormaps.get_cmap('viridis_r')\n",
    "    for i in range(np.shape(y_eval)[1]):\n",
    "        axis[10].fill_between(x_eval, y_eval[:,i,0] - 2*np.sqrt(y_eval_err[:,i,0]), y_eval[:,i,0] + 2*np.sqrt(y_eval_err[:,i,0]), \n",
    "                        alpha=0.2, color = cmap(i/np.shape(y_eval)[1]))\n",
    "        \n",
    "        axis[10].plot(x_train, y_train[:,i,0], color = cmap(i/np.shape(y_eval)[1]), linestyle = ' ', marker = 'o')\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-04-05 19:00:04.309\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mGPDataLoader\u001b[0m:\u001b[36m97\u001b[0m - \u001b[1m\n",
      "x: (5,) \n",
      "y: (5, 202) \n",
      "x_pred(9,)\u001b[0m\n",
      "\u001b[32m2025-04-05 19:00:06.692\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mEISGPTrain\u001b[0m:\u001b[36m62\u001b[0m - \u001b[1mC0 - Iter 100/200\tLoss: -0.396728515625\u001b[0m\n",
      "\u001b[32m2025-04-05 19:00:08.839\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mEISGPTrain\u001b[0m:\u001b[36m62\u001b[0m - \u001b[1mC0 - Iter 200/200\tLoss: -0.7807697057723999\u001b[0m\n",
      "\u001b[32m2025-04-05 19:00:08.862\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mGPDataExporter\u001b[0m:\u001b[36m153\u001b[0m - \u001b[1m\n",
      "x: (5,) \n",
      "y: (5, 101, 2) \n",
      "x_pred(9,) \n",
      "y_pred(9, 101, 2) \n",
      "y_pred(9, 101, 2)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "x_day_full = [datetime.strptime(date, '%Y%m%d') for date in EISDict.keys()]\n",
    "    \n",
    "x_train_full, y_train_full, x_eval_full, y_eval_full, y_eval_err_full, eis_cluster_eval = \\\n",
    "    PiecewiseGPR(x_day_full, chData, eis_seq, eis_cluster, SPEED_RATE = 2, training_iter = 200, lr = 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eis_cluster_eval\n",
    "# eis_cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Baihm\\AppData\\Local\\Temp\\3\\ipykernel_8760\\2037515821.py:134: UserWarning: No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n",
      "  axis[9].legend()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.2, 0.5, 'EIE  : 11207147\\nCHID : 002\\nFrom : 2024-12-11\\nTo   : 2024-12-15')"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib qt\n",
    "%gui qt\n",
    "\n",
    "ELE_name = \"11207147\"\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(16, 9), constrained_layout=True)\n",
    "EISPreprocessPlot(fig, chData, x_train_full, y_train_full, x_eval_full, y_eval_full, y_eval_err_full, eis_seq, eis_cluster, eis_anomaly)\n",
    "    \n",
    "axis = fig.add_subplot(3,4,12)\n",
    "axis.axis('off')\n",
    "font_properties = {\n",
    "    'family': 'monospace',  # 固定宽度字体\n",
    "    'size': 14,             # 字体大小\n",
    "    'weight': 'bold'        # 加粗\n",
    "}\n",
    "\n",
    "text = f\"EIE  : {ELE_name}\\nCHID : {ch_id:03d}\\nFrom : {x_day_full[0].strftime('%Y-%m-%d')}\\nTo   : {x_day_full[-1].strftime('%Y-%m-%d')}\"\n",
    "    \n",
    "\n",
    "axis.text(0.2, 0.5, text, fontdict = font_properties, ha='left', va='center')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EISNN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
