{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07804a10",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51a22399",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import gc\n",
    "import sys\n",
    "\n",
    "from loguru import logger\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "# %matplotlib qt\n",
    "%matplotlib qt\n",
    "\n",
    "# Detect device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5889426",
   "metadata": {},
   "source": [
    "# Input Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ab60e9",
   "metadata": {},
   "source": [
    "## Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e8604f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ONLY_SEQ_FLAG = True    \n",
    "READ_RAW_FLAG = False\n",
    "freq_list = np.linspace(0,5000-1,101,dtype=int, endpoint=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0cfa4725",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SearchELE(rootPath, ele_pattern = re.compile(r\"(.+?)_归档\")):\n",
    "    '''==================================================\n",
    "        Search all electrode directories in the rootPath\n",
    "        Parameter: \n",
    "            rootPath: current search path\n",
    "            ele_pattern: electrode dir name patten\n",
    "        Returen:\n",
    "            ele_list: list of electrode directories\n",
    "        ==================================================\n",
    "    '''\n",
    "    ele_list = []\n",
    "    for i in os.listdir(rootPath):\n",
    "        _path = os.path.join(rootPath, i)\n",
    "        if os.path.isdir(_path):\n",
    "            match_ele = ele_pattern.match(i)\n",
    "            if match_ele:\n",
    "                ele_list.append([_path, match_ele.group(1)])\n",
    "            else:\n",
    "                ele_list.extend(SearchELE(_path, ele_pattern))\n",
    "\n",
    "    return ele_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0bf6a19",
   "metadata": {},
   "source": [
    "## ARchive Old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cbe9174a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Blacklist = [\n",
    "    '01067093',     # Not look like EIS\n",
    "    '01067094',     # Connection Error\n",
    "    '02017385',     # Connection Error\n",
    "    '05127177',     # Open to Short\n",
    "    '06047729',     # Open to Short\n",
    "    '06047730',     # Open to Short\n",
    "    '06047731',     # Open to Short\n",
    "    '09207024',     # Connection Error\n",
    "    '10017038',     # Connection Error\n",
    "    '10037050',     # Connection Error\n",
    "    '10047056',     # Connection Error\n",
    "    '10057069',     # Connection Error\n",
    "    '10057083',     # Always Open\n",
    "    '10057084',     # Chaos\n",
    "    '10057087',     # Connection Error\n",
    "    '22017367',     # Connection Error\n",
    "    '22017371',     # Chaos\n",
    "]\n",
    "\n",
    "GrayList = [\n",
    "    '10037051',     # Connection Error\n",
    "    '10037052',     # Connection Error\n",
    "    '10057071',     # Connection Error\n",
    "    '10067077',     # Wired Shape like connection error\n",
    "    '10150201',     # Wired Shape\n",
    "    '10150202',     # Wired Shape\n",
    "    '10150203',     # Wired Shape\n",
    "    '20037515',     # Wired Shape\n",
    "    '20037516',     # Wired Shape\n",
    "    '20037517',     # Wired Shape\n",
    "    '22037378',     # Connection Error\n",
    "    '22037380',     # Connection Error\n",
    "    '22047376',     # Connection Error\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "265ead9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if READ_RAW_FLAG:\n",
    "    rootPath = \"D:/Baihm/EISNN/Archive/\"\n",
    "    ele_list = SearchELE(rootPath, re.compile(r\"(.+?)_归档\"))\n",
    "    n_ele = len(ele_list)\n",
    "    logger.info(f\"Search in {rootPath} and find {n_ele:03d} electrodes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b3a68207",
   "metadata": {},
   "outputs": [],
   "source": [
    "if READ_RAW_FLAG:\n",
    "    DATASET_SUFFIX = \"Outlier_Ver03\"\n",
    "\n",
    "    vitro0_start_list = []\n",
    "    vitro0_start_id_list = []\n",
    "    vitro0_data_list = []\n",
    "    vitro0_id_list = []\n",
    "\n",
    "    n_avaliable = 0\n",
    "\n",
    "    for i in range(n_ele):\n",
    "    # for i in range(3):\n",
    "        fd_pt = os.path.join(ele_list[i][0], DATASET_SUFFIX, f\"{ele_list[i][1]}_{DATASET_SUFFIX}.pt\")\n",
    "        if not os.path.exists(fd_pt):\n",
    "            logger.warning(f\"{fd_pt} does not exist\")\n",
    "            continue\n",
    "        data_pt = torch.load(fd_pt, weights_only=False)\n",
    "        _meta_group = data_pt[\"meta_group\"]\n",
    "        _data_group = data_pt[\"data_group\"]\n",
    "\n",
    "        n_day       = _meta_group[\"n_day\"]\n",
    "        n_ch        = _meta_group[\"n_ch\"]\n",
    "        n_valid_ch  = len(_data_group[\"Channels\"])\n",
    "\n",
    "\n",
    "        logger.info(f\"ELE [{i}/{n_ele}]: {ele_list[i][0]}\")\n",
    "\n",
    "        n_avaliable = n_avaliable + 1\n",
    "\n",
    "\n",
    "\n",
    "        # Iteration by channel\n",
    "        for j in _data_group['Channels']:\n",
    "            _ch_data = _data_group[j][\"chData\"]\n",
    "\n",
    "            if ONLY_SEQ_FLAG:\n",
    "                eis_seq = _data_group[j][\"eis_seq\"]\n",
    "                _ch_data = _ch_data[eis_seq,:,:]\n",
    "\n",
    "\n",
    "            _ch_data_log = np.log(_ch_data[:,1,:] + 1j*_ch_data[:,2,:])\n",
    "            _ch_data[:,1,:] = np.real(_ch_data_log)\n",
    "            _ch_data[:,2,:] = np.imag(_ch_data_log)\n",
    "            if _ch_data.shape[2] == 5000:\n",
    "                _ch_data = np.hstack((_ch_data[:,1,freq_list],_ch_data[:,2,freq_list]))\n",
    "            else:\n",
    "                _ch_data = np.hstack((_ch_data[:,1,:],_ch_data[:,2,:]))\n",
    "            vitro0_data_list.append(_ch_data)\n",
    "            vitro0_start_list.append(_ch_data[0,:])\n",
    "\n",
    "\n",
    "            _ch_id = j\n",
    "\n",
    "            _id = [i, _ch_id] * np.shape(_ch_data)[0]\n",
    "            _id = np.array(_id).reshape(-1,2)\n",
    "            _eis_cluster = _data_group[j]['eis_cluster']\n",
    "            _id = np.hstack((_id, _eis_cluster.reshape(-1,1)))\n",
    "            \n",
    "            vitro0_id_list.append(_id)\n",
    "            vitro0_start_id_list.append(_id[0,:])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    vitro0_data_list = np.vstack(vitro0_data_list)\n",
    "    vitro0_id_list = np.vstack(vitro0_id_list)\n",
    "    vitro0_start_list = np.vstack(vitro0_start_list)\n",
    "    vitro0_start_id_list = np.vstack(vitro0_start_id_list)\n",
    "\n",
    "    vitro0_ele_list = [i[1] for i in ele_list]\n",
    "\n",
    "    logger.info(f\"Total {vitro0_data_list.shape[0]} data points from {n_avaliable} electrodes\")\n",
    "\n",
    "    del data_pt, _meta_group, _data_group, _ch_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1088575",
   "metadata": {},
   "source": [
    "## Archive New"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "615a8c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if READ_RAW_FLAG:\n",
    "    rootPath = \"D:/Baihm/EISNN/Archive_New/\"\n",
    "    ele_list = SearchELE(rootPath,re.compile(r\"(.+?)_归档\"))\n",
    "    n_ele = len(ele_list)\n",
    "    logger.info(f\"Search in {rootPath} and find {n_ele:03d} electrodes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "af096432",
   "metadata": {},
   "outputs": [],
   "source": [
    "if READ_RAW_FLAG:\n",
    "    DATASET_SUFFIX = \"Outlier_Ver02\"\n",
    "\n",
    "    vitro1_start_list = []\n",
    "    vitro1_start_id_list = []\n",
    "    vitro1_data_list = []\n",
    "    vitro1_id_list = []\n",
    "\n",
    "    n_avaliable = 0\n",
    "\n",
    "    for i in range(n_ele):\n",
    "    # for i in range(3):\n",
    "        fd_pt = os.path.join(ele_list[i][0], DATASET_SUFFIX, f\"{ele_list[i][1]}_{DATASET_SUFFIX}.pt\")\n",
    "        if not os.path.exists(fd_pt):\n",
    "            logger.warning(f\"{fd_pt} does not exist\")\n",
    "            continue\n",
    "        data_pt = torch.load(fd_pt, weights_only=False)\n",
    "        _meta_group = data_pt[\"meta_group\"]\n",
    "        _data_group = data_pt[\"data_group\"]\n",
    "\n",
    "        n_day       = _meta_group[\"n_day\"]\n",
    "        n_ch        = _meta_group[\"n_ch\"]\n",
    "        n_valid_ch  = len(_data_group[\"Channels\"])\n",
    "\n",
    "\n",
    "        logger.info(f\"ELE [{i}/{n_ele}]: {ele_list[i][0]}\")\n",
    "\n",
    "        n_avaliable = n_avaliable + 1\n",
    "\n",
    "        # Iteration by channel\n",
    "        for j in _data_group['Channels']:\n",
    "            _ch_data = _data_group[j][\"chData\"]\n",
    "\n",
    "            if ONLY_SEQ_FLAG:\n",
    "                eis_seq = _data_group[j][\"eis_seq\"]\n",
    "                _ch_data = _ch_data[eis_seq,:,:]\n",
    "\n",
    "            _ch_data_log = np.log(_ch_data[:,1,:] + 1j*_ch_data[:,2,:])\n",
    "            _ch_data[:,1,:] = np.real(_ch_data_log)\n",
    "            _ch_data[:,2,:] = np.imag(_ch_data_log)\n",
    "            if _ch_data.shape[2] == 5000:\n",
    "                _ch_data = np.hstack((_ch_data[:,1,freq_list],_ch_data[:,2,freq_list]))\n",
    "            else:\n",
    "                _ch_data = np.hstack((_ch_data[:,1,:],_ch_data[:,2,:]))\n",
    "            vitro1_data_list.append(_ch_data)\n",
    "            vitro1_start_list.append(_ch_data[0,:])\n",
    "\n",
    "\n",
    "            _ch_id = j\n",
    "\n",
    "            _id = [i, _ch_id] * np.shape(_ch_data)[0]\n",
    "            _id = np.array(_id).reshape(-1,2)\n",
    "            _eis_cluster = _data_group[j]['eis_cluster']\n",
    "            _id = np.hstack((_id, _eis_cluster.reshape(-1,1)))\n",
    "            \n",
    "            vitro1_id_list.append(_id)\n",
    "            vitro1_start_id_list.append(_id[0,:])\n",
    "\n",
    "    vitro1_data_list = np.vstack(vitro1_data_list)\n",
    "    vitro1_id_list = np.vstack(vitro1_id_list)\n",
    "    vitro1_start_list = np.vstack(vitro1_start_list)\n",
    "    vitro1_start_id_list = np.vstack(vitro1_start_id_list)\n",
    "\n",
    "    vitro1_ele_list = [i[1] for i in ele_list]\n",
    "\n",
    "    logger.info(f\"Total {vitro1_data_list.shape[0]} data points from {n_avaliable} electrodes\")\n",
    "\n",
    "    del data_pt, _meta_group, _data_group, _ch_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6ab038",
   "metadata": {},
   "source": [
    "## In vivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "41fe3d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "if READ_RAW_FLAG:\n",
    "    rootPath = \"D:/Baihm/EISNN/Invivo/\"\n",
    "    ele_list = SearchELE(rootPath,re.compile(r\"(.+?)_Ver02\"))\n",
    "    n_ele = len(ele_list)\n",
    "    logger.info(f\"Search in {rootPath} and find {n_ele:03d} electrodes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b3064dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "if READ_RAW_FLAG:\n",
    "    DATASET_SUFFIX = \"Outlier_Ver04\"\n",
    "\n",
    "    vivo0_start_list = []\n",
    "    vivo0_start_id_list = []\n",
    "    vivo0_data_list = []\n",
    "    vivo0_id_list = []\n",
    "\n",
    "    n_avaliable = 0\n",
    "\n",
    "    for i in range(n_ele):\n",
    "    # for i in range(3):\n",
    "        fd_pt = os.path.join(ele_list[i][0], DATASET_SUFFIX, f\"{ele_list[i][1]}_{DATASET_SUFFIX}.pt\")\n",
    "        if not os.path.exists(fd_pt):\n",
    "            logger.warning(f\"{fd_pt} does not exist\")\n",
    "            continue\n",
    "        data_pt = torch.load(fd_pt, weights_only=False)\n",
    "        _meta_group = data_pt[\"meta_group\"]\n",
    "        _data_group = data_pt[\"data_group\"]\n",
    "\n",
    "        n_day       = _meta_group[\"n_day\"]\n",
    "        n_ch        = _meta_group[\"n_ch\"]\n",
    "        n_valid_ch  = len(_data_group[\"Channels\"])\n",
    "\n",
    "\n",
    "        logger.info(f\"ELE [{i}/{n_ele}]: {ele_list[i][0]}\")\n",
    "\n",
    "        n_avaliable = n_avaliable + 1\n",
    "\n",
    "        # Iteration by channel\n",
    "        for j in _data_group['Channels']:\n",
    "            _ch_data = _data_group[j][\"chData\"]\n",
    "\n",
    "            if ONLY_SEQ_FLAG:\n",
    "                eis_seq = _data_group[j][\"eis_seq\"]\n",
    "                _ch_data = _ch_data[eis_seq,:,:]\n",
    "\n",
    "            _ch_data_log = np.log(_ch_data[:,1,:] + 1j*_ch_data[:,2,:])\n",
    "            _ch_data[:,1,:] = np.real(_ch_data_log)\n",
    "            _ch_data[:,2,:] = np.imag(_ch_data_log)\n",
    "            if _ch_data.shape[2] == 5000:\n",
    "                _ch_data = np.hstack((_ch_data[:,1,freq_list],_ch_data[:,2,freq_list]))\n",
    "            else:\n",
    "                _ch_data = np.hstack((_ch_data[:,1,:],_ch_data[:,2,:]))\n",
    "            vivo0_data_list.append(_ch_data)\n",
    "            vivo0_start_list.append(_ch_data[0,:])\n",
    "\n",
    "\n",
    "            _ch_id = j\n",
    "\n",
    "            _id = [i, _ch_id] * np.shape(_ch_data)[0]\n",
    "            _id = np.array(_id).reshape(-1,2)\n",
    "            _eis_cluster = _data_group[j]['eis_cluster']\n",
    "            _id = np.hstack((_id, _eis_cluster.reshape(-1,1)))\n",
    "            \n",
    "            vivo0_id_list.append(_id)\n",
    "            vivo0_start_id_list.append(_id[0,:])\n",
    "\n",
    "    vivo0_data_list = np.vstack(vivo0_data_list)\n",
    "    vivo0_id_list = np.vstack(vivo0_id_list)\n",
    "    vivo0_start_list = np.vstack(vivo0_start_list)\n",
    "    vivo0_start_id_list = np.vstack(vivo0_start_id_list)\n",
    "\n",
    "    vivo0_ele_list = [i[1] for i in ele_list]\n",
    "\n",
    "    logger.info(f\"Total {vivo0_data_list.shape[0]} data points from {n_avaliable} electrodes\")\n",
    "\n",
    "    del data_pt, _meta_group, _data_group, _ch_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517c415c",
   "metadata": {},
   "source": [
    "## Data Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c9fcb737",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-25 23:02:35.515\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m25\u001b[0m - \u001b[1mVitro0:\t(98690, 202)\t(12170, 202)\u001b[0m\n",
      "\u001b[32m2025-05-25 23:02:35.515\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m26\u001b[0m - \u001b[1mvitro1:\t(81674, 202)\t(9708, 202)\u001b[0m\n",
      "\u001b[32m2025-05-25 23:02:35.515\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m27\u001b[0m - \u001b[1mVivo0:\t(9406, 202)\t(719, 202)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "if not READ_RAW_FLAG:\n",
    "    # Data_Path = \"D:/Baihm/EISNN/Feature/AllData.npz\"\n",
    "    Data_Path = \"D:/Baihm/EISNN/Feature/SEQData.npz\"\n",
    "    if os.path.exists(Data_Path):\n",
    "        AllData = np.load(Data_Path)\n",
    "        vitro0_data_list = AllData[\"vitro0_data_list\"]\n",
    "        vitro0_id_list = AllData[\"vitro0_id_list\"]\n",
    "        vitro0_start_list = AllData[\"vitro0_start_list\"]\n",
    "        vitro0_start_id_list = AllData[\"vitro0_start_id_list\"]\n",
    "        vitro0_ele_list = AllData[\"vitro0_ele_list\"]\n",
    "        \n",
    "        vitro1_data_list = AllData[\"vitro1_data_list\"]\n",
    "        vitro1_id_list = AllData[\"vitro1_id_list\"]\n",
    "        vitro1_start_list = AllData[\"vitro1_start_list\"]\n",
    "        vitro1_start_id_list = AllData[\"vitro1_start_id_list\"]\n",
    "        vitro1_ele_list = AllData[\"vitro1_ele_list\"]\n",
    "\n",
    "        \n",
    "        vivo0_data_list = AllData[\"vivo0_data_list\"]\n",
    "        vivo0_id_list = AllData[\"vivo0_id_list\"]\n",
    "        vivo0_start_list = AllData[\"vivo0_start_list\"]\n",
    "        vivo0_start_id_list = AllData[\"vivo0_start_id_list\"]\n",
    "        vivo0_ele_list = AllData[\"vivo0_ele_list\"]\n",
    "\n",
    "        logger.info(f\"Vitro0:\\t{vitro0_data_list.shape}\\t{vitro0_start_list.shape}\")\n",
    "        logger.info(f\"vitro1:\\t{vitro1_data_list.shape}\\t{vitro1_start_list.shape}\")\n",
    "        logger.info(f\"Vivo0:\\t{vivo0_data_list.shape}\\t{vivo0_start_list.shape}\")\n",
    "        \n",
    "    else:\n",
    "        logger.warning(f\"{Data_Path} does not exist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "61280984",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_list = np.vstack((vitro0_data_list, vitro1_data_list, vivo0_data_list))\n",
    "all_id_list = np.vstack((vitro0_id_list, vitro1_id_list, vivo0_id_list))\n",
    "all_start_list = np.vstack((vitro0_start_list, vitro1_start_list, vivo0_start_list))\n",
    "all_start_id_list = np.vstack((vitro0_start_id_list, vitro1_start_id_list, vivo0_start_id_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4c63ab35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-25 23:02:35.597\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m1\u001b[0m - \u001b[1mVitro0:\t(98690, 202)\t(12170, 202)\u001b[0m\n",
      "\u001b[32m2025-05-25 23:02:35.597\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m2\u001b[0m - \u001b[1mvitro1:\t(81674, 202)\t(9708, 202)\u001b[0m\n",
      "\u001b[32m2025-05-25 23:02:35.597\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m3\u001b[0m - \u001b[1mVivo0:\t(9406, 202)\t(719, 202)\u001b[0m\n",
      "\u001b[32m2025-05-25 23:02:35.598\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m4\u001b[0m - \u001b[1mAll:\t\t(189770, 202)\t(22597, 202)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "logger.info(f\"Vitro0:\\t{vitro0_data_list.shape}\\t{vitro0_start_list.shape}\")\n",
    "logger.info(f\"vitro1:\\t{vitro1_data_list.shape}\\t{vitro1_start_list.shape}\")\n",
    "logger.info(f\"Vivo0:\\t{vivo0_data_list.shape}\\t{vivo0_start_list.shape}\")\n",
    "logger.info(f\"All:\\t\\t{all_data_list.shape}\\t{all_start_list.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641a414d",
   "metadata": {},
   "source": [
    "# Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f9f8c480",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "def load_all2ch(data_list, id_list = None):\n",
    "    '''==================================================\n",
    "        Load all data and split into 2 channels\n",
    "        Parameter: \n",
    "            data_list: data list    n x 202\n",
    "            id_list: id list        n x 2\n",
    "        Returen:\n",
    "            ch_data_list: channel data list     n x 101 x 2\n",
    "            ch_id_list: channel id list         n x 2\n",
    "        ==================================================\n",
    "    '''\n",
    "    ch_data_list = np.array([data_list[:,:101],data_list[:,101:]])\n",
    "    ch_data_list = ch_data_list.transpose(1,2,0)\n",
    "\n",
    "    ch_id_list = id_list\n",
    "\n",
    "    return ch_data_list, ch_id_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b4f0bb",
   "metadata": {},
   "source": [
    "## Tran & Eval Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "54674192",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_loss(x_rec, x, mu, logvar, beta=1e-3):\n",
    "    rec = F.mse_loss(x_rec, x)\n",
    "    kld = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return rec + beta * kld\n",
    "\n",
    "# ===== 训练函数 =====\n",
    "def train_model(model, train_ds, val_ds, num_epochs=20, batch_size=64, lr=1e-3):\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "    val_loader   = DataLoader(val_ds,   batch_size=batch_size)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    train_loss_recorder = []\n",
    "    eval_loss_recorder = []\n",
    "\n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for x in train_loader:\n",
    "            x = x.to(device)\n",
    "\n",
    "            x_rec, mu, lv = model(x)\n",
    "            loss = vae_loss(x_rec, x, mu, lv)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * x.size(0)\n",
    "\n",
    "        # 验证\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for x in val_loader:\n",
    "                x = x.to(device)\n",
    "\n",
    "                x_rec, mu, lv = model(x)\n",
    "                loss = vae_loss(x_rec, x, mu, lv)\n",
    "                # mu, lv = model.encode(x)\n",
    "                # x_rec = model.decode(mu)\n",
    "                # loss = vae_loss(x_rec, x, mu, lv)\n",
    "                \n",
    "                val_loss += loss.item() * x.size(0)\n",
    "\n",
    "        train_loss /= len(train_ds)\n",
    "        val_loss   /= len(val_ds)\n",
    "\n",
    "        train_loss_recorder.append(train_loss)\n",
    "        eval_loss_recorder.append(val_loss)\n",
    "        print(f\"Epoch {epoch}/{num_epochs}  Train: {train_loss:.6f}  Val: {val_loss:.6f}\")\n",
    "\n",
    "    return model, train_loss_recorder, eval_loss_recorder\n",
    "\n",
    "\n",
    "# ===== 可视化重建 =====\n",
    "def visualize_EISVAECNN(model, ds, num=5):\n",
    "    # ds = EISDataset_CNN(data_list)\n",
    "    loader = DataLoader(ds, batch_size=num, shuffle=True)\n",
    "    x = next(iter(loader)).to(device)   # [num,2,101]\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        x_rec, mu, lv = model(x)\n",
    "\n",
    "    x = x.cpu().numpy()\n",
    "    x_rec = x_rec.cpu().numpy()\n",
    "\n",
    "    for i in range(num):\n",
    "        plt.figure(figsize=(6,3))\n",
    "        # 实部\n",
    "        plt.subplot(1,2,1)\n",
    "        plt.plot(x[i,0], label=\"orig\", alpha = 0.5)\n",
    "        plt.plot(x_rec[i,0], '--', label=\"rec\")\n",
    "        plt.title(f\"Sample {i} Real\")\n",
    "        plt.legend()\n",
    "        # 虚部\n",
    "        plt.subplot(1,2,2)\n",
    "        plt.plot(x[i,1], label=\"orig\", alpha = 0.5)\n",
    "        plt.plot(x_rec[i,1], '--', label=\"rec\")\n",
    "        plt.title(f\"Sample {i} Imag\")\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc456ed9",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3d4df681",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EISDataset_CNN(Dataset):\n",
    "    def __init__(self, data_list, id_list = None):\n",
    "        # data_list: n x m x k x l x 2 list\n",
    "        # n: number of electrodes\n",
    "        # m: number of channels\n",
    "        # k: number of timestamps\n",
    "        # l: number of freq as dimensions\n",
    "        # 2: real and imaginary parts after logrithm\n",
    "\n",
    "        _data, _id  = load_all2ch(data_list, id_list)\n",
    "        _data = [torch.tensor(x, dtype=torch.float32) for x in _data]\n",
    "\n",
    "        self.data = _data\n",
    "        self.id = _id\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Return [2,101] for Conv1D\n",
    "        return self.data[idx].permute(1,0)  # [2,101] [in_ch, in_dim]\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7297b4",
   "metadata": {},
   "source": [
    "## Plot Latent Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "65200e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def VAE_latent(model, ds, batch_size=64):\n",
    "    loader = DataLoader(ds, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    _len_data = ds.__len__()\n",
    "    _poi = 0\n",
    "\n",
    "    latent_space_inst = []\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x in loader:\n",
    "            x = x.to(device)\n",
    "            mu, lv = model.encoder(x)\n",
    "            latent_space_inst.append(mu.cpu().numpy())\n",
    "\n",
    "            _poi = _poi + x.size(0)\n",
    "            if _poi % 1000 == 0:\n",
    "                logger.info(f\"[{_poi}]/[{_len_data}]\")\n",
    "\n",
    "    latent_space_inst = np.concatenate(latent_space_inst, axis=0)  # [B,z_dim]\n",
    "\n",
    "\n",
    "    _pca_inst = PCA(n_components=latent_space_inst.shape[1])\n",
    "    latent_dd = _pca_inst.fit_transform(latent_space_inst)\n",
    "    \n",
    "    \n",
    "    explained = _pca_inst.explained_variance_ratio_\n",
    "    eff_dim = (explained.cumsum() < 0.99).sum() + 1\n",
    "\n",
    "\n",
    "    fig, axis = plt.subplots(2,1,\n",
    "                gridspec_kw={'height_ratios': [4,1]},\n",
    "                figsize=(9, 9))\n",
    "    axis[0].scatter(latent_dd[:, 0], latent_dd[:, 1], alpha=0.5, s = 0.001)\n",
    "\n",
    "    axis[0].set_aspect('equal', adjustable='box')\n",
    "    axis[0].set_box_aspect(1)\n",
    "    axis[0].set_title(\"Latent Space\")\n",
    "    \n",
    "    axis[1].plot(_pca_inst.explained_variance_ratio_,\n",
    "                 label = f\"Valid Dimension = {eff_dim}\")\n",
    "    axis[1].legend()\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "\n",
    "    return latent_dd, eff_dim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34d2dab",
   "metadata": {},
   "source": [
    "## Kernel Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c0453b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_conv1d_layers(model, layer_type = nn.Conv1d):\n",
    "    conv_layers = []\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, layer_type):\n",
    "            conv_layers.append((name, module))\n",
    "    return conv_layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6abd5dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_conv1d_kernel_importance(conv_layer, layer_name=None, mode='l2'):\n",
    "    \"\"\"\n",
    "    mode: 'l2' 或 'l1'\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        weights = conv_layer.weight.cpu()  # [out_ch, in_ch, kernel_size]\n",
    "\n",
    "        if mode == 'l2':\n",
    "            importance = torch.norm(weights.view(weights.size(0), -1), p=2, dim=1)  # 每个 out_ch 的 L2 norm\n",
    "        elif mode == 'l1':\n",
    "            importance = torch.norm(weights.view(weights.size(0), -1), p=1, dim=1)\n",
    "        else:\n",
    "            raise ValueError(\"Only 'l2' and 'l1' are supported\")\n",
    "\n",
    "        importance = importance.numpy()\n",
    "\n",
    "    # 可视化\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.bar(np.arange(len(importance)), importance)\n",
    "    plt.xlabel('Kernel Index')\n",
    "    plt.ylabel(f'{mode.upper()} Norm')\n",
    "    plt.title(f'Kernel Importance in {layer_name or \"Conv1d Layer\"}')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def visualize_conv1d_kernels(conv, title='Conv1d Kernels'):\n",
    "    weight = conv.weight.data.cpu()  # shape: [out_ch, in_ch, kernel_size]\n",
    "    out_ch, in_ch, k_size = weight.shape\n",
    "\n",
    "    # 每个输出通道我们可以画成一个子图，每行是一个输入通道的 kernel\n",
    "    n_rows = np.floor(np.sqrt(out_ch)).astype(int)\n",
    "    n_cols = np.ceil(out_ch / n_rows).astype(int)\n",
    "\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(n_cols*2, n_rows*2), sharex=True, sharey=True)\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for idx in range(out_ch):\n",
    "        ax = axes[idx]\n",
    "        kernel = weight[idx]  # shape: [in_ch, k_size]\n",
    "\n",
    "        for ic in range(in_ch):\n",
    "            ax.plot(kernel[ic].numpy(), label=f'InCh {ic}', alpha=0.7)\n",
    "\n",
    "        ax.set_title(f'OutCh {idx}', fontsize=8)\n",
    "        if idx % n_cols != 0:\n",
    "            ax.set_yticklabels([])\n",
    "        if idx < (n_rows - 1) * n_cols:\n",
    "            ax.set_xticklabels([])\n",
    "\n",
    "    # Remove unused subplots\n",
    "    for i in range(out_ch, len(axes)):\n",
    "        fig.delaxes(axes[i])\n",
    "\n",
    "    fig.suptitle(title, fontsize=14)\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7c6f33ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel_dimensionality(conv, plot=True):\n",
    "    W = conv.weight.data.cpu().numpy()  # [out_ch, in_ch, k_size]\n",
    "    W_flat = W.reshape(W.shape[0], -1)  # [out_ch, in_ch * k_size]\n",
    "\n",
    "    pca = PCA(n_components=W_flat.shape[0])\n",
    "    pca.fit(W_flat)\n",
    "    explained = pca.explained_variance_ratio_\n",
    "\n",
    "    if plot:\n",
    "        import matplotlib.pyplot as plt\n",
    "        plt.figure(figsize=(5,3))\n",
    "        plt.plot(explained.cumsum(), marker='o')\n",
    "        plt.xlabel('Number of Components')\n",
    "        plt.ylabel('Explained Variance Ratio')\n",
    "        plt.title('Cumulative PCA on Conv Kernels')\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "    # Return effective dimension \n",
    "    # (minimum number of dimensions that explain 99% variance)\n",
    "    \n",
    "    eff_dim = (explained.cumsum() < 0.99).sum() + 1\n",
    "    return eff_dim, explained"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0062a601",
   "metadata": {},
   "source": [
    "# Vectorization Model Design"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0836947",
   "metadata": {},
   "source": [
    "## Ver01 - 3 x CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09881e0c",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a72a3241",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Curve2VecEncoder_Ver01(nn.Module):\n",
    "    def __init__(self, in_ch, in_dim, hid_ch, \n",
    "                 z_dim, kernel_size):\n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "        _layers = []\n",
    "\n",
    "        pre_ch = in_ch\n",
    "        poi_ch = hid_ch\n",
    "        _layers.append(nn.Conv1d(pre_ch, poi_ch, kernel_size=kernel_size))\n",
    "        _layers.append(nn.ReLU())\n",
    "        # _layers.append(nn.BatchNorm1d(poi_ch))\n",
    "        \n",
    "        pre_ch = poi_ch\n",
    "        poi_ch = poi_ch * 2\n",
    "        _layers.append(nn.Conv1d(pre_ch, poi_ch, kernel_size=kernel_size))\n",
    "        _layers.append(nn.ReLU())\n",
    "        # _layers.append(nn.BatchNorm1d(poi_ch))\n",
    "        \n",
    "        pre_ch = poi_ch\n",
    "        poi_ch = poi_ch * 2\n",
    "        _layers.append(nn.Conv1d(pre_ch, poi_ch, kernel_size=kernel_size))\n",
    "        _layers.append(nn.ReLU())\n",
    "        # _layers.append(nn.BatchNorm1d(poi_ch))\n",
    "\n",
    "\n",
    "        self.conv = nn.Sequential(*_layers)\n",
    "        self.pool = nn.AdaptiveAvgPool1d(1)\n",
    "\n",
    "\n",
    "        self.fc_mu = nn.Linear(poi_ch, z_dim)\n",
    "        self.fc_lv = nn.Linear(poi_ch, z_dim)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.conv(x)                # [B,ch,in_dim]\n",
    "        h = self.pool(h).squeeze(-1)    # [B,ch]\n",
    "        return self.fc_mu(h), self.fc_lv(h) \n",
    "\n",
    "\n",
    "class Curve2VecDecoder_Ver01(nn.Module):\n",
    "    def __init__(self, out_ch, out_dim, hid_ch, \n",
    "                 z_dim, kernel_size):\n",
    "        super().__init__()\n",
    "        self.hid_ch = hid_ch\n",
    "        self.out_dim = out_dim\n",
    "\n",
    "\n",
    "        self.fc_expand = nn.Linear(z_dim, hid_ch * out_dim)\n",
    "\n",
    "\n",
    "        _layers = []\n",
    "        _layers.append(nn.ReLU())\n",
    "\n",
    "        pre_ch = hid_ch\n",
    "        poi_ch = hid_ch//2\n",
    "        _layers.append(nn.ConvTranspose1d(pre_ch, poi_ch, kernel_size=kernel_size, padding=kernel_size//2))\n",
    "        _layers.append(nn.ReLU())\n",
    "        # _layers.append(nn.BatchNorm1d(poi_ch))\n",
    "        \n",
    "        # pre_ch = poi_ch\n",
    "        # poi_ch = poi_ch//2\n",
    "        # _layers.append(nn.ConvTranspose1d(pre_ch, poi_ch, kernel_size=kernel_size, padding=kernel_size//2))\n",
    "        # _layers.append(nn.ReLU())\n",
    "        # # _layers.append(nn.BatchNorm1d(poi_ch))\n",
    "\n",
    "        pre_ch = poi_ch\n",
    "        poi_ch = out_ch\n",
    "        _layers.append(nn.Conv1d(pre_ch, poi_ch, kernel_size=kernel_size, padding=kernel_size//2))\n",
    "\n",
    "\n",
    "        # pre_ch = hid_ch\n",
    "        # poi_ch = out_ch\n",
    "        # _layers.append(nn.Conv1d(pre_ch, poi_ch, kernel_size=kernel_size, padding=kernel_size//2))\n",
    "\n",
    "\n",
    "        \n",
    "        self.deconv = nn.Sequential(*_layers)\n",
    "\n",
    "\n",
    "    def forward(self, z):\n",
    "        h = self.fc_expand(z)           # [B,in_ch*in_dim]\n",
    "        h = h.view(-1, self.hid_ch, self.out_dim)\n",
    "        h = self.deconv(h)               # [B,in_ch,in_dim]\n",
    "        return h                        # [B,in_ch,in_dim]\n",
    "\n",
    "class Curve2VecVAE_Ver01(nn.Module):\n",
    "    def __init__(self, in_ch=2, in_dim=101, \n",
    "                 enc_hid_ch = 16,\n",
    "                 dec_hid_ch = 16,\n",
    "                 z_dim = 16, kernel_size = 13):\n",
    "        super().__init__()\n",
    "        self.encoder = Curve2VecEncoder_Ver01(in_ch, in_dim, enc_hid_ch, z_dim, kernel_size)\n",
    "        self.decoder = Curve2VecDecoder_Ver01(in_ch, in_dim, dec_hid_ch, z_dim, kernel_size)\n",
    "\n",
    "    def reparam(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, lv = self.encoder(x)\n",
    "        z = self.reparam(mu, lv)\n",
    "        x_rec = self.decoder(z)\n",
    "        return x_rec, mu, lv \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596f2188",
   "metadata": {},
   "source": [
    "### Running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dd429ac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65242\n"
     ]
    }
   ],
   "source": [
    "model_ver01 = Curve2VecVAE_Ver01().to(device)\n",
    "print(count_parameters(model_ver01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1a900e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 200\n",
    "batch_size=128\n",
    "lr=1e-3\n",
    "random_state = None\n",
    "\n",
    "train_list, val_list = train_test_split(all_data_list, test_size=0.2, random_state=random_state)\n",
    "len(val_list)\n",
    "\n",
    "all_ds = EISDataset_CNN(all_data_list)\n",
    "train_ds = EISDataset_CNN(train_list)\n",
    "val_ds   = EISDataset_CNN(val_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2a3f1a77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200  Train: 2.440765  Val: 0.282994\n",
      "Epoch 2/200  Train: 0.244326  Val: 0.203530\n",
      "Epoch 3/200  Train: 0.162992  Val: 0.126925\n",
      "Epoch 4/200  Train: 0.109234  Val: 0.094743\n",
      "Epoch 5/200  Train: 0.075916  Val: 0.057839\n",
      "Epoch 6/200  Train: 0.058495  Val: 0.058915\n",
      "Epoch 7/200  Train: 0.055690  Val: 0.067451\n",
      "Epoch 8/200  Train: 0.046415  Val: 0.037308\n",
      "Epoch 9/200  Train: 0.037960  Val: 0.027565\n",
      "Epoch 10/200  Train: 0.029589  Val: 0.032086\n",
      "Epoch 11/200  Train: 0.025367  Val: 0.030340\n",
      "Epoch 12/200  Train: 0.022151  Val: 0.027276\n",
      "Epoch 13/200  Train: 0.020039  Val: 0.022645\n",
      "Epoch 14/200  Train: 0.019006  Val: 0.016399\n",
      "Epoch 15/200  Train: 0.018001  Val: 0.017722\n",
      "Epoch 16/200  Train: 0.017381  Val: 0.027798\n",
      "Epoch 17/200  Train: 0.016484  Val: 0.015474\n",
      "Epoch 18/200  Train: 0.015688  Val: 0.018571\n",
      "Epoch 19/200  Train: 0.015306  Val: 0.015113\n",
      "Epoch 20/200  Train: 0.014591  Val: 0.012225\n",
      "Epoch 21/200  Train: 0.014228  Val: 0.014339\n",
      "Epoch 22/200  Train: 0.013377  Val: 0.083636\n",
      "Epoch 23/200  Train: 0.013915  Val: 0.011220\n",
      "Epoch 24/200  Train: 0.012610  Val: 0.026065\n",
      "Epoch 25/200  Train: 0.012238  Val: 0.011001\n",
      "Epoch 26/200  Train: 0.011530  Val: 0.026249\n",
      "Epoch 27/200  Train: 0.011333  Val: 0.009298\n",
      "Epoch 28/200  Train: 0.010871  Val: 0.009469\n",
      "Epoch 29/200  Train: 0.010556  Val: 0.010502\n",
      "Epoch 30/200  Train: 0.010834  Val: 0.008149\n",
      "Epoch 31/200  Train: 0.009851  Val: 0.010369\n",
      "Epoch 32/200  Train: 0.009985  Val: 0.010192\n",
      "Epoch 33/200  Train: 0.009835  Val: 0.009329\n",
      "Epoch 34/200  Train: 0.009691  Val: 0.009517\n",
      "Epoch 35/200  Train: 0.009287  Val: 0.012426\n",
      "Epoch 36/200  Train: 0.009409  Val: 0.007514\n",
      "Epoch 37/200  Train: 0.009269  Val: 0.008362\n",
      "Epoch 38/200  Train: 0.009102  Val: 0.008728\n",
      "Epoch 39/200  Train: 0.009019  Val: 0.009619\n",
      "Epoch 40/200  Train: 0.008830  Val: 0.007311\n",
      "Epoch 41/200  Train: 0.008760  Val: 0.008410\n",
      "Epoch 42/200  Train: 0.008508  Val: 0.007193\n",
      "Epoch 43/200  Train: 0.008592  Val: 0.007789\n",
      "Epoch 44/200  Train: 0.008315  Val: 0.006970\n",
      "Epoch 45/200  Train: 0.008480  Val: 0.012358\n",
      "Epoch 46/200  Train: 0.008211  Val: 0.008159\n",
      "Epoch 47/200  Train: 0.008119  Val: 0.006644\n",
      "Epoch 48/200  Train: 0.008131  Val: 0.007236\n",
      "Epoch 49/200  Train: 0.008018  Val: 0.019461\n",
      "Epoch 50/200  Train: 0.007999  Val: 0.008148\n",
      "Epoch 51/200  Train: 0.007910  Val: 0.006322\n",
      "Epoch 52/200  Train: 0.007824  Val: 0.007545\n",
      "Epoch 53/200  Train: 0.007785  Val: 0.011990\n",
      "Epoch 54/200  Train: 0.007774  Val: 0.011438\n",
      "Epoch 55/200  Train: 0.007839  Val: 0.006762\n",
      "Epoch 56/200  Train: 0.007688  Val: 0.009227\n",
      "Epoch 57/200  Train: 0.007560  Val: 0.007422\n",
      "Epoch 58/200  Train: 0.007605  Val: 0.006941\n",
      "Epoch 59/200  Train: 0.007618  Val: 0.006470\n",
      "Epoch 60/200  Train: 0.007589  Val: 0.007053\n",
      "Epoch 61/200  Train: 0.007424  Val: 0.006807\n",
      "Epoch 62/200  Train: 0.007450  Val: 0.061996\n",
      "Epoch 63/200  Train: 0.007591  Val: 0.008887\n",
      "Epoch 64/200  Train: 0.007267  Val: 0.008480\n",
      "Epoch 65/200  Train: 0.007383  Val: 0.006905\n",
      "Epoch 66/200  Train: 0.007261  Val: 0.006366\n",
      "Epoch 67/200  Train: 0.007131  Val: 0.006868\n",
      "Epoch 68/200  Train: 0.007162  Val: 0.009830\n",
      "Epoch 69/200  Train: 0.007094  Val: 0.029742\n",
      "Epoch 70/200  Train: 0.007132  Val: 0.006896\n",
      "Epoch 71/200  Train: 0.006876  Val: 0.006335\n",
      "Epoch 72/200  Train: 0.006915  Val: 0.006094\n",
      "Epoch 73/200  Train: 0.006933  Val: 0.006243\n",
      "Epoch 74/200  Train: 0.006938  Val: 0.005674\n",
      "Epoch 75/200  Train: 0.006691  Val: 0.005895\n",
      "Epoch 76/200  Train: 0.006749  Val: 0.006691\n",
      "Epoch 77/200  Train: 0.006785  Val: 0.007299\n",
      "Epoch 78/200  Train: 0.006572  Val: 0.012580\n",
      "Epoch 79/200  Train: 0.006717  Val: 0.007830\n",
      "Epoch 80/200  Train: 0.006545  Val: 0.010182\n",
      "Epoch 81/200  Train: 0.006571  Val: 0.006450\n",
      "Epoch 82/200  Train: 0.006463  Val: 0.006467\n",
      "Epoch 83/200  Train: 0.006459  Val: 0.005240\n",
      "Epoch 84/200  Train: 0.006470  Val: 0.005549\n",
      "Epoch 85/200  Train: 0.006294  Val: 0.005416\n",
      "Epoch 86/200  Train: 0.006486  Val: 0.006317\n",
      "Epoch 87/200  Train: 0.006320  Val: 0.006546\n",
      "Epoch 88/200  Train: 0.006420  Val: 0.008624\n",
      "Epoch 89/200  Train: 0.006119  Val: 0.005555\n",
      "Epoch 90/200  Train: 0.006244  Val: 0.005339\n",
      "Epoch 91/200  Train: 0.006225  Val: 0.006030\n",
      "Epoch 92/200  Train: 0.006207  Val: 0.008675\n",
      "Epoch 93/200  Train: 0.006114  Val: 0.006102\n",
      "Epoch 94/200  Train: 0.006145  Val: 0.005885\n",
      "Epoch 95/200  Train: 0.005998  Val: 0.004809\n",
      "Epoch 96/200  Train: 0.006105  Val: 0.005020\n",
      "Epoch 97/200  Train: 0.006023  Val: 0.006423\n",
      "Epoch 98/200  Train: 0.005976  Val: 0.011341\n",
      "Epoch 99/200  Train: 0.006008  Val: 0.006691\n",
      "Epoch 100/200  Train: 0.005995  Val: 0.005935\n",
      "Epoch 101/200  Train: 0.005995  Val: 0.004961\n",
      "Epoch 102/200  Train: 0.005903  Val: 0.006654\n",
      "Epoch 103/200  Train: 0.005902  Val: 0.005244\n",
      "Epoch 104/200  Train: 0.005894  Val: 0.006822\n",
      "Epoch 105/200  Train: 0.005839  Val: 0.006323\n",
      "Epoch 106/200  Train: 0.005888  Val: 0.007795\n",
      "Epoch 107/200  Train: 0.005936  Val: 0.005361\n",
      "Epoch 108/200  Train: 0.005769  Val: 0.005759\n",
      "Epoch 109/200  Train: 0.005725  Val: 0.010533\n",
      "Epoch 110/200  Train: 0.005840  Val: 0.004889\n",
      "Epoch 111/200  Train: 0.005708  Val: 0.007316\n",
      "Epoch 112/200  Train: 0.005746  Val: 0.007289\n",
      "Epoch 113/200  Train: 0.005719  Val: 0.009286\n",
      "Epoch 114/200  Train: 0.005652  Val: 0.005101\n",
      "Epoch 115/200  Train: 0.005682  Val: 0.005362\n",
      "Epoch 116/200  Train: 0.005681  Val: 0.008195\n",
      "Epoch 117/200  Train: 0.005671  Val: 0.005310\n",
      "Epoch 118/200  Train: 0.005478  Val: 0.008165\n",
      "Epoch 119/200  Train: 0.005707  Val: 0.004664\n",
      "Epoch 120/200  Train: 0.005493  Val: 0.004860\n",
      "Epoch 121/200  Train: 0.005523  Val: 0.004588\n",
      "Epoch 122/200  Train: 0.005552  Val: 0.005273\n",
      "Epoch 123/200  Train: 0.005591  Val: 0.004705\n",
      "Epoch 124/200  Train: 0.005420  Val: 0.006274\n",
      "Epoch 125/200  Train: 0.005461  Val: 0.005837\n",
      "Epoch 126/200  Train: 0.005441  Val: 0.006311\n",
      "Epoch 127/200  Train: 0.005425  Val: 0.004972\n",
      "Epoch 128/200  Train: 0.005510  Val: 0.008244\n",
      "Epoch 129/200  Train: 0.005402  Val: 0.004542\n",
      "Epoch 130/200  Train: 0.005352  Val: 0.005674\n",
      "Epoch 131/200  Train: 0.005385  Val: 0.006480\n",
      "Epoch 132/200  Train: 0.005312  Val: 0.016110\n",
      "Epoch 133/200  Train: 0.005479  Val: 0.004868\n",
      "Epoch 134/200  Train: 0.005276  Val: 0.004978\n",
      "Epoch 135/200  Train: 0.005247  Val: 0.005494\n",
      "Epoch 136/200  Train: 0.005362  Val: 0.005863\n",
      "Epoch 137/200  Train: 0.005252  Val: 0.004919\n",
      "Epoch 138/200  Train: 0.005303  Val: 0.005195\n",
      "Epoch 139/200  Train: 0.005223  Val: 0.005805\n",
      "Epoch 140/200  Train: 0.005241  Val: 0.004952\n",
      "Epoch 141/200  Train: 0.005149  Val: 0.009356\n",
      "Epoch 142/200  Train: 0.005266  Val: 0.004601\n",
      "Epoch 143/200  Train: 0.005198  Val: 0.004817\n",
      "Epoch 144/200  Train: 0.005091  Val: 0.004232\n",
      "Epoch 145/200  Train: 0.005134  Val: 0.006912\n",
      "Epoch 146/200  Train: 0.005193  Val: 0.008382\n",
      "Epoch 147/200  Train: 0.005082  Val: 0.006624\n",
      "Epoch 148/200  Train: 0.005182  Val: 0.007885\n",
      "Epoch 149/200  Train: 0.004980  Val: 0.004712\n",
      "Epoch 150/200  Train: 0.005148  Val: 0.004453\n",
      "Epoch 151/200  Train: 0.004964  Val: 0.004257\n",
      "Epoch 152/200  Train: 0.005093  Val: 0.004497\n",
      "Epoch 153/200  Train: 0.005113  Val: 0.004926\n",
      "Epoch 154/200  Train: 0.005053  Val: 0.007638\n",
      "Epoch 155/200  Train: 0.004999  Val: 0.004300\n",
      "Epoch 156/200  Train: 0.004977  Val: 0.009344\n",
      "Epoch 157/200  Train: 0.005002  Val: 0.004657\n",
      "Epoch 158/200  Train: 0.005005  Val: 0.005270\n",
      "Epoch 159/200  Train: 0.004943  Val: 0.007830\n",
      "Epoch 160/200  Train: 0.004981  Val: 0.005889\n",
      "Epoch 161/200  Train: 0.004852  Val: 0.004287\n",
      "Epoch 162/200  Train: 0.004910  Val: 0.005223\n",
      "Epoch 163/200  Train: 0.004914  Val: 0.004773\n",
      "Epoch 164/200  Train: 0.004847  Val: 0.004890\n",
      "Epoch 165/200  Train: 0.004895  Val: 0.008093\n",
      "Epoch 166/200  Train: 0.004882  Val: 0.004549\n",
      "Epoch 167/200  Train: 0.004802  Val: 0.004072\n",
      "Epoch 168/200  Train: 0.004842  Val: 0.004661\n",
      "Epoch 169/200  Train: 0.004809  Val: 0.004830\n",
      "Epoch 170/200  Train: 0.004812  Val: 0.004006\n",
      "Epoch 171/200  Train: 0.004769  Val: 0.004423\n",
      "Epoch 172/200  Train: 0.004802  Val: 0.004535\n",
      "Epoch 173/200  Train: 0.004848  Val: 0.004974\n",
      "Epoch 174/200  Train: 0.004695  Val: 0.005050\n",
      "Epoch 175/200  Train: 0.004739  Val: 0.004189\n",
      "Epoch 176/200  Train: 0.004686  Val: 0.005113\n",
      "Epoch 177/200  Train: 0.004818  Val: 0.005460\n",
      "Epoch 178/200  Train: 0.004642  Val: 0.004356\n",
      "Epoch 179/200  Train: 0.004726  Val: 0.005479\n",
      "Epoch 180/200  Train: 0.004650  Val: 0.005982\n",
      "Epoch 181/200  Train: 0.004646  Val: 0.003841\n",
      "Epoch 182/200  Train: 0.004765  Val: 0.004296\n",
      "Epoch 183/200  Train: 0.004586  Val: 0.004222\n",
      "Epoch 184/200  Train: 0.004665  Val: 0.005750\n",
      "Epoch 185/200  Train: 0.004604  Val: 0.004428\n",
      "Epoch 186/200  Train: 0.004594  Val: 0.004061\n",
      "Epoch 187/200  Train: 0.004597  Val: 0.004556\n",
      "Epoch 188/200  Train: 0.004645  Val: 0.005471\n",
      "Epoch 189/200  Train: 0.004637  Val: 0.004674\n",
      "Epoch 190/200  Train: 0.004553  Val: 0.006222\n",
      "Epoch 191/200  Train: 0.004636  Val: 0.004319\n",
      "Epoch 192/200  Train: 0.004474  Val: 0.004229\n",
      "Epoch 193/200  Train: 0.004617  Val: 0.006328\n",
      "Epoch 194/200  Train: 0.004545  Val: 0.004904\n",
      "Epoch 195/200  Train: 0.004532  Val: 0.006990\n",
      "Epoch 196/200  Train: 0.004491  Val: 0.005234\n",
      "Epoch 197/200  Train: 0.004470  Val: 0.004050\n",
      "Epoch 198/200  Train: 0.004509  Val: 0.004114\n",
      "Epoch 199/200  Train: 0.004506  Val: 0.004048\n",
      "Epoch 200/200  Train: 0.004420  Val: 0.004466\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_ver01, train_loss, eval_loss = train_model(model_ver01, train_ds, val_ds, num_epochs=num_epochs, batch_size=batch_size, lr=lr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e8f846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if False:\n",
    "#     # eis2vec_save_path = \"D:\\Baihm\\EISNN\\Feature\\SeqData_Convx2_z_ConvTx1_Convx1.pt\"\n",
    "#     torch.save(model_ver01.state_dict(), eis2vec_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed555a32",
   "metadata": {},
   "source": [
    "### Plot Loss & Eval Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5c27ddad",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "# plt.semilogy(train_loss, label=\"train\")\n",
    "# plt.semilogy(eval_loss, label=\"eval\")\n",
    "plt.semilogy(train_loss, label=\"train\")\n",
    "plt.semilogy(eval_loss, label=\"eval\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1b45b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_EISVAECNN(model_ver01, val_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c77cce4",
   "metadata": {},
   "source": [
    "### Plot Latent Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9b16ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_expr, _ = VAE_latent(model_ver01, all_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8d201c",
   "metadata": {},
   "source": [
    "### Plot Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83038e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_layers = get_all_conv1d_layers(model_ver01, layer_type=nn.Conv1d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959e3507",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _layer in conv_layers:\n",
    "    layer_name, layer = _layer\n",
    "    print(layer_name, layer)\n",
    "    # visualize_conv1d_kernels(layer)\n",
    "    # W = layer.weight.data.cpu().numpy()  # [out_ch, in_ch, k_size]\n",
    "    # print(W.shape)\n",
    "    kernel_dimensionality(layer)\n",
    "    # visualize_conv1d_kernel_importance(layer, layer_name=layer_name, mode='l1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00ff714",
   "metadata": {},
   "source": [
    "### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9335f9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    eis2vec_save_path = \"D:/Baihm/EISNN/PredictionModel/model/Convx2_z_ConvTx1_Convx1.pt\"\n",
    "    torch.save(model_ver01.state_dict(), eis2vec_save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EISNN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
