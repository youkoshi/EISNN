{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2432f189",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1231f43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import OneClassSVM\n",
    "from scipy.special import expit  \n",
    "import joblib\n",
    "\n",
    "import re\n",
    "import os\n",
    "import gc\n",
    "from loguru import logger\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib qt\n",
    "\n",
    "from collections import defaultdict\n",
    "# from datetime import datetime\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d169ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gatherCSV(rootPath, outsuffix = 'Tracking'):\n",
    "    '''==================================================\n",
    "        Collect all EIS.csv files in the rootPath\n",
    "        Parameter: \n",
    "            rootPath: current search path\n",
    "            outsuffix: Saving path of EIS.csv files\n",
    "        Returen:\n",
    "            EISDict: a 2D-dict of EIS data\n",
    "            Storage Frame: EISDict[_sessionIndex][_channelIndex] = \"_filepath\"\n",
    "        ==================================================\n",
    "    '''\n",
    "    _filename       = None\n",
    "    _filepath       = None\n",
    "    _trackpath      = None\n",
    "    _csvpath        = None\n",
    "    _sessionIndex   = None\n",
    "    _channelIndex   = None\n",
    "    _processed      = None\n",
    "\n",
    "    EISDict = defaultdict(dict)\n",
    "\n",
    "    ## Iterate session\n",
    "    session_pattern = re.compile(r\"(.+?)_(\\d{8})_01\")\n",
    "    bank_pattern    = re.compile(r\"([1-4])\")\n",
    "    file_pattern    = re.compile(r\"EIS_ch(\\d{3})\\.csv\")\n",
    "\n",
    "    ## RootDir\n",
    "    for i in os.listdir(rootPath):\n",
    "        match_session = session_pattern.match(i)\n",
    "        ## SessionDir\n",
    "        if match_session:\n",
    "            logger.info(f\"Session Begin: {i}\")\n",
    "            _sessionIndex = match_session[2]\n",
    "            for j in os.listdir(f\"{rootPath}/{i}\"):\n",
    "                match_bank = bank_pattern.match(j)\n",
    "                ## BankDir\n",
    "                if match_bank:\n",
    "                    logger.info(f\"Bank Begin: {j}\")\n",
    "                    _trackpath = f\"{rootPath}/{i}/{j}/{outsuffix}\"\n",
    "                    if not os.path.exists(_trackpath):\n",
    "                        continue\n",
    "\n",
    "                    for k in os.listdir(f\"{rootPath}/{i}/{j}/{outsuffix}\"):\n",
    "                        match_file = file_pattern.match(k)\n",
    "                        ## File\n",
    "                        if match_file:\n",
    "                            _filename = k\n",
    "                            _filepath = f\"{rootPath}/{i}/{j}/{outsuffix}/{k}\"\n",
    "                            _channelIndex = (int(match_bank[1])-1)*32+int(match_file[1])\n",
    "                            \n",
    "                            EISDict[_sessionIndex][_channelIndex] = f\"{rootPath}/{i}/{j}/{outsuffix}/{k}\"\n",
    "                            \n",
    "    return EISDict\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Data Readout\n",
    "def readChannel(chID, fileDict):\n",
    "    '''==================================================\n",
    "        Read EIS.csv file by Channel\n",
    "        Parameter: \n",
    "            chID: channel index\n",
    "            fileDict: EISDict[_sessionIndex][_channelIndex] = \"_filepath\"\n",
    "        Returen:\n",
    "            freq: frequency\n",
    "            Zreal: real part of impedance\n",
    "            Zimag: imaginary part of impedance\n",
    "        ==================================================\n",
    "    '''\n",
    "    chData = []\n",
    "    for ssID in fileDict.keys():\n",
    "        _data   = np.loadtxt(fileDict[ssID][chID], delimiter=',')\n",
    "        _freq   = _data[:,0]\n",
    "        _Zreal  = _data[:,3]\n",
    "        _Zimag  = _data[:,4]\n",
    "        chData.append(np.stack((_freq, _Zreal, _Zimag),axis=0))\n",
    "\n",
    "    return np.stack(chData, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8564c3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def EIS_recal(data):\n",
    "    f_poi = data[0,:]\n",
    "    # Z_poi = data[1,:] * np.exp(1j*np.deg2rad(data[2,:]))\n",
    "    Z_poi = data[1,:] + 1j*data[2,:]\n",
    "    Y_poi = 1/Z_poi\n",
    "\n",
    "    Rg0 = 1.611e13\n",
    "    Cp0 = 1.4e-9\n",
    "    \n",
    "    _Rg0_rescale = 1/Rg0*np.power(f_poi,1.583)\n",
    "    _Cp0_rescale = Cp0*np.power(f_poi,0.911)\n",
    "    Y_org = Y_poi - _Rg0_rescale + 1j*_Cp0_rescale\n",
    "    Z_org = 1/Y_org\n",
    "\n",
    "    # Amp Calibration\n",
    "    Z_ampC = np.abs(Z_org)\n",
    "\n",
    "    # Phz Calibration\n",
    "    Z_phzC = np.angle(Z_org)\n",
    "    \n",
    "    Z_rec = Z_ampC * np.exp(1j*Z_phzC)\n",
    "\n",
    "    \n",
    "    return np.transpose(np.array([f_poi, np.real(Z_rec), np.imag(Z_rec)])).T\n",
    "\n",
    "\n",
    "def EIS_recal_ver02(data, _phz_0 = None):\n",
    "    f_poi = data[0,:]\n",
    "    # Z_poi = data[1,:] * np.exp(1j*np.deg2rad(data[2,:]))\n",
    "    Z_poi = data[1,:] + 1j*data[2,:]\n",
    "    Y_poi = 1/Z_poi\n",
    "\n",
    "    Rg0 = 1.611e13\n",
    "    Cp0 = 1.4e-9\n",
    "    \n",
    "    _Rg0_rescale = 1/Rg0*np.power(f_poi,1.583)\n",
    "    _Cp0_rescale = Cp0*np.power(f_poi,0.911)\n",
    "    Y_org = Y_poi - _Rg0_rescale + 1j*_Cp0_rescale\n",
    "    Z_org = 1/Y_org\n",
    "\n",
    "    # Phz Calibration\n",
    "    if _phz_0 is None:\n",
    "        _phz_0 = np.loadtxt(\"./phz_Calib.txt\")\n",
    "    \n",
    "    Z_ampC = np.abs(Z_org)\n",
    "    # Z_phzC = np.angle(Z_org) - _phz_0\n",
    "    Z_phzC = np.angle(Z_org) - _phz_0\n",
    "\n",
    "    Z_rec = Z_ampC * np.exp(1j*Z_phzC)\n",
    "\n",
    "    # C = 5e-10\n",
    "    Rs0 = 100\n",
    "    Z_rec = Z_rec - Rs0\n",
    "\n",
    "\n",
    "\n",
    "    Cp0 = 5e-10\n",
    "    _Cp0_rescale = Cp0 * f_poi\n",
    "    Z_rec = 1/(1/Z_rec - 1j * _Cp0_rescale)\n",
    "\n",
    "    \n",
    "\n",
    "    # Ls0 = 1.7e-4\n",
    "    Ls0 = 5e-4\n",
    "    _Ls0_rescale = Ls0 * f_poi\n",
    "    Z_rec = Z_rec - 1j * _Ls0_rescale\n",
    "\n",
    "    # C = 5e-10\n",
    "    Rs0 = 566\n",
    "    Z_rec = Z_rec - Rs0\n",
    "    \n",
    "    return np.stack([f_poi, np.real(Z_rec), np.imag(Z_rec)], axis=1).T\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3efff0d0",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d20f7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SearchELE(rootPath, ele_pattern = re.compile(r\"(.+?)_归档\")):\n",
    "    '''==================================================\n",
    "        Search all electrode directories in the rootPath\n",
    "        Parameter: \n",
    "            rootPath: current search path\n",
    "            ele_pattern: electrode dir name patten\n",
    "        Returen:\n",
    "            ele_list: list of electrode directories\n",
    "        ==================================================\n",
    "    '''\n",
    "    ele_list = []\n",
    "    for i in os.listdir(rootPath):\n",
    "        _path = os.path.join(rootPath, i)\n",
    "        if os.path.isdir(_path):\n",
    "            match_ele = ele_pattern.match(i)\n",
    "            if match_ele:\n",
    "                ele_list.append([_path, match_ele.group(1)])\n",
    "            else:\n",
    "                ele_list.extend(SearchELE(_path, ele_pattern))\n",
    "\n",
    "    return ele_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82018c86",
   "metadata": {},
   "source": [
    "### Archive_Old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fef3f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rootPath = \"D:/Baihm/EISNN/Archive/\"\n",
    "ele_list = SearchELE(rootPath)\n",
    "n_ele = len(ele_list)\n",
    "logger.info(f\"Search in {rootPath} and find {n_ele:03d} electrodes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e037858",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 首先我们把128/128看似完全没问题的这部分电极拿出来做聚类看看\n",
    "# 数据量也比较小，跑起来应该会更快\n",
    "\n",
    "MODEL_SUFFIX = \"Matern12_Ver01\"\n",
    "\n",
    "almost_start_list = []\n",
    "almost_start_id_list = []\n",
    "almost_data_list = []\n",
    "almost_id_list = []\n",
    "_ch_pattern = re.compile(r\"ch_(\\d{3})\")\n",
    "\n",
    "for i in range(n_ele):\n",
    "# for i in range(3):\n",
    "    fd_pt = os.path.join(ele_list[i][0], MODEL_SUFFIX, f\"{ele_list[i][1]}_{MODEL_SUFFIX}.pt\")\n",
    "    if not os.path.exists(fd_pt):\n",
    "        # logger.warning(f\"{fd_pt} does not exist\")\n",
    "        continue\n",
    "    data_pt = torch.load(fd_pt, weights_only=False)\n",
    "    _meta_group = data_pt[\"meta_group\"]\n",
    "    _data_group = data_pt[\"data_group\"]\n",
    "\n",
    "    n_day       = _meta_group[\"n_day\"]\n",
    "    n_ch        = _meta_group[\"n_ch\"]\n",
    "    n_valid_ch  = len(_data_group[\"Channels\"])\n",
    "\n",
    "    # ignore abnormal ele\n",
    "    if n_ch != 128 or n_valid_ch != n_ch:\n",
    "        if n_day < 5 or n_valid_ch <= 100:\n",
    "            continue\n",
    "\n",
    "    logger.info(f\"ELE [{i}/{n_ele}]: {ele_list[i][0]}\")\n",
    "\n",
    "\n",
    "    # Iteration by channel\n",
    "    for j in _data_group['Channels']:\n",
    "        _ch_data = _data_group[j][\"y_eval\"]\n",
    "        # _ch_data_log = np.log(_ch_data[:,:,0] + 1j*_ch_data[:,:,1])\n",
    "        # _ch_data[:,:,0] = np.real(_ch_data_log)\n",
    "        # _ch_data[:,:,1] = np.imag(_ch_data_log)\n",
    "        _ch_data = np.hstack((_ch_data[:,:,0],_ch_data[:,:,1]))\n",
    "        almost_data_list.append(_ch_data)\n",
    "        almost_start_list.append(_ch_data[0,:])\n",
    "\n",
    "\n",
    "        _ch_id = _ch_pattern.match(j)\n",
    "        _ch_id = int(_ch_id.group(1))\n",
    "\n",
    "        _id = [i, _ch_id] * np.shape(_ch_data)[0]\n",
    "        _id = np.array(_id).reshape(-1,2)\n",
    "        almost_id_list.append(_id)\n",
    "        almost_start_id_list.append(_id[0,:])\n",
    "\n",
    "almost_data_list = np.vstack(almost_data_list)\n",
    "almost_id_list = np.vstack(almost_id_list)\n",
    "almost_start_list = np.vstack(almost_start_list)\n",
    "almost_start_id_list = np.vstack(almost_start_id_list)\n",
    "\n",
    "\n",
    "del data_pt, _meta_group, _data_group, _ch_data\n",
    "gc.collect()\n",
    "\n",
    "almost_data_list.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12bd0b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = almost_data_list[:,:101] + 1j*almost_data_list[:,101:]\n",
    "# test_data = np.log(almost_data_list[:,:101] + 1j*almost_data_list[:,101:])\n",
    "test_data = np.concatenate([test_data.real, test_data.imag], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b994e710",
   "metadata": {},
   "source": [
    "### Archive_New"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca9326a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rootPath = \"D:/Baihm/EISNN/Archive_New/\"\n",
    "ele_list = SearchELE(rootPath)\n",
    "n_ele = len(ele_list)\n",
    "logger.info(f\"Search in {rootPath} and find {n_ele:03d} electrodes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68551906",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 首先我们把128/128看似完全没问题的这部分电极拿出来做聚类看看\n",
    "# 数据量也比较小，跑起来应该会更快\n",
    "\n",
    "DATASET_SUFFIX = \"Outlier_Ver02\"\n",
    "\n",
    "almost_start_list = []\n",
    "almost_start_id_list = []\n",
    "almost_data_list = []\n",
    "almost_id_list = []\n",
    "\n",
    "for i in range(n_ele):\n",
    "# for i in range(3):\n",
    "    fd_pt = os.path.join(ele_list[i][0], DATASET_SUFFIX, f\"{ele_list[i][1]}_{DATASET_SUFFIX}.pt\")\n",
    "    if not os.path.exists(fd_pt):\n",
    "        # logger.warning(f\"{fd_pt} does not exist\")\n",
    "        continue\n",
    "    data_pt = torch.load(fd_pt, weights_only=False)\n",
    "    _meta_group = data_pt[\"meta_group\"]\n",
    "    _data_group = data_pt[\"data_group\"]\n",
    "\n",
    "    n_day       = _meta_group[\"n_day\"]\n",
    "    n_ch        = _meta_group[\"n_ch\"]\n",
    "    n_valid_ch  = len(_data_group[\"Channels\"])\n",
    "\n",
    "\n",
    "    logger.info(f\"ELE [{i}/{n_ele}]: {ele_list[i][0]}\")\n",
    "\n",
    "\n",
    "    # Iteration by channel\n",
    "    for j in _data_group['Channels']:\n",
    "        _ch_data = _data_group[j][\"chData\"]\n",
    "        _ch_data_log = np.log(_ch_data[:,1,:] + 1j*_ch_data[:,2,:])\n",
    "        _ch_data[:,1,:] = np.real(_ch_data_log)\n",
    "        _ch_data[:,2,:] = np.imag(_ch_data_log)\n",
    "        _ch_data = np.hstack((_ch_data[:,1,:],_ch_data[:,2,:]))\n",
    "        almost_data_list.append(_ch_data)\n",
    "        almost_start_list.append(_ch_data[0,:])\n",
    "\n",
    "\n",
    "        _ch_id = j\n",
    "\n",
    "        _id = [i, _ch_id] * np.shape(_ch_data)[0]\n",
    "        _id = np.array(_id).reshape(-1,2)\n",
    "        almost_id_list.append(_id)\n",
    "        almost_start_id_list.append(_id[0,:])\n",
    "\n",
    "almost_data_list = np.vstack(almost_data_list)\n",
    "almost_id_list = np.vstack(almost_id_list)\n",
    "almost_start_list = np.vstack(almost_start_list)\n",
    "almost_start_id_list = np.vstack(almost_start_id_list)\n",
    "\n",
    "\n",
    "del data_pt, _meta_group, _data_group, _ch_data\n",
    "gc.collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf25c846",
   "metadata": {},
   "source": [
    "# Load Testdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c746e9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = almost_data_list[:,:101] + 1j*almost_data_list[:,101:]\n",
    "# test_data = np.log(almost_data_list[:,:101] + 1j*almost_data_list[:,101:])\n",
    "test_data = np.concatenate([test_data.real, test_data.imag], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb6db54",
   "metadata": {},
   "source": [
    "# Weird model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f85117",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d97fce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# rootPath = \"D:\\Baihm\\EISNN\\Archive_New/2025/2月/20047596_归档\"\n",
    "# rootPath = \"D:\\Baihm\\EISNN\\Archive_New/2025/2月/20047597_归档\"\n",
    "# rootPath = \"D:\\Baihm\\EISNN\\Archive/01067094_归档\"\n",
    "# rootPath = \"D:\\Baihm\\EISNN\\Archive/22037380_归档\"\n",
    "rootPath = \"D:\\Baihm\\EISNN\\Archive/10067077_归档\"\n",
    "\n",
    "\n",
    "EISDict = gatherCSV(rootPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0c6210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20047596_归档\n",
    "# ch_white_list = np.array([0,1,2,3,8,9,10,15,16,17,18,19,20,24,25,26,27,39,47,48,49,56,57,63,69,70,71,85,87,89,90,91,96,100,101,102,104,105,106,107])\n",
    "# day_black_list = [0,5, 9,15]\n",
    "\n",
    "# 20047597_归档\n",
    "# ch_white_list = np.array([1,3,8,10,15,17,24,26,27,39,55,65,68,70,71,79,80,85,86,87,90,95,101,103,106,107,111,120,121])\n",
    "# day_black_list = [0,5,9,10,15,16]\n",
    "\n",
    "# 01067094_归档\n",
    "# ch_white_list = np.array([0,1,2,3,7,8,10,15,16,17,18,19,20,24,25,26,27,28,39,47,48,50,51,52,55,56,57,59,63,65,68,69,70,71,85,86,87,95,96,97,100,101,102,103,104,105,107,111,122,123,124])\n",
    "# day_black_list = [1,4,5,6,8, 11,13,14,15,16]\n",
    "\n",
    "# day_white_list = [0,2,3,7,9,10]\n",
    "\n",
    "# 22037380_归档\n",
    "# ch_white_list = np.array([16,17,18,19,20,21,24,26,27,28])\n",
    "# ch_white_list = np.arange(128)\n",
    "# day_black_list = [1,8,9,10,11,12]\n",
    "\n",
    "\n",
    "# 10067077_归档\n",
    "ch_white_list = np.array([16,17,18,19,20,21,24,26,27,28])\n",
    "# ch_white_list = np.arange(128)\n",
    "day_black_list = [0,1,2,3,7,10]\n",
    "# day_white_list = [4,5,6,8,9,11,12,13,14]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a83f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = []\n",
    "_data = []\n",
    "for ch_id in ch_white_list:\n",
    "    chData = readChannel(ch_id, EISDict)\n",
    "    # all_data.append(chData[day_white_list])\n",
    "    # _data = chData[day_white_list]\n",
    "    _data = np.delete(chData, day_black_list, axis=0)\n",
    "\n",
    "    _eis = np.abs(_data[:,1,2500:] + 1j*_data[:,2,2500:])\n",
    "    _data = _data[(_eis<1e6).all(axis=1),:,:]\n",
    "    \n",
    "    all_data.append(_data)\n",
    "\n",
    "all_data = np.concatenate(all_data, axis=0)\n",
    "all_data.shape\n",
    "\n",
    "chEIS = np.abs(all_data[:,1,:] + 1j*all_data[:,2,:])\n",
    "plt.figure()\n",
    "for i in range(all_data.shape[0]):\n",
    "    plt.semilogy(chEIS[i,:], label = f\"{i}\")\n",
    "    # plt.plot(chEIS[i,:], label = f\"{i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c0a941",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save(\"./Weird/EIS_10067077_weird.npy\", all_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3802c2c",
   "metadata": {},
   "source": [
    "## Feature Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4dba842",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = []\n",
    "all_data.append(np.load(\"./Weird/EIS_20047596_weird.npy\"))\n",
    "all_data.append(np.load(\"./Weird/EIS_20047597_weird.npy\"))\n",
    "all_data.append(np.load(\"./Weird/EIS_01067094_weird.npy\"))\n",
    "all_data.append(np.load(\"./Weird/EIS_22037380_weird.npy\"))\n",
    "all_data.append(np.load(\"./Weird/EIS_10067077_weird.npy\"))\n",
    "all_data = np.concatenate(all_data, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6791aa31",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axis = plt.subplots(1,2, figsize=(12,6))\n",
    "for i in range(all_data.shape[0]):\n",
    "# for i in range(550,600):\n",
    "    chEIS = all_data[i,1,:] + 1j*all_data[i,2,:]\n",
    "    axis[0].semilogy(np.abs(chEIS), label = f\"{i}\", alpha = 0.1)\n",
    "    axis[1].plot(np.angle(chEIS), label = f\"{i}\", alpha = 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3699ec84",
   "metadata": {},
   "source": [
    "## SVM for failure mode detction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a06149",
   "metadata": {},
   "source": [
    "### Input layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf67d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_list = np.linspace(0, 5000-1,101,dtype=int, endpoint=True)\n",
    "weird_data = np.log(all_data[:,1,freq_list] + 1j*all_data[:,2,freq_list])\n",
    "\n",
    "weird_data = np.concatenate([weird_data.real, weird_data.imag], axis=1)\n",
    "\n",
    "print(f\"weird_data shape: {weird_data.shape}\")\n",
    "# print(f\"test_data shape: {test_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7224a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "weird_cluster_Archive_New = np.load(\"D:\\Baihm\\EISNN\\Dataset\\Anomaly\\Weird\\Archive_New_cluster.npy\")\n",
    "weird_cluster_Archive     = np.load(\"D:\\Baihm\\EISNN\\Dataset\\Anomaly\\Weird\\Archive_Weird_cluster.npy\")\n",
    "weird_data = np.vstack([weird_data,weird_cluster_Archive_New])\n",
    "weird_data = np.vstack([weird_data,weird_cluster_Archive])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e035f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax0 = fig.add_subplot(121)\n",
    "ax1 = fig.add_subplot(122)\n",
    "\n",
    "\n",
    "for i in range(weird_data.shape[0]):\n",
    "    ax0.semilogy(np.exp(weird_data[i,:101]), label = f\"{i}\", alpha=0.005)\n",
    "    ax1.plot(np.rad2deg(weird_data[i,101:]), label = f\"{i}\", alpha=0.005)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc46881",
   "metadata": {},
   "source": [
    "#### Calib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2203cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    _poi_data = np.zeros_like(all_data)\n",
    "    phz_calibration = np.loadtxt(\"./phz_Calib.txt\")\n",
    "    for i in range(np.shape(all_data)[0]):\n",
    "        # ch_eis = EIS_recal(chData[i,:,:])\n",
    "        ch_eis = EIS_recal_ver02(all_data[i,:,:], phz_calibration)\n",
    "        _poi_data[i,:,:] = ch_eis\n",
    "\n",
    "    plt.figure()\n",
    "    for i in range(_poi_data.shape[0]):\n",
    "        _poi_eis = np.log(_poi_data[i,1,:] + 1j*_poi_data[i,2,:])\n",
    "        plt.plot(_poi_eis.real, label = f\"{i}\",alpha=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d015af76",
   "metadata": {},
   "source": [
    "### SVM Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd38e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "weirdSVMmodel = OneClassSVM(kernel='rbf', gamma='auto', nu=0.01)  # nu 调整宽松程度\n",
    "weirdSVMmodel.fit(weird_data)\n",
    "\n",
    "# joblib.dump(weirdSVMmodel, \"weirdSVMmodel.pkl\")\n",
    "# joblib.dump(weirdSVMmodel, \"../../Outlier/weirdSVMmodel.pkl\")\n",
    "# joblib.dump(weirdSVMmodel, \"weirdSVMmodel_20250516_01.pkl\")\n",
    "# joblib.dump(weirdSVMmodel, \"../../Outlier/weirdSVMmodel_20250516_01.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4832653",
   "metadata": {},
   "outputs": [],
   "source": [
    "_scores = weirdSVMmodel.decision_function(test_data)  # 越大越像训练数据\n",
    "\n",
    "_probs = expit(_scores * 5)  # 可调整缩放因子以控制置信度\n",
    "\n",
    "# Step 5: 拼出 m x 2 输出\n",
    "weirdProbs = np.stack([_probs, 1 - _probs], axis=1)\n",
    "print(weirdProbs.shape)  # (m, 2)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(_scores[:])\n",
    "# plt.plot(weirdProbs[:,0])\n",
    "\n",
    "\n",
    "weird_test_data = test_data[_probs>0.5]\n",
    "\n",
    "fig = plt.figure()\n",
    "ax0 = fig.add_subplot(121)\n",
    "ax1 = fig.add_subplot(122)\n",
    "\n",
    "\n",
    "for i in range(weird_test_data.shape[0]):\n",
    "    ax0.semilogy(np.exp(weird_test_data[i,:101]), label = f\"{i}\", alpha=0.005)\n",
    "    ax1.plot(np.rad2deg(weird_test_data[i,101:]), label = f\"{i}\", alpha=0.005)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2caaf8f2",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65109bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "_pca_m = PCA(n_components = 10)\n",
    "_scale = StandardScaler()\n",
    "_data_norm = _scale.fit_transform(test_data)\n",
    "# _data_norm = _scale.fit_transform(almost_data_list)\n",
    "_pca_data = _pca_m.fit_transform(_data_norm)\n",
    "\n",
    "cmap = plt.colormaps.get_cmap(\"rainbow_r\")\n",
    "# cmap = plt.colormaps.get_cmap(\"Set1\")\n",
    "\n",
    "plt.figure(figsize=(9,9))\n",
    "plt.scatter(_pca_data[:,0],_pca_data[:,1],color=cmap(_probs), s=0.01)\n",
    "# plt.scatter(_pca_data[:,0],_pca_data[:,1],color=cmap(_probs>0.5), s=0.01)\n",
    "# plt.scatter(_pca_data[:,0],_pca_data[:,1],color=cmap(weirdProbs[:,0]>0.45), s=0.01)\n",
    "# plt.gca().set_aspect('equal', adjustable='box')\n",
    "plt.title('PCA')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32df6347",
   "metadata": {},
   "source": [
    "## Weird Criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5c8750",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weirdCriterion(model:OneClassSVM, test_data, threshold=0.5):\n",
    "    '''==================================================\n",
    "        Define the criterion of weird data\n",
    "        Parameter: \n",
    "            model: trained OneClassSVM model\n",
    "            test_data: data to be tested [n x 202] - (logZ)\n",
    "            threshold: threshold of weird data\n",
    "        Returen:\n",
    "            weird_mask: True for weird data\n",
    "        ==================================================\n",
    "    '''\n",
    "    _scores = model.decision_function(test_data) \n",
    "\n",
    "    _probs = expit(_scores * 5)\n",
    "\n",
    "    weird_mask = _probs > threshold\n",
    "    # weird_mask = (_probs > 0.4) & (_probs < 0.41)\n",
    "\n",
    "    return weird_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63cf02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "_model = joblib.load(\"weirdSVMmodel.pkl\")\n",
    "weird_mask = weirdCriterion(_model, test_data, threshold=0.5)\n",
    "\n",
    "weird_test_data = test_data[weird_mask]\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "for i in range(weird_test_data.shape[0]):\n",
    "    plt.plot(weird_test_data[i,:101], label = f\"{i}\", alpha=0.01)\n",
    "    # plt.plot(weird_test_data[i,101:], label = f\"{i}\", alpha=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3db3a8f",
   "metadata": {},
   "source": [
    "### PCA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5390a929",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "_pca_m = PCA(n_components = 10)\n",
    "_scale = StandardScaler()\n",
    "_data_norm = _scale.fit_transform(almost_data_list)\n",
    "_pca_data = _pca_m.fit_transform(_data_norm)\n",
    "\n",
    "# cmap = plt.colormaps.get_cmap(\"rainbow_r\")\n",
    "cmap = plt.colormaps.get_cmap(\"Set1\")\n",
    "\n",
    "plt.figure(figsize=(9,9))\n",
    "plt.scatter(_pca_data[:,0],_pca_data[:,1],color=cmap(weird_mask), s=0.01)\n",
    "# plt.scatter(_pca_data[:,0],_pca_data[:,1],color=cmap(weirdProbs[:,0]>0.45), s=0.01)\n",
    "# plt.gca().set_aspect('equal', adjustable='box')\n",
    "plt.title('PCA')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29639243",
   "metadata": {},
   "source": [
    "# Short Criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03cbc448",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shortCriterion(freq, test_data, threshold = np.log(1e4)):\n",
    "    '''==================================================\n",
    "        Define the criterion of short data\n",
    "        Parameter: \n",
    "            freq: frequency of EIS data [101,]\n",
    "            test_data: data to be tested [n x 202] - (logZ)\n",
    "            threshold: threshold of short data\n",
    "        Returen:\n",
    "            short_mask: True for shorted data\n",
    "        ==================================================\n",
    "    '''\n",
    "    _freq_short_mask = np.zeros(test_data.shape[1])\n",
    "    _freq_short_mask[:_freq_short_mask.shape[0]//2] = freq > 1e4\n",
    "    _freq_short_mask = _freq_short_mask.astype(bool)\n",
    "\n",
    "    short_mask = np.all(test_data[:,_freq_short_mask] < threshold, axis=1)\n",
    "\n",
    "    return short_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbb5964",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_list = np.linspace(0,5000-1,101,dtype=int, endpoint=True)\n",
    "_freq_all = chData[0,0,freq_list]\n",
    "\n",
    "short_mask = shortCriterion(_freq_all, test_data, threshold=np.log(1e4))\n",
    "\n",
    "short_test_data = test_data[short_mask]\n",
    "\n",
    "fig, axis = plt.subplots(1,2)\n",
    "\n",
    "for i in range(short_test_data.shape[0]):\n",
    "    axis[0].loglog(_freq_all, np.exp(short_test_data[i,:101]), label = f\"{i}\", alpha=0.005)\n",
    "    axis[1].semilogx(_freq_all, np.rad2deg(short_test_data[i,101:]), label = f\"{i}\", alpha=0.005)\n",
    "    axis[0].grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26243d62",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c802b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "_pca_m = PCA(n_components = 10)\n",
    "_scale = StandardScaler()\n",
    "_data_norm = _scale.fit_transform(almost_data_list)\n",
    "_pca_data = _pca_m.fit_transform(_data_norm)\n",
    "\n",
    "\n",
    "# cmap = plt.colormaps.get_cmap(\"rainbow_r\")\n",
    "cmap = plt.colormaps.get_cmap(\"Set1\")\n",
    "\n",
    "plt.figure(figsize=(9,9))\n",
    "plt.scatter(_pca_data[:,0],_pca_data[:,1],color=cmap(short_mask.astype(int)+2), s=0.01)\n",
    "plt.title('PCA')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da95f7ed",
   "metadata": {},
   "source": [
    "# Open Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ebfe6a4",
   "metadata": {},
   "source": [
    "## Input Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4192567",
   "metadata": {},
   "outputs": [],
   "source": [
    "open_data = np.load(\"D:\\Baihm\\EISNN\\Dataset\\Anomaly\\Open\\EIS_Open.npy\")\n",
    "open_data_Archive_New = np.load(\"D:\\Baihm\\EISNN\\Dataset\\Anomaly\\Open\\Archive_New_Open_cluster.npy\")\n",
    "open_data = np.vstack([open_data, open_data_Archive_New])\n",
    "print(f\"weird_data shape: {open_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9584c0e5",
   "metadata": {},
   "source": [
    "## SVM Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49354eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "openSVMmodel = OneClassSVM(kernel='rbf', gamma='auto', nu=0.005)  # nu 调整宽松程度\n",
    "openSVMmodel.fit(open_data)\n",
    "\n",
    "# joblib.dump(openSVMmodel, \"openSVMmodel.pkl\")\n",
    "# joblib.dump(openSVMmodel, \"../../Outlier/openSVMmodel.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd79e993",
   "metadata": {},
   "outputs": [],
   "source": [
    "_scores = openSVMmodel.decision_function(test_data)  # 越大越像训练数据\n",
    "\n",
    "_probs = expit(_scores * 5) # 可调整缩放因子以控制置信度\n",
    "\n",
    "# Step 5: 拼出 m x 2 输出\n",
    "openProbs = np.stack([_probs, 1 - _probs], axis=1)\n",
    "print(openProbs.shape)  # (m, 2)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(_scores[:])\n",
    "# plt.plot(weirdProbs[:,0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9dd127",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cecbc25",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "_pca_m = PCA(n_components = 10)\n",
    "_scale = StandardScaler()\n",
    "_data_norm = _scale.fit_transform(almost_data_list)\n",
    "_pca_data = _pca_m.fit_transform(_data_norm)\n",
    "\n",
    "# cmap = plt.colormaps.get_cmap(\"rainbow_r\")\n",
    "cmap = plt.colormaps.get_cmap(\"Set1\")\n",
    "\n",
    "plt.figure(figsize=(9,9))\n",
    "plt.scatter(_pca_data[:,0],_pca_data[:,1],color=cmap(_probs>0.5), s=0.01)\n",
    "# plt.scatter(_pca_data[:,0],_pca_data[:,1],color=cmap((_probs>0.1) & (_probs<0.2)), s=0.01)\n",
    "# plt.gca().set_aspect('equal', adjustable='box')\n",
    "plt.title('PCA')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a239b95a",
   "metadata": {},
   "source": [
    "## Open Criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2fc2788",
   "metadata": {},
   "outputs": [],
   "source": [
    "def openCriterion_threshold(freq, test_data, threshold = np.log(3e6)):\n",
    "    '''==================================================\n",
    "        Define the criterion of open data\n",
    "        Parameter: \n",
    "            freq: frequency of EIS data [101,]\n",
    "            test_data: data to be tested [n x 202] - (logZ)\n",
    "            threshold: threshold of open data\n",
    "        Returen:\n",
    "            open_mask: True for open data\n",
    "        ==================================================\n",
    "    '''\n",
    "    _freq_open_mask = np.zeros(test_data.shape[1])\n",
    "    _freq_open_mask[:_freq_open_mask.shape[0]//2] = freq < 1e3\n",
    "    _freq_open_mask = _freq_open_mask.astype(bool)\n",
    "\n",
    "    # open_mask = np.all(test_data[:,_freq_open_mask] > threshold, axis=1)\n",
    "    open_mask = np.all((test_data[:,_freq_open_mask] > np.log(2e6)), axis=1)\n",
    "\n",
    "    return open_mask\n",
    "\n",
    "def openCriterion(model:OneClassSVM, test_data, threshold=0.5):\n",
    "    '''==================================================\n",
    "        Define the criterion of open data\n",
    "        Parameter: \n",
    "            model: trained OneClassSVM model\n",
    "            test_data: data to be tested [n x 202] - (logZ)\n",
    "            threshold: threshold of weird data\n",
    "        Returen:\n",
    "            open_mask: True for open data\n",
    "        ==================================================\n",
    "    '''\n",
    "    _scores = model.decision_function(test_data) \n",
    "\n",
    "    _probs = expit(_scores * 5)\n",
    "\n",
    "    open_mask = _probs > threshold\n",
    "\n",
    "    return open_mask\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7212b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "_model = joblib.load(\"openSVMmodel.pkl\")\n",
    "open_mask = openCriterion(_model, test_data, threshold=0.5)\n",
    "\n",
    "open_test_data = test_data[open_mask]\n",
    "\n",
    "\n",
    "freq_list = np.linspace(0,5000-1,101,dtype=int, endpoint=True)\n",
    "# _freq_all = chData[0,0,freq_list]\n",
    "_freq_all = np.logspace(0,6,101, endpoint=True)\n",
    "_rand_ch = np.floor(np.random.rand(1000)*open_test_data.shape[0]).astype(int)\n",
    "  \n",
    "plt.figure()\n",
    "for i in range(_rand_ch.shape[0]):\n",
    "    plt.loglog(_freq_all, np.exp(open_test_data[_rand_ch[i],:101]), label = f\"{i}\", alpha=0.05)\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798c5042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# freq_list = np.linspace(0,5000-1,101,dtype=int, endpoint=True)\n",
    "# _freq_all = chData[0,0,freq_list]\n",
    "\n",
    "# # open_mask = openCriterion(_freq_all, test_data, threshold=np.log(3e6))\n",
    "# open_mask = openCriterion(_freq_all, test_data)\n",
    "\n",
    "# open_test_data = test_data[open_mask]\n",
    "\n",
    "# open_test_data.shape\n",
    "\n",
    "\n",
    "# fig, axis = plt.subplots(1,2)\n",
    "\n",
    "# for i in range(open_test_data.shape[0]):\n",
    "#     axis[0].loglog(_freq_all, np.exp(open_test_data[i,:101]), label = f\"{i}\", alpha=0.005)\n",
    "#     axis[1].semilogx(_freq_all, np.rad2deg(open_test_data[i,101:]), label = f\"{i}\", alpha=0.005)\n",
    "#     axis[0].grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408f21ba",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a3a58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "_pca_m = PCA(n_components = 10)\n",
    "_scale = StandardScaler()\n",
    "_data_norm = _scale.fit_transform(almost_data_list)\n",
    "_pca_data = _pca_m.fit_transform(_data_norm)\n",
    "\n",
    "\n",
    "# cmap = plt.colormaps.get_cmap(\"rainbow_r\")\n",
    "cmap = plt.colormaps.get_cmap(\"Set1\")\n",
    "\n",
    "plt.figure(figsize=(9,9))\n",
    "plt.scatter(_pca_data[:,0],_pca_data[:,1],color=cmap(open_mask.astype(int)), s=0.01)\n",
    "plt.title('PCA')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EISNN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
