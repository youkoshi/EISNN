{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "from scipy.spatial.distance import squareform\n",
    "from scipy import stats\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "from sklearn.cluster import OPTICS\n",
    "from sklearn.mixture import BayesianGaussianMixture\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "import re\n",
    "import os\n",
    "from loguru import logger\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib.patches as mpatches\n",
    "%matplotlib qt\n",
    "\n",
    "from collections import defaultdict\n",
    "# from datetime import datetime\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gatherCSV(rootPath, outsuffix = 'Tracking'):\n",
    "    '''==================================================\n",
    "        Collect all EIS.csv files in the rootPath\n",
    "        Parameter: \n",
    "            rootPath: current search path\n",
    "            outsuffix: Saving path of EIS.csv files\n",
    "        Returen:\n",
    "            EISDict: a 2D-dict of EIS data\n",
    "            Storage Frame: EISDict[_sessionIndex][_channelIndex] = \"_filepath\"\n",
    "        ==================================================\n",
    "    '''\n",
    "    _filename       = None\n",
    "    _filepath       = None\n",
    "    _trackpath      = None\n",
    "    _csvpath        = None\n",
    "    _sessionIndex   = None\n",
    "    _channelIndex   = None\n",
    "    _processed      = None\n",
    "\n",
    "    EISDict = defaultdict(dict)\n",
    "\n",
    "    ## Iterate session\n",
    "    session_pattern = re.compile(r\"(.+?)_(\\d{8})_01\")\n",
    "    bank_pattern    = re.compile(r\"([1-4])\")\n",
    "    file_pattern    = re.compile(r\"EIS_ch(\\d{3})\\.csv\")\n",
    "\n",
    "    ## RootDir\n",
    "    for i in os.listdir(rootPath):\n",
    "        match_session = session_pattern.match(i)\n",
    "        ## SessionDir\n",
    "        if match_session:\n",
    "            logger.info(f\"Session Begin: {i}\")\n",
    "            _sessionIndex = match_session[2]\n",
    "            for j in os.listdir(f\"{rootPath}/{i}\"):\n",
    "                match_bank = bank_pattern.match(j)\n",
    "                ## BankDir\n",
    "                if match_bank:\n",
    "                    logger.info(f\"Bank Begin: {j}\")\n",
    "                    _trackpath = f\"{rootPath}/{i}/{j}/{outsuffix}\"\n",
    "                    if not os.path.exists(_trackpath):\n",
    "                        continue\n",
    "\n",
    "                    for k in os.listdir(f\"{rootPath}/{i}/{j}/{outsuffix}\"):\n",
    "                        match_file = file_pattern.match(k)\n",
    "                        ## File\n",
    "                        if match_file:\n",
    "                            _filename = k\n",
    "                            _filepath = f\"{rootPath}/{i}/{j}/{outsuffix}/{k}\"\n",
    "                            _channelIndex = (int(match_bank[1])-1)*32+int(match_file[1])\n",
    "                            \n",
    "                            EISDict[_sessionIndex][_channelIndex] = f\"{rootPath}/{i}/{j}/{outsuffix}/{k}\"\n",
    "                            \n",
    "    return EISDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Readout\n",
    "def readChannel(chID, fileDict):\n",
    "    '''==================================================\n",
    "        Read EIS.csv file by Channel\n",
    "        Parameter: \n",
    "            chID: channel index\n",
    "            fileDict: EISDict[_sessionIndex][_channelIndex] = \"_filepath\"\n",
    "        Returen:\n",
    "            freq: frequency\n",
    "            Zreal: real part of impedance\n",
    "            Zimag: imaginary part of impedance\n",
    "        ==================================================\n",
    "    '''\n",
    "    chData = []\n",
    "    for ssID in fileDict.keys():\n",
    "        _data   = np.loadtxt(fileDict[ssID][chID], delimiter=',')\n",
    "        _freq   = _data[:,0]\n",
    "        _Zreal  = _data[:,3]\n",
    "        _Zimag  = _data[:,4]\n",
    "        chData.append(np.stack((_freq, _Zreal, _Zimag),axis=0))\n",
    "\n",
    "    return np.stack(chData, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EIS_recal(data):\n",
    "    f_poi = data[0,:]\n",
    "    # Z_poi = data[1,:] * np.exp(1j*np.deg2rad(data[2,:]))\n",
    "    Z_poi = data[1,:] + 1j*data[2,:]\n",
    "    Y_poi = 1/Z_poi\n",
    "\n",
    "    Rg0 = 1.611e13\n",
    "    Cp0 = 1.4e-9\n",
    "    \n",
    "    _Rg0_rescale = 1/Rg0*np.power(f_poi,1.583)\n",
    "    _Cp0_rescale = Cp0*np.power(f_poi,0.911)\n",
    "    Y_org = Y_poi - _Rg0_rescale + 1j*_Cp0_rescale\n",
    "    Z_org = 1/Y_org\n",
    "\n",
    "    # Amp Calibration\n",
    "    Z_ampC = np.abs(Z_org)\n",
    "\n",
    "    # Phz Calibration\n",
    "    Z_phzC = np.angle(Z_org)\n",
    "    \n",
    "    Z_rec = Z_ampC * np.exp(1j*Z_phzC)\n",
    "\n",
    "    \n",
    "    return np.transpose(np.array([f_poi, np.real(Z_rec), np.imag(Z_rec)])).T\n",
    "\n",
    "\n",
    "def EIS_recal_ver02(data, _phz_0 = None):\n",
    "    f_poi = data[0,:]\n",
    "    # Z_poi = data[1,:] * np.exp(1j*np.deg2rad(data[2,:]))\n",
    "    Z_poi = data[1,:] + 1j*data[2,:]\n",
    "    Y_poi = 1/Z_poi\n",
    "\n",
    "    Rg0 = 1.611e13\n",
    "    Cp0 = 1.4e-9\n",
    "    \n",
    "    _Rg0_rescale = 1/Rg0*np.power(f_poi,1.583)\n",
    "    _Cp0_rescale = Cp0*np.power(f_poi,0.911)\n",
    "    Y_org = Y_poi - _Rg0_rescale + 1j*_Cp0_rescale\n",
    "    Z_org = 1/Y_org\n",
    "\n",
    "    # Phz Calibration\n",
    "    if _phz_0 is None:\n",
    "        _phz_0 = np.loadtxt(\"./phz_Calib.txt\")\n",
    "    \n",
    "    Z_ampC = np.abs(Z_org)\n",
    "    # Z_phzC = np.angle(Z_org) - _phz_0\n",
    "    Z_phzC = np.angle(Z_org) - _phz_0\n",
    "\n",
    "    Z_rec = Z_ampC * np.exp(1j*Z_phzC)\n",
    "\n",
    "    # C = 5e-10\n",
    "    Rs0 = 100\n",
    "    Z_rec = Z_rec - Rs0\n",
    "\n",
    "\n",
    "\n",
    "    Cp0 = 5e-10\n",
    "    _Cp0_rescale = Cp0 * f_poi\n",
    "    Z_rec = 1/(1/Z_rec - 1j * _Cp0_rescale)\n",
    "\n",
    "    \n",
    "\n",
    "    # Ls0 = 1.7e-4\n",
    "    Ls0 = 5e-4\n",
    "    _Ls0_rescale = Ls0 * f_poi\n",
    "    Z_rec = Z_rec - 1j * _Ls0_rescale\n",
    "\n",
    "    # C = 5e-10\n",
    "    Rs0 = 566\n",
    "    Z_rec = Z_rec - Rs0\n",
    "    \n",
    "    return np.stack([f_poi, np.real(Z_rec), np.imag(Z_rec)], axis=1).T\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-08 18:08:19.309\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mSession Begin: 22037380_20241205_01\u001b[0m\n",
      "\u001b[32m2025-05-08 18:08:19.310\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 1\u001b[0m\n",
      "\u001b[32m2025-05-08 18:08:19.311\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 2\u001b[0m\n",
      "\u001b[32m2025-05-08 18:08:19.311\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 3\u001b[0m\n",
      "\u001b[32m2025-05-08 18:08:19.312\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 4\u001b[0m\n",
      "\u001b[32m2025-05-08 18:08:19.313\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mSession Begin: 22037380_20241206_01\u001b[0m\n",
      "\u001b[32m2025-05-08 18:08:19.314\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 1\u001b[0m\n",
      "\u001b[32m2025-05-08 18:08:19.315\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 2\u001b[0m\n",
      "\u001b[32m2025-05-08 18:08:19.316\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 3\u001b[0m\n",
      "\u001b[32m2025-05-08 18:08:19.317\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 4\u001b[0m\n",
      "\u001b[32m2025-05-08 18:08:19.318\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mSession Begin: 22037380_20241207_01\u001b[0m\n",
      "\u001b[32m2025-05-08 18:08:19.319\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 1\u001b[0m\n",
      "\u001b[32m2025-05-08 18:08:19.322\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 2\u001b[0m\n",
      "\u001b[32m2025-05-08 18:08:19.323\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 3\u001b[0m\n",
      "\u001b[32m2025-05-08 18:08:19.326\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 4\u001b[0m\n",
      "\u001b[32m2025-05-08 18:08:19.328\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mSession Begin: 22037380_20241208_01\u001b[0m\n",
      "\u001b[32m2025-05-08 18:08:19.330\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 1\u001b[0m\n",
      "\u001b[32m2025-05-08 18:08:19.332\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 2\u001b[0m\n",
      "\u001b[32m2025-05-08 18:08:19.333\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 3\u001b[0m\n",
      "\u001b[32m2025-05-08 18:08:19.335\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 4\u001b[0m\n",
      "\u001b[32m2025-05-08 18:08:19.337\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mSession Begin: 22037380_20241209_01\u001b[0m\n",
      "\u001b[32m2025-05-08 18:08:19.338\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 1\u001b[0m\n",
      "\u001b[32m2025-05-08 18:08:19.339\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 2\u001b[0m\n",
      "\u001b[32m2025-05-08 18:08:19.341\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 3\u001b[0m\n",
      "\u001b[32m2025-05-08 18:08:19.342\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 4\u001b[0m\n",
      "\u001b[32m2025-05-08 18:08:19.343\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mSession Begin: 22037380_20241210_01\u001b[0m\n",
      "\u001b[32m2025-05-08 18:08:19.344\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 1\u001b[0m\n",
      "\u001b[32m2025-05-08 18:08:19.345\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 2\u001b[0m\n",
      "\u001b[32m2025-05-08 18:08:19.346\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 3\u001b[0m\n",
      "\u001b[32m2025-05-08 18:08:19.348\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 4\u001b[0m\n",
      "\u001b[32m2025-05-08 18:08:19.349\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mSession Begin: 22037380_20241211_01\u001b[0m\n",
      "\u001b[32m2025-05-08 18:08:19.351\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 1\u001b[0m\n",
      "\u001b[32m2025-05-08 18:08:19.352\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 2\u001b[0m\n",
      "\u001b[32m2025-05-08 18:08:19.353\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 3\u001b[0m\n",
      "\u001b[32m2025-05-08 18:08:19.354\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 4\u001b[0m\n",
      "\u001b[32m2025-05-08 18:08:19.355\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mSession Begin: 22037380_20241212_01\u001b[0m\n",
      "\u001b[32m2025-05-08 18:08:19.355\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 1\u001b[0m\n",
      "\u001b[32m2025-05-08 18:08:19.356\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 2\u001b[0m\n",
      "\u001b[32m2025-05-08 18:08:19.356\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 3\u001b[0m\n",
      "\u001b[32m2025-05-08 18:08:19.356\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 4\u001b[0m\n",
      "\u001b[32m2025-05-08 18:08:19.357\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mSession Begin: 22037380_20241213_01\u001b[0m\n",
      "\u001b[32m2025-05-08 18:08:19.357\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 1\u001b[0m\n",
      "\u001b[32m2025-05-08 18:08:19.357\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 2\u001b[0m\n",
      "\u001b[32m2025-05-08 18:08:19.358\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 3\u001b[0m\n",
      "\u001b[32m2025-05-08 18:08:19.358\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 4\u001b[0m\n",
      "\u001b[32m2025-05-08 18:08:19.359\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mSession Begin: 22037380_20241214_01\u001b[0m\n",
      "\u001b[32m2025-05-08 18:08:19.359\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 1\u001b[0m\n",
      "\u001b[32m2025-05-08 18:08:19.360\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 2\u001b[0m\n",
      "\u001b[32m2025-05-08 18:08:19.361\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 3\u001b[0m\n",
      "\u001b[32m2025-05-08 18:08:19.362\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 4\u001b[0m\n",
      "\u001b[32m2025-05-08 18:08:19.363\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mSession Begin: 22037380_20241215_01\u001b[0m\n",
      "\u001b[32m2025-05-08 18:08:19.364\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 1\u001b[0m\n",
      "\u001b[32m2025-05-08 18:08:19.365\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 2\u001b[0m\n",
      "\u001b[32m2025-05-08 18:08:19.365\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 3\u001b[0m\n",
      "\u001b[32m2025-05-08 18:08:19.366\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 4\u001b[0m\n",
      "\u001b[32m2025-05-08 18:08:19.367\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mSession Begin: 22037380_20241216_01\u001b[0m\n",
      "\u001b[32m2025-05-08 18:08:19.368\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 1\u001b[0m\n",
      "\u001b[32m2025-05-08 18:08:19.368\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 2\u001b[0m\n",
      "\u001b[32m2025-05-08 18:08:19.369\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 3\u001b[0m\n",
      "\u001b[32m2025-05-08 18:08:19.369\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 4\u001b[0m\n",
      "\u001b[32m2025-05-08 18:08:19.371\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mSession Begin: 22037380_20241217_01\u001b[0m\n",
      "\u001b[32m2025-05-08 18:08:19.371\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 1\u001b[0m\n",
      "\u001b[32m2025-05-08 18:08:19.371\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 2\u001b[0m\n",
      "\u001b[32m2025-05-08 18:08:19.372\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 3\u001b[0m\n",
      "\u001b[32m2025-05-08 18:08:19.372\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgatherCSV\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mBank Begin: 4\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "PLOT_FLAG = False\n",
    "\n",
    "\n",
    "# rootPath = \"D:/Baihm/EISNN/Archive/01037160_归档\"  \n",
    "# ch_id = 0   # Default  \n",
    "# ch_id = 1   # 居然分成一类了？？？\n",
    "# ch_id = 9   # 异常值没被筛出去，类似还有3\n",
    "# ch_id = 7  # Normal Example\n",
    "# ch_id = 20  # Normal to Short, Same to GPR  \n",
    "# ch_id = 52    \n",
    "# ch_id = 89  \n",
    "\n",
    "\n",
    "# rootPath = \"D:/Baihm/EISNN/Archive/01037161_归档\"   # #2\n",
    "# ch_id = 10  # Error - Ap结果不连续\n",
    "\n",
    "# rootPath = \"D:/Baihm/EISNN/Dataset/05087163_归档\"\n",
    "# ch_id = 7   # one outlier\n",
    "# ch_id = 50  # No outlier but in two Phases\n",
    "# ch_id = 55  # One outlier &wired end point\n",
    "# ch_id = 114 # Open Circuit with on outpler\n",
    "\n",
    "\n",
    "# rootPath = \"D:\\Baihm\\EISNN\\Archive/06047730_归档\"\n",
    "# ch_id = 41  # outlier detection error but has 20 samples\n",
    "\n",
    "# rootPath = \"D:/Baihm/EISNN/Archive/02067447_归档\"\n",
    "# ch_id = 68  # Short all the time\n",
    "\n",
    "# rootPath = \"D:/Baihm/EISNN/Archive/01067095_归档\"\n",
    "# ch_id = 19    # First Sample is outlier\n",
    "\n",
    "# rootPath = \"D:/Baihm/EISNN/Archive/09290511_归档\"\n",
    "# ch_id = 13    # Up & Down, 2 outliers\n",
    "# ch_id = 21    # Normal + 2 outlier\n",
    "# ch_id = 41    # Normal + 2 outlier - *(Hard To Tell)\n",
    "# ch_id = 79    # 3-class, What a mess\n",
    "\n",
    "# rootPath = \"D:/Baihm/EISNN/Archive/11057712_归档\"\n",
    "# ch_id = 106    # Very Good Electrode with 1 hidden outlier, and one phase shift\n",
    "\n",
    "# rootPath = \"D:\\Baihm\\EISNN\\Archive/10057084_归档\"\n",
    "# ch_id = 16    # Totaly Mess\n",
    "# ch_id = 18    # Totaly Mess\n",
    "\n",
    "# rootPath = \"D:\\Baihm\\EISNN\\Archive/11067223_归档\"\n",
    "# ch_id = 124     # Perfect but two phase with one outlier\n",
    "\n",
    "\n",
    "# rootPath = \"D:\\Baihm\\EISNN\\Archive/15361101_归档\"\n",
    "# ch_id = 0     # Only One Sample - Run With Error\n",
    "\n",
    "\n",
    "# rootPath = \"D:\\Baihm\\EISNN\\Archive/11207147_归档\"\n",
    "# ch_id = 0     # Only Three Sample - Run With Error\n",
    "\n",
    "\n",
    "# rootPath = \"D:\\Baihm\\EISNN\\Archive/06017758_归档\"\n",
    "# ch_id = 96     # Perfect of Perfect\n",
    "\n",
    "\n",
    "rootPath = \"D:\\Baihm\\EISNN\\Archive/22037380_归档\"\n",
    "ch_id = 20     # Connection Error\n",
    "\n",
    "\n",
    "EISDict = gatherCSV(rootPath)\n",
    "chData_full = readChannel(ch_id, EISDict)\n",
    "freq_list = np.linspace(1000,np.shape(chData_full)[2]-1,101,dtype=int, endpoint=True)\n",
    "\n",
    "if False:\n",
    "    phz_calibration = np.loadtxt(\"./phz_Calib.txt\")\n",
    "    for i in range(np.shape(chData)[0]):\n",
    "        # ch_eis = EIS_recal(chData[i,:,:])\n",
    "        ch_eis = EIS_recal_ver02(chData[i,:,:], phz_calibration)\n",
    "        chData[i,:,:] = ch_eis\n",
    "chData = chData_full[:,:,freq_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PLOT_FLAG:\n",
    "    fig, axis = plt.subplots(1,4,figsize=(15,6))\n",
    "    cmap = plt.colormaps.get_cmap('rainbow_r')\n",
    "    for i in range(np.shape(chData)[0]):\n",
    "    # for i in [0,4,2,11]:\n",
    "        ch_eis = chData[i,:,:]\n",
    "        # ch_eis = EIS_recal(chData[i,:,:])[:,freq_list]\n",
    "        _color = cmap(i/np.shape(chData)[0])\n",
    "        axis[0].loglog(ch_eis[0,:], np.abs(ch_eis[1,:]+1j*ch_eis[2,:]), color = _color, linewidth=2, label=f\"Session {i}\")\n",
    "        axis[1].semilogx(ch_eis[0,:], np.rad2deg(np.angle(ch_eis[1,:]+1j*ch_eis[2,:])), color = _color, linewidth=2, label=f\"Session {i}\")\n",
    "        axis[2].plot(ch_eis[1,:], -ch_eis[2,:], color = _color, linewidth=2, label=f\"Session {i}\")\n",
    "        # axis[4].loglog(ch_eis[1,:], -ch_eis[2,:], color = _color, linewidth=2, label=f\"Session {i}\")\n",
    "    \n",
    "        # _poi_Z = np.log(np.abs(ch_eis[1,:]+1j*ch_eis[2,:]))\n",
    "        # _poi_P = np.angle(ch_eis[1,:]+1j*ch_eis[2,:])\n",
    "        # _poi_eis = _poi_Z * np.exp(1j*_poi_P)\n",
    "        # axis[3].plot(np.real(_poi_eis), -np.imag(_poi_eis), color = _color, linewidth=2, label=f\"Session {i}\")\n",
    "        _poi_Z = np.log(ch_eis[1,:]+1j*ch_eis[2,:])\n",
    "        axis[3].plot(np.real(_poi_Z), -np.imag(_poi_Z), color = _color, linewidth=2, label=f\"Session {i}\")\n",
    "        \n",
    "\n",
    "    axis[0].legend(frameon=False, loc='upper left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outlier_Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DTW Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "REVERSED_DTW_FLAG = True\n",
    "\n",
    "if not REVERSED_DTW_FLAG:\n",
    "    # # From 0,0 -> n-1,n-1\n",
    "    def dtw_calculation(s1, s2):\n",
    "        len_s1, len_s2 = np.shape(s1)[0], np.shape(s2)[0]\n",
    "        dtw_matrix = np.full((len_s1 + 1, len_s2 + 1), np.inf)\n",
    "        dtw_trace = np.zeros((len_s1 + 1, len_s2 + 1, 2))\n",
    "        dtw_matrix[0, 0] = 0\n",
    "        for i in range(1, len_s1 + 1):\n",
    "            for j in range(1, len_s2 + 1):\n",
    "                cost = np.abs(s1[i - 1] - s2[j - 1])\n",
    "                penalty = [ dtw_matrix[i - 1, j],\n",
    "                            dtw_matrix[i, j - 1],\n",
    "                            dtw_matrix[i - 1, j - 1]]\n",
    "                i_p = np.argmin(penalty)\n",
    "                dtw_matrix[i, j] = cost + penalty[i_p]\n",
    "                if i_p == 0: dtw_trace[i, j] = [i - 1, j]  \n",
    "                elif i_p==1: dtw_trace[i, j] = [i, j - 1]\n",
    "                elif i_p==2: dtw_trace[i, j] = [i - 1, j - 1]\n",
    "        dtw_sequence = []\n",
    "        i,j = int(len_s1), int(len_s2)\n",
    "        while  i!=0 and j!=0:\n",
    "            dtw_sequence.append([i-1, j-1])     # Have to be i-1 & j-1, because matrix range from 1 to len + 1 \n",
    "            i,j = int(dtw_trace[i, j, 0]), int(dtw_trace[i, j, 1])\n",
    "\n",
    "\n",
    "        return [dtw_matrix[1:,1:], np.array(dtw_sequence)]\n",
    "\n",
    "\n",
    "else: \n",
    "    # From n-1,n-1 -> 0,0\n",
    "    def dtw_calculation(s1, s2):\n",
    "        len_s1, len_s2 = np.shape(s1)[0], np.shape(s2)[0]\n",
    "        dtw_matrix = np.full((len_s1 + 1, len_s2 + 1), np.inf)\n",
    "        dtw_trace = np.zeros((len_s1 + 1, len_s2 + 1, 2))\n",
    "        dtw_matrix[len_s1, len_s1] = 0\n",
    "        for i in reversed(range(0, len_s1)):\n",
    "            for j in reversed(range(0, len_s2)):\n",
    "                cost = np.abs(s1[i] - s2[j])\n",
    "                penalty = [ dtw_matrix[i + 1, j],\n",
    "                            dtw_matrix[i, j + 1],\n",
    "                            dtw_matrix[i + 1, j + 1]]\n",
    "                i_p = np.argmin(penalty)\n",
    "                dtw_matrix[i, j] = cost + penalty[i_p]\n",
    "                if i_p == 0: dtw_trace[i, j] = [i + 1, j]  \n",
    "                elif i_p==1: dtw_trace[i, j] = [i, j + 1]\n",
    "                elif i_p==2: dtw_trace[i, j] = [i + 1, j + 1]\n",
    "        dtw_sequence = []\n",
    "        i,j = 0, 0\n",
    "        while  i!=len_s1-1 and j!=len_s1-1:\n",
    "            dtw_sequence.append([i, j])     # Have to be i-1 & j-1, because matrix range from 1 to len + 1 \n",
    "            i,j = int(dtw_trace[i, j, 0]), int(dtw_trace[i, j, 1])\n",
    "\n",
    "\n",
    "        return [dtw_matrix[:-1,:-1], np.array(dtw_sequence)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DTW - Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch_EIS = chData[:,1,:] + 1j*chData[:,2,:]\n",
    "\n",
    "# 计算DTW距离矩阵\n",
    "# data = ch_DRT\n",
    "# data = ch_EIS\n",
    "data = np.log(ch_EIS)\n",
    "\n",
    "num_samples = np.shape(data)[0]\n",
    "num_data = np.shape(data)[1]\n",
    "dtw_dist_value = np.zeros((num_samples,num_samples))\n",
    "dtw_dist_matrix = defaultdict(lambda: defaultdict(list))\n",
    "dtw_dist_trace = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "\n",
    "# plt.figure()\n",
    "for i in range(num_samples):\n",
    "    dtw_dist_matrix[i][i] = np.zeros((np.shape(data[i])[0],np.shape(data[i])[0]))\n",
    "    dtw_dist_value[i,i] = 0\n",
    "    for j in range(i + 1, num_samples):\n",
    "        distance, dtw_sequence = dtw_calculation(data[i], data[j])\n",
    "        dtw_dist_matrix[i][j] = distance\n",
    "        dtw_dist_matrix[j][i] = distance\n",
    "        dtw_dist_trace[i][j] = [dtw_sequence[:,0],dtw_sequence[:,1]]\n",
    "        dtw_dist_trace[j][i] = [dtw_sequence[:,1],dtw_sequence[:,0]]\n",
    "\n",
    "        if REVERSED_DTW_FLAG:\n",
    "            dtw_dist_value[i,j] = distance[0,0]\n",
    "            dtw_dist_value[j,i] = distance[0,0]\n",
    "        else: \n",
    "            dtw_dist_value[i,j] = distance[-1,-1]\n",
    "            dtw_dist_value[j,i] = distance[-1,-1]\n",
    "\n",
    "        # plt.plot(dtw_sequence[:,0],dtw_sequence[:,1])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot DTW Dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    _n = num_samples\n",
    "    _m = num_samples\n",
    "\n",
    "    fig, axis = plt.subplots(_n,_m)\n",
    "    # cmap = plt.colormaps.get_cmap('rainbow_r')\n",
    "    # colors = cmap(np.linspace(0,1,num_samples))\n",
    "\n",
    "\n",
    "    for i in range(_n):\n",
    "        for j in range(_m):\n",
    "            # if i==j: continue\n",
    "            axis[i,j].imshow(dtw_dist_matrix[i][j], cmap='coolwarm', interpolation='nearest')\n",
    "        # axis[int(i/_m),int(i%_m)].legend()\n",
    "\n",
    "if False:\n",
    "    _n = num_samples\n",
    "    _m = num_samples\n",
    "    plt.figure()\n",
    "    for i in range(_n):\n",
    "        for j in range(_m):\n",
    "            plt.imshow(dtw_dist_value, cmap='coolwarm', interpolation='nearest')\n",
    "        # axis[int(i/_m),int(i%_m)].legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot DTW Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    _n = 3\n",
    "    _m = int(num_samples/3)+1\n",
    "\n",
    "    fig, axis = plt.subplots(_n,_m)\n",
    "    cmap = plt.colormaps.get_cmap('rainbow_r')\n",
    "    colors = cmap(np.linspace(0,1,num_samples))\n",
    "\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        for j in range(num_samples):\n",
    "            if i==j: continue\n",
    "            axis[int(i/_m),int(i%_m)].plot(dtw_dist_trace[i][j][0][:],dtw_dist_trace[i][j][1][:],color = colors[j])\n",
    "        # axis[int(i/_m),int(i%_m)].legend()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DTW Phase Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Distance & Phase from DTW Data\n",
    "def DTW_Amp(dtw_dist_inst, reversed = False):\n",
    "    # dtw_dist_avg = dtw_dist_inst[-1][-1]/(np.shape(dtw_dist_inst)[0]-1)\n",
    "    if reversed:\n",
    "        dtw_dist_avg = dtw_dist_inst[0][0]\n",
    "    else:\n",
    "        dtw_dist_avg = dtw_dist_inst[-1][-1]\n",
    "    return dtw_dist_avg\n",
    "\n",
    "def DTW_Phz(dtw_trace_inst, reversed = False):\n",
    "    if reversed:\n",
    "        _x = dtw_trace_inst[0]\n",
    "        _y = dtw_trace_inst[1]\n",
    "\n",
    "    else:\n",
    "        _x = dtw_trace_inst[0][::-1]\n",
    "        _y = dtw_trace_inst[1][::-1]\n",
    "\n",
    "    _x_diff = np.diff(_x[:])\n",
    "    _y_diff = np.diff(_y[:])\n",
    "\n",
    "    phz_scale = _x_diff + _y_diff\n",
    "    phz_value = np.array(_x[1:] - _y[1:])\n",
    "\n",
    "    dtw_phz_norm = np.sum(phz_scale * phz_value) / (_x[-1]**2)      # _x[-1] & _y[-1] = len(curve)-1\n",
    "\n",
    "    return dtw_phz_norm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtw_temporal_imag = np.zeros((num_samples,num_samples,2))\n",
    "\n",
    "for i in range(num_samples):\n",
    "    for j in range(i+1, num_samples):\n",
    "        _amp = DTW_Amp(dtw_dist_matrix[i][j], REVERSED_DTW_FLAG)\n",
    "        _phz = DTW_Phz(dtw_dist_trace[i][j], REVERSED_DTW_FLAG)\n",
    "        dtw_temporal_imag[i, j] = [_amp,_phz]\n",
    "        dtw_temporal_imag[j, i] = [-_amp,_phz]  # Flip \n",
    "\n",
    "dtw_temporal_vec = dtw_temporal_imag[:,:,0] * np.exp(1j * dtw_temporal_imag[:,:,1] * np.pi / 2)\n",
    "\n",
    "dtw_temporal_dist = np.zeros((num_samples,num_samples))\n",
    "for i in range(num_samples):\n",
    "    for j in range(i+1, num_samples):\n",
    "        dtw_temporal_dist[i,j] = np.sum(np.abs(dtw_temporal_vec[i,:] - dtw_temporal_vec[j,:]))\n",
    "        dtw_temporal_dist[j,i] = dtw_temporal_dist[i,j]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Plot Phase Space Dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PLOT_FLAG:\n",
    "    plt.figure()\n",
    "    plt.imshow(dtw_temporal_dist, cmap='coolwarm', interpolation='nearest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot Phase Space By Time [i,j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PLOT_FLAG:\n",
    "    _n = 3\n",
    "    _m = int(num_samples/3)+1\n",
    "\n",
    "    fig, axis = plt.subplots(_n,_m, sharex=True, sharey=True)\n",
    "    cmap = plt.colormaps.get_cmap('rainbow_r')\n",
    "    colors = cmap(np.linspace(0,1,num_samples))\n",
    "\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        for j in range(num_samples):\n",
    "            if i==j: \n",
    "                axis[int(i/_m),int(i%_m)].scatter(0,0,color = colors[j])\n",
    "            else:\n",
    "                _poi = dtw_temporal_vec[i,j]\n",
    "                axis[int(i/_m),int(i%_m)].scatter(np.real(_poi),np.imag(_poi),color = colors[j])\n",
    "        # axis[int(i/_m),int(i%_m)].legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot Phase Space By Sample [j,i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    _n = 3\n",
    "    _m = int(num_samples/3)+1\n",
    "\n",
    "    fig, axis = plt.subplots(_n,_m, sharex=True, sharey=True)\n",
    "    cmap = plt.colormaps.get_cmap('rainbow_r')\n",
    "    colors = cmap(np.linspace(0,1,num_samples))\n",
    "\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        for j in range(num_samples):\n",
    "            if i==j: \n",
    "                axis[int(i/_m),int(i%_m)].scatter(0,0,color = colors[j])\n",
    "            else:\n",
    "                _poi = dtw_temporal_vec[j,i]\n",
    "                axis[int(i/_m),int(i%_m)].scatter(np.real(_poi),np.imag(_poi),color = colors[j])\n",
    "        # axis[int(i/_m),int(i%_m)].legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    from sklearn.decomposition import PCA\n",
    "    from sklearn.manifold import TSNE, Isomap, LocallyLinearEmbedding, MDS\n",
    "    import umap.umap_ as umap  # 请确保安装了 umap-learn\n",
    "\n",
    "    dtw_manifold_stack_vec = np.zeros((num_samples , num_samples*2))\n",
    "    for i in range(num_samples):\n",
    "        dtw_manifold_stack_vec[i,:] = np.hstack((dtw_temporal_vec[i,:].real, dtw_temporal_vec[i,:].imag))\n",
    "\n",
    "\n",
    "    data = dtw_manifold_stack_vec\n",
    "\n",
    "    _order = 3\n",
    "    methods = {\n",
    "        'PCA': PCA(n_components=_order),\n",
    "        't-SNE': TSNE(n_components=_order, perplexity=5, random_state=42),  # perplexity 设置为 5\n",
    "        'Isomap': Isomap(n_components=_order),\n",
    "        'LLE': LocallyLinearEmbedding(n_components=_order, random_state=42),\n",
    "        'MDS': MDS(n_components=_order, random_state=42),\n",
    "        'UMAP': umap.UMAP(n_components=_order, random_state=42)\n",
    "    }\n",
    "\n",
    "    embeddings = {}\n",
    "    emb_dist = {}\n",
    "    for name, method in methods.items():\n",
    "        embedding = method.fit_transform(data)\n",
    "        embeddings[name] = embedding\n",
    "        \n",
    "        _x = embedding[:,0].flatten()\n",
    "        _y = embedding[:,1].flatten()\n",
    "\n",
    "        emb_dist[name] = np.sqrt((_x[:, np.newaxis] - _x[np.newaxis, :])**2 + \n",
    "                            (_y[:, np.newaxis] - _y[np.newaxis, :])**2)\n",
    "        \n",
    "\n",
    "\n",
    "    fig, axis = plt.subplots(3,4,figsize=(12,6))\n",
    "    for i, (name, emb) in enumerate(embeddings.items()):\n",
    "        _x = emb[:,0].flatten()\n",
    "        _y = emb[:,1].flatten()\n",
    "\n",
    "        _dist = np.sqrt((_x[:, np.newaxis] - _x[np.newaxis, :])**2 + \n",
    "                            (_y[:, np.newaxis] - _y[np.newaxis, :])**2)\n",
    "        # _dist = emb_dist[name]\n",
    "\n",
    "        axis[np.int16(i/2),(i%2)*2].scatter(emb[:, 0], emb[:, 1], c=np.arange(np.shape(data)[0]), cmap='rainbow_r', edgecolor='k', s=100)\n",
    "        axis[np.int16(i/2),(i%2)*2].set_title(name)\n",
    "\n",
    "        s = axis[np.int16(i/2),(i%2)*2+1].imshow(_dist, cmap='coolwarm', interpolation='nearest')\n",
    "        fig.colorbar(s, ax=axis[np.int16(i/2),(i%2)*2+1])\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DTW Order Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linkage Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13, 13)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = squareform(dtw_temporal_dist)\n",
    "b = dtw_temporal_dist\n",
    "\n",
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = linkage(squareform(dtw_temporal_dist), method='single', optimal_ordering=True)  # 使用 Ward 方式进行层次聚类\n",
    "# Z = linkage(squareform(dtw_dist_value), method='ward', optimal_ordering=True)  # 使用 Ward 方式进行层次聚类\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plot Linkage Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PLOT_FLAG:\n",
    "    plt.figure(figsize=(8,4))\n",
    "    _ = dendrogram(Z, labels=np.arange(num_samples))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construct DTW Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DTW_TREE:\n",
    "    def __init__(self, id, value = np.inf):\n",
    "        self.id = id\n",
    "        self.value = value\n",
    "        self.l = None\n",
    "        self.r = None\n",
    "        self.p = None\n",
    "\n",
    "    def display(self):\n",
    "        lines, *_ = self._display_aux()\n",
    "        for line in lines: print(line)\n",
    "    def _display_aux(self):\n",
    "        if self.l is None and self.r is None:\n",
    "            # _s = f\"{self.id:03d}:{self.value:03d}\"\n",
    "            _s = f\"{self.id:03d}\"\n",
    "            _w = len(_s)\n",
    "            _h = 1\n",
    "            _m = _w//2\n",
    "            return [_s], _w, _h, _m\n",
    "        elif self.r is None:\n",
    "            _sr, _wr, _hr, _mr = self.l._display_aux()\n",
    "            # _s = f\"{self.id:03d}:{self.value:03d}\"\n",
    "            _s = f\"{self.id:03d}\"\n",
    "            _w = len(_s)\n",
    "            \n",
    "            _line_0 = (_mr + 1) * ' ' + (_wr - _mr - 1) * '_' + _s\n",
    "            _line_1 = _mr * ' ' + '/' + (_wr - _mr - 1 + _w) * ' '\n",
    "            _lins_s = [line + _w * ' ' for line in _sr]\n",
    "            return [_line_0, _line_1] + _lins_s, _wr + _w, _hr + 2, _wr + _w // 2\n",
    "        elif self.l is None:\n",
    "            _sl, _wl, _hl, _ml = self.r._display_aux()\n",
    "            # _s = f\"{self.id:03d}:{self.value:03d}\"\n",
    "            _s = f\"{self.id:03d}\"\n",
    "            _w = len(_s)\n",
    "            _line_0 = _s + _ml * '_' + (_wl - _ml) * ' '\n",
    "            _line_1 = (_w + _ml) * ' ' + '\\\\' + (_wl - _ml - 1) * ' '\n",
    "            _lins_s = [_w * ' ' + line for line in _sl]\n",
    "            return [_line_0, _line_1] + _lins_s, _wl + _w, _hl + 2, _w // 2\n",
    "        else:\n",
    "            _sl, _wl, _hl, _ml = self.l._display_aux()\n",
    "            _sr, _wr, _hr, _mr = self.r._display_aux()\n",
    "            # _s = f\"{self.id:03d}:{self.value:03d}\"\n",
    "            _s = f\"{self.id:03d}\"\n",
    "            _w = len(_s)\n",
    "            first_line = (_ml + 1) * ' ' + (_wl - _ml - 1) * '_' + _s + _mr * '_' + (_wr - _mr) * ' '\n",
    "            second_line = _ml * ' ' + '/' + (_wl - _ml - 1 + _w + _mr) * ' ' + '\\\\' + (_wr - _mr - 1) * ' '\n",
    "            if _hl < _hr:\n",
    "                _sl += [_wl * ' '] * (_hr - _hl)\n",
    "            elif _hr < _hl:\n",
    "                _sr += [_wr * ' '] * (_hl - _hr)\n",
    "            zipped_lines = zip(_sl, _sr)\n",
    "            lines = [first_line, second_line] + [a + _w * ' ' + b for a, b in zipped_lines]\n",
    "            return lines, _wl + _wr + _w, max(_hl, _hr) + 2, _wl + _w // 2\n",
    "\n",
    "\n",
    "def dtw_tree_merge(dtw_tree_A, dtw_tree_B, id):\n",
    "    if dtw_tree_A.value < dtw_tree_B.value:\n",
    "        dtw_tree_root = DTW_TREE(id, dtw_tree_A.value)\n",
    "        dtw_tree_root.l = dtw_tree_A\n",
    "        dtw_tree_root.r = dtw_tree_B\n",
    "    else:\n",
    "        dtw_tree_root = DTW_TREE(id, dtw_tree_B.value)\n",
    "        dtw_tree_root.l = dtw_tree_B\n",
    "        dtw_tree_root.r = dtw_tree_A\n",
    "    dtw_tree_A.p = dtw_tree_root\n",
    "    dtw_tree_B.p = dtw_tree_root\n",
    "    return dtw_tree_root\n",
    "\n",
    "\n",
    "def dtw_leaf_ordering(dtw_tree):\n",
    "    if dtw_tree is None: return []\n",
    "    if dtw_tree.l is None and dtw_tree.r is None: return [dtw_tree.value]\n",
    "    return dtw_leaf_ordering(dtw_tree.l) + dtw_leaf_ordering(dtw_tree.r)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.00000000e+01, 1.10000000e+01, 6.22011937e+01, 2.00000000e+00],\n",
       "       [7.00000000e+00, 8.00000000e+00, 6.59984316e+01, 2.00000000e+00],\n",
       "       [9.00000000e+00, 1.30000000e+01, 1.05099121e+02, 3.00000000e+00],\n",
       "       [2.00000000e+00, 3.00000000e+00, 1.52244376e+02, 2.00000000e+00],\n",
       "       [6.00000000e+00, 5.00000000e+00, 1.83287163e+02, 2.00000000e+00],\n",
       "       [1.50000000e+01, 1.20000000e+01, 2.33033063e+02, 4.00000000e+00],\n",
       "       [1.60000000e+01, 1.70000000e+01, 4.04212154e+02, 4.00000000e+00],\n",
       "       [0.00000000e+00, 1.90000000e+01, 4.31296430e+02, 5.00000000e+00],\n",
       "       [2.00000000e+01, 4.00000000e+00, 6.20542277e+02, 6.00000000e+00],\n",
       "       [2.10000000e+01, 1.00000000e+00, 6.88872071e+02, 7.00000000e+00],\n",
       "       [2.20000000e+01, 1.80000000e+01, 1.45105963e+03, 1.10000000e+01],\n",
       "       [1.40000000e+01, 2.30000000e+01, 2.25101994e+03, 1.30000000e+01]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         ______________________024____     \n",
      "                                        /                             \\    \n",
      "                                   ____023________________          _014_  \n",
      "                                  /                       \\        /     \\ \n",
      "                             ____022_          __________018_     007   008\n",
      "                            /        \\        /              \\             \n",
      "     ______________________021_     001     _015____        012            \n",
      "    /                          \\           /        \\                      \n",
      "  _020__________              004         009     _013_                    \n",
      " /              \\                                /     \\                   \n",
      "000        ____019____                          010   011                  \n",
      "          /           \\                                                    \n",
      "        _016_       _017_                                                  \n",
      "       /     \\     /     \\                                                 \n",
      "      002   003   005   006                                                \n"
     ]
    }
   ],
   "source": [
    "_node_cnt = num_samples \n",
    "dtw_node_list = []\n",
    "for i in range(num_samples):\n",
    "    dtw_node_list.append(DTW_TREE(i,i))\n",
    "\n",
    "for i in Z:\n",
    "    dtw_node_list.append(dtw_tree_merge(dtw_node_list[int(i[0])], dtw_node_list[int(i[1])],_node_cnt))\n",
    "    _node_cnt = _node_cnt + 1\n",
    "\n",
    "dtw_node_list[-1].display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construct DTW Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_longest_increasing_subsequences(seq):\n",
    "    if not seq:\n",
    "        return []\n",
    "    \n",
    "    n = len(seq)\n",
    "    dp = [1] * n\n",
    "    prev = [[] for _ in range(n)]\n",
    "    \n",
    "    for i in range(n):\n",
    "        for j in range(i):\n",
    "            if seq[j] < seq[i]:\n",
    "                if dp[j] + 1 > dp[i]:\n",
    "                    dp[i] = dp[j] + 1\n",
    "                    prev[i] = [j]\n",
    "                elif dp[j] + 1 == dp[i]:\n",
    "                    prev[i].append(j)\n",
    "    \n",
    "    max_len = max(dp)\n",
    "    end_indices = [i for i in range(n) if dp[i] == max_len]\n",
    "    \n",
    "    def backtrack(i):\n",
    "        if not prev[i]:\n",
    "            return [[seq[i]]]\n",
    "        res = []\n",
    "        for j in prev[i]:\n",
    "            for subseq in backtrack(j):\n",
    "                res.append(subseq + [seq[i]])\n",
    "        return res\n",
    "\n",
    "    result = []\n",
    "    for i in end_indices:\n",
    "        result.extend(backtrack(i))\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "def longest_sequence_dist(dist_matrix, sequence):\n",
    "    _dist = 0\n",
    "    for i in range(1,len(sequence)):\n",
    "        _dist = _dist + dist_matrix[i-1,i]\n",
    "    return _dist\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-08 17:03:19.180\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1m\n",
      " Ordered Linkage Tree: \t\t\t[0, 2, 3, 5, 6, 4, 1, 9, 10, 11, 12, 7, 8]\n",
      " Longest Increasing Subsequence: \t[[0, 2, 3, 5, 6, 9, 10, 11, 12]]\n",
      " Min-Dist Subsequence: \t\t\t[ 0  2  3  5  6  9 10 11 12]\n",
      " Outliers: \t\t\t\t[4 1 7 8]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "leaf_full = dtw_leaf_ordering(dtw_node_list[-1])\n",
    "# Find All longest_increasing_subsequences\n",
    "leaf_ordering = all_longest_increasing_subsequences(leaf_full)\n",
    "\n",
    "optimal_seq_dist = longest_sequence_dist(dtw_temporal_dist, leaf_ordering[0])\n",
    "optimal_seq_id = 0\n",
    "for i in range(1, len(leaf_ordering)):\n",
    "    _dist = longest_sequence_dist(dtw_temporal_dist, leaf_ordering[i])\n",
    "    if _dist < optimal_seq_dist:\n",
    "        optimal_seq_dist = _dist\n",
    "        optimal_seq_id = i\n",
    "\n",
    "leaf_optimal_seq = np.array(leaf_ordering[optimal_seq_id])\n",
    "leaf_optimal_len = np.shape(leaf_optimal_seq)[0]\n",
    "\n",
    "leaf_anomaly = np.array([poi for poi in leaf_full if poi not in leaf_optimal_seq])\n",
    "logger.info(f\"\\n Ordered Linkage Tree: \\t\\t\\t{leaf_full}\\n Longest Increasing Subsequence: \\t{leaf_ordering}\\n Min-Dist Subsequence: \\t\\t\\t{leaf_optimal_seq}\\n Outliers: \\t\\t\\t\\t{leaf_anomaly}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-08 17:03:19.203\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m19\u001b[0m - \u001b[1m\n",
      " [Affinity] #Cluster: 2, #Score: 0.7844332508826075 \n",
      " Cluster: [0 0 0 0 0 1 1 1 1]\u001b[0m\n",
      "\u001b[32m2025-05-08 17:03:19.332\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m36\u001b[0m - \u001b[1m\n",
      " [OPTICS] #Cluster: 4, #Score: 0.6670633359337405 \n",
      " Cluster: [-1  0  0  1  1  2  2  2  2]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "dtw_seq_vec = np.zeros((leaf_optimal_len, leaf_optimal_len*2))\n",
    "for i in range(leaf_optimal_len):\n",
    "    dtw_seq_vec[i,:] = np.hstack((dtw_temporal_vec[leaf_optimal_seq[i],leaf_optimal_seq].real, dtw_temporal_vec[leaf_optimal_seq[i],leaf_optimal_seq].imag))\n",
    "\n",
    "# # ———— Affinity Clustering ————\n",
    "best_model = None\n",
    "best_score = -np.inf\n",
    "for seed in [7,42,1999]:\n",
    "    aff_prop = AffinityPropagation()\n",
    "    aff_prop.fit(dtw_seq_vec)\n",
    "    _score = silhouette_score(dtw_seq_vec, aff_prop.labels_)\n",
    "    if _score > best_score:\n",
    "        best_score = _score\n",
    "        best_model = aff_prop.labels_\n",
    "\n",
    "dtw_cluster = best_model\n",
    "n_clusters = len(np.unique(dtw_cluster))\n",
    "\n",
    "logger.info(f\"\\n [Affinity] #Cluster: {n_clusters}, #Score: {best_score} \\n Cluster: {dtw_cluster}\")\n",
    "\n",
    "# ———— OPTICS ————\n",
    "optics = OPTICS(\n",
    "    min_samples=2,        # 核心点的最小邻居数，默认5\n",
    "    xi=0.05,              # 用于提取簇的最小斜率，默认0.05\n",
    "    metric='euclidean',   # 在特征空间上用欧氏距离\n",
    ")\n",
    "optics.fit(dtw_seq_vec)\n",
    "labels_opt = optics.labels_   # 噪声点标记为 -1\n",
    "mask = labels_opt >= 0\n",
    "if mask.sum() > 1:\n",
    "    score_opt = silhouette_score(dtw_seq_vec[mask], labels_opt[mask])\n",
    "\n",
    "\n",
    "dtw_cluster = labels_opt\n",
    "n_clusters = len(np.unique(dtw_cluster))\n",
    "logger.info(f\"\\n [OPTICS] #Cluster: {n_clusters}, #Score: {score_opt} \\n Cluster: {dtw_cluster}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plot Anomaly & Ordering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PLOT_FLAG:\n",
    "# fig, axis = plt.subplots(1,3, figsize=(15,5), constrained_layout=True)\n",
    "    fig= plt.figure(figsize=(12,6))\n",
    "    cmap = plt.colormaps.get_cmap('rainbow_r')\n",
    "    axis = [1,2,3,4,5,6]\n",
    "    axis[0] = fig.add_subplot(2,3,1)\n",
    "    axis[1] = fig.add_subplot(2,3,2)\n",
    "    axis[2] = fig.add_subplot(2,3,3)\n",
    "    axis[3] = fig.add_subplot(2,3,4, projection='3d')\n",
    "    axis[4] = fig.add_subplot(2,3,5, projection='3d')\n",
    "    # axis[5] = fig.add_subplot(2,3,6, projection='3d')\n",
    "\n",
    "    axis[0].set_title(\"All Samples\")\n",
    "    axis[1].set_title(\"Ordering Samples\")\n",
    "    axis[2].set_title(\"Anomaly Samples\")\n",
    "\n",
    "    for i in leaf_optimal_seq:\n",
    "        ch_eis = chData[i,:,:]\n",
    "        _color = cmap(i/num_samples)\n",
    "        axis[0].loglog(ch_eis[0,:], np.abs(ch_eis[1,:]+1j*ch_eis[2,:]), color = _color, linewidth=2, label=f\"S{i:02d}\")\n",
    "        axis[1].loglog(ch_eis[0,:], np.abs(ch_eis[1,:]+1j*ch_eis[2,:]), color = _color, linewidth=2, label=f\"S{i:02d}\")\n",
    "    axis[1].legend()\n",
    "    axis[1].sharex(axis[0])\n",
    "    axis[1].sharey(axis[0])\n",
    "\n",
    "\n",
    "    for i in leaf_anomaly:\n",
    "        ch_eis = chData[i,:,:]\n",
    "        _color = cmap(i/num_samples)\n",
    "        axis[0].loglog(ch_eis[0,:], np.abs(ch_eis[1,:]+1j*ch_eis[2,:]), color = _color, linewidth=2, label=f\"S{i:02d}\")\n",
    "        axis[2].loglog(ch_eis[0,:], np.abs(ch_eis[1,:]+1j*ch_eis[2,:]), color = _color, linewidth=2, label=f\"S{i:02d}\")\n",
    "    axis[2].legend()\n",
    "    axis[2].sharex(axis[0])\n",
    "    axis[2].sharey(axis[0])\n",
    "\n",
    "\n",
    "\n",
    "    init_elev = 21  # 仰角\n",
    "    init_azim = 55  # 方位角\n",
    "    axis[3].view_init(elev=init_elev, azim=init_azim)\n",
    "    axis[4].view_init(elev=init_elev, azim=init_azim)\n",
    "    # axis[5].view_init(elev=init_elev, azim=init_azim)\n",
    "\n",
    "    _x = np.arange(num_samples)\n",
    "    _y = np.log10(chData[0,0,:]).flatten()\n",
    "    X, Y = np.meshgrid(_x, _y, indexing='ij')\n",
    "    axis[3].plot_surface(X, Y, np.log10(np.abs(chData[:,1,:]+1j*chData[:,2,:])), cmap='viridis_r', alpha=0.8)\n",
    "\n",
    "    _x = np.arange(num_samples)[leaf_optimal_seq]\n",
    "    _y = np.log10(chData[0,0,:]).flatten()\n",
    "    X, Y = np.meshgrid(_x, _y, indexing='ij')\n",
    "    axis[4].plot_surface(X, Y, np.log10(np.abs(chData[leaf_optimal_seq,1,:]+1j*chData[leaf_optimal_seq,2,:])), cmap='viridis_r', alpha=0.8)\n",
    "\n",
    "    # _x = np.arange(num_samples)\n",
    "    # _y = np.log10(chData[0,0,:]).flatten()\n",
    "    # X, Y = np.meshgrid(_x, _y, indexing='ij')\n",
    "    # axis[3].plot_surface(X, Y, np.log10(np.abs(chData[:,1,:]+1j*chData[:,2,:])), cmap='viridis_r', alpha=0.8)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### State Transition Detection "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Affinity Propagation Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-08 17:03:19.380\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m19\u001b[0m - \u001b[1m\n",
      " [Affinity] #Cluster: 2, #Score: 0.7844332508826075 \n",
      " Cluster: [0 0 0 0 0 1 1 1 1]\u001b[0m\n",
      "\u001b[32m2025-05-08 17:03:19.394\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m36\u001b[0m - \u001b[1m\n",
      " [OPTICS] #Cluster: 4, #Score: 0.6670633359337405 \n",
      " Cluster: [-1  0  0  1  1  2  2  2  2]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "dtw_seq_vec = np.zeros((leaf_optimal_len, leaf_optimal_len*2))\n",
    "for i in range(leaf_optimal_len):\n",
    "    dtw_seq_vec[i,:] = np.hstack((dtw_temporal_vec[leaf_optimal_seq[i],leaf_optimal_seq].real, dtw_temporal_vec[leaf_optimal_seq[i],leaf_optimal_seq].imag))\n",
    "\n",
    "# # ———— Affinity Clustering ————\n",
    "best_model = None\n",
    "best_score = -np.inf\n",
    "for seed in [7,42,1999]:\n",
    "    aff_prop = AffinityPropagation()\n",
    "    aff_prop.fit(dtw_seq_vec)\n",
    "    _score = silhouette_score(dtw_seq_vec, aff_prop.labels_)\n",
    "    if _score > best_score:\n",
    "        best_score = _score\n",
    "        best_model = aff_prop.labels_\n",
    "\n",
    "dtw_cluster = best_model\n",
    "n_clusters = len(np.unique(dtw_cluster))\n",
    "\n",
    "logger.info(f\"\\n [Affinity] #Cluster: {n_clusters}, #Score: {best_score} \\n Cluster: {dtw_cluster}\")\n",
    "\n",
    "# ———— OPTICS ————\n",
    "optics = OPTICS(\n",
    "    min_samples=2,        # 核心点的最小邻居数，默认5\n",
    "    xi=0.01,             # 用于提取簇的最小斜率，默认0.05\n",
    "    metric='euclidean',   # 在特征空间上用欧氏距离\n",
    ")\n",
    "optics.fit(dtw_seq_vec)\n",
    "labels_opt = optics.labels_   # 噪声点标记为 -1\n",
    "mask = labels_opt >= 0\n",
    "if mask.sum() > 1:\n",
    "    score_opt = silhouette_score(dtw_seq_vec[mask], labels_opt[mask])\n",
    "\n",
    "\n",
    "dtw_cluster = labels_opt\n",
    "n_clusters = len(np.unique(dtw_cluster))\n",
    "logger.info(f\"\\n [OPTICS] #Cluster: {n_clusters}, #Score: {score_opt} \\n Cluster: {dtw_cluster}\")\n",
    "\n",
    "if False:\n",
    "    plt.figure()\n",
    "    \n",
    "    plt.plot(np.arange(optics.reachability_.shape[0]),optics.reachability_[optics.ordering_])\n",
    "    plt.title('Reachability Plot')\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def errro_cluster_remover(dtw_cluster, leaf_optimal_seq):\n",
    "    A_org = np.array(leaf_optimal_seq)\n",
    "    B_org = np.array(dtw_cluster)\n",
    "    \n",
    "    # Remove -1\n",
    "    A = A_org[B_org != -1]\n",
    "    B = B_org[B_org != -1]\n",
    "\n",
    "    # Maintain ascending sereis\n",
    "    C = np.append(np.diff(B),0)\n",
    "    \n",
    "    mask = C < 0\n",
    "    bad_classes = np.unique(B[mask])\n",
    "\n",
    "    keep_mask = ~np.isin(B, bad_classes)\n",
    "\n",
    "    A_filtered = A[keep_mask]\n",
    "    B_filtered = B[keep_mask]\n",
    "\n",
    "    unique_vals = np.unique(B_filtered)\n",
    "    new_labels = np.arange(len(unique_vals))\n",
    "    mapping = dict(zip(unique_vals, new_labels))\n",
    "\n",
    "    B_remapped = np.vectorize(mapping.get)(B_filtered)\n",
    "\n",
    "    return A_filtered,B_remapped\n",
    "\n",
    "def errro_cluster_remover_ver02(dtw_cluster, leaf_optimal_seq):\n",
    "    A_org = np.array(leaf_optimal_seq)\n",
    "    B_org = np.array(dtw_cluster)\n",
    "    \n",
    "    # Remove -1\n",
    "    A = A_org[B_org != -1]\n",
    "    B = B_org[B_org != -1]\n",
    "\n",
    "    # Maintain ascending sereis\n",
    "    C = np.append(np.diff(B),0)\n",
    "    \n",
    "    mask = C < 0\n",
    "    bad_classes = np.unique(B[mask])\n",
    "\n",
    "    keep_mask = ~np.isin(B, bad_classes)\n",
    "\n",
    "    A_filtered = A[keep_mask]\n",
    "    B_filtered = B[keep_mask]\n",
    "\n",
    "    unique_vals = np.unique(B_filtered)\n",
    "    new_labels = np.arange(len(unique_vals))\n",
    "    mapping = dict(zip(unique_vals, new_labels))\n",
    "\n",
    "    B_remapped = np.vectorize(mapping.get)(B_filtered)\n",
    "\n",
    "    return A_filtered,B_remapped\n",
    "\n",
    "\n",
    "leaf_optimal_seq, dtw_cluster = errro_cluster_remover(dtw_cluster, leaf_optimal_seq)\n",
    "leaf_optimal_len = np.shape(leaf_optimal_seq)[0]\n",
    "n_clusters = len(np.unique(dtw_cluster))\n",
    "\n",
    "dtw_seq_vec = np.zeros((leaf_optimal_len, leaf_optimal_len*2))\n",
    "for i in range(leaf_optimal_len):\n",
    "    dtw_seq_vec[i,:] = np.hstack((dtw_temporal_vec[leaf_optimal_seq[i],leaf_optimal_seq].real, dtw_temporal_vec[leaf_optimal_seq[i],leaf_optimal_seq].imag))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plot AP result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    from sklearn.manifold import Isomap\n",
    "    # 使用Isomap将数据降至2维（用于可视化）\n",
    "    isomap = Isomap(n_components=2)\n",
    "    X_iso = isomap.fit_transform(dtw_seq_vec)\n",
    "\n",
    "    # 绘制二维散点图，不同颜色代表不同聚类\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    scatter = plt.scatter(X_iso[:, 0], X_iso[:, 1], c=dtw_cluster, cmap='Set1', s=50)\n",
    "    plt.title(f'Affinity Propagation Clusters (n_clusters={n_clusters})')\n",
    "    plt.xlabel('Isomap Component 1')\n",
    "    plt.ylabel('Isomap Component 2')\n",
    "    plt.colorbar(scatter, label='Cluster Label')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confidence of Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Unilateral_T_test(data, x):\n",
    "    n = len(data)\n",
    "    if n < 2: return [-1]\n",
    "    mean = np.mean(data)\n",
    "    std = np.std(data, ddof=1)      # ddof = 1 for small sample\n",
    "\n",
    "    # t_stat = (x - mean) / (std)\n",
    "    t_stat = (x - mean) / (std / np.sqrt(n))\n",
    "    # H0： x > μ\n",
    "    p_value = 1 - stats.t.cdf(t_stat, df=n-1)\n",
    "\n",
    "\n",
    "    return p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00123629]\n",
      "[1.32356555e-06]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 1, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Validation of cluster\n",
    "inner_dist = []\n",
    "for i in range(n_clusters):\n",
    "    _cluster = dtw_seq_vec[dtw_cluster[:]==i,:]\n",
    "    if np.shape(_cluster)[0] < 2:\n",
    "        inner_dist.append([0])\n",
    "    else: \n",
    "        inner_dist.append(np.sum(np.diff(dtw_seq_vec[dtw_cluster[:]==i,:],axis=0)**2, axis=1))\n",
    "\n",
    "\n",
    "global_dist = []\n",
    "for i in range(np.shape(dtw_seq_vec)[0]-1):\n",
    "    global_dist.append(np.sum((dtw_seq_vec[i+1,:]-dtw_seq_vec[i,:])**2))\n",
    "global_dist = np.array(global_dist)\n",
    "\n",
    "cluster_connect = [] \n",
    "\n",
    "# # Average Criterion\n",
    "for i in range(n_clusters-1):\n",
    "    _pre = dtw_seq_vec[dtw_cluster[:]==i,:][-1]\n",
    "    _poi = dtw_seq_vec[dtw_cluster[:]==i+1,:][0]\n",
    "    intp_dst = np.sum(np.diff([_pre,_poi], axis=0)**2, axis=1)\n",
    "    _p_value = Unilateral_T_test(np.hstack((inner_dist[i],inner_dist[i+1])), intp_dst)\n",
    "    print(_p_value)\n",
    "    if _p_value > 0.005:\n",
    "        cluster_connect.append(i)\n",
    "\n",
    "\n",
    "# Global Criterion\n",
    "# for i in range(n_clusters-1):\n",
    "#     _pre = dtw_seq_vec[dtw_cluster[:]==i,:][-1]\n",
    "#     _poi = dtw_seq_vec[dtw_cluster[:]==i+1,:][0]\n",
    "#     intp_dst = np.sum(np.diff([_pre,_poi], axis=0)**2, axis=1)\n",
    "#     _p_value = Unilateral_T_test(global_dist, intp_dst)\n",
    "#     print(_p_value)\n",
    "#     if _p_value > 0.01:\n",
    "#         cluster_connect.append(i)\n",
    "\n",
    "# Closest Criterion\n",
    "# for i in range(n_clusters-1):\n",
    "#     _pre = dtw_seq_vec[dtw_cluster[:]==i,:][-1]\n",
    "#     _poi = dtw_seq_vec[dtw_cluster[:]==i+1,:][0]\n",
    "#     intp_dst = np.sum(np.diff([_pre,_poi], axis=0)**2, axis=1)\n",
    "\n",
    "#     p_pre = Unilateral_T_test(inner_dist[i], intp_dst)\n",
    "#     p_poi = Unilateral_T_test(inner_dist[i+1], intp_dst)\n",
    "#     print(p_pre, p_poi)\n",
    "#     if np.max([p_pre,p_poi]) > 0.01 and np.min([p_pre,p_poi]) > 0:    # merge\n",
    "#         cluster_connect.append(i)\n",
    "\n",
    "\n",
    "# Merge from high to low\n",
    "dtw_cluster_revision = np.array(dtw_cluster)\n",
    "for i in cluster_connect[::-1]:\n",
    "    for j in range(np.shape(dtw_cluster_revision)[0]):\n",
    "        _x = dtw_cluster_revision[j]\n",
    "        if _x == i + 1:\n",
    "            dtw_cluster_revision[j] = i\n",
    "        elif _x > i + 1:\n",
    "            dtw_cluster_revision[j] = _x - 1\n",
    "\n",
    "\n",
    "n_clusters_revision = len(np.unique(dtw_cluster_revision))\n",
    "dtw_cluster_revision\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empirical \n",
    "if False:\n",
    "    _cluster_0 = np.sum(np.diff(dtw_seq_vec[dtw_cluster[:]==0,:],axis=0)**2, axis=1)\n",
    "    _cluster_1 = np.sum(np.diff(dtw_seq_vec[dtw_cluster[:]==1,:],axis=0)**2, axis=1)\n",
    "    _cluster_2 = np.sum(np.diff(dtw_seq_vec[dtw_cluster[:]==2,:],axis=0)**2, axis=1)\n",
    "    _cluster_3 = np.sum(np.diff(dtw_seq_vec[dtw_cluster[:]==3,:],axis=0)**2, axis=1)\n",
    "\n",
    "    print(f\"\\n{_cluster_0}\\n{_cluster_1}\\n{_cluster_2}\\n{_cluster_3}\")\n",
    "\n",
    "    _intp_01 = np.sum(np.diff([dtw_seq_vec[dtw_cluster[:]==0,:][-1],dtw_seq_vec[dtw_cluster[:]==1,:][0]],axis=0)**2, axis=1)\n",
    "    _intp_12 = np.sum(np.diff([dtw_seq_vec[dtw_cluster[:]==1,:][-1],dtw_seq_vec[dtw_cluster[:]==2,:][0]],axis=0)**2, axis=1)\n",
    "    _intp_23 = np.sum(np.diff([dtw_seq_vec[dtw_cluster[:]==2,:][-1],dtw_seq_vec[dtw_cluster[:]==3,:][0]],axis=0)**2, axis=1)\n",
    "\n",
    "    print(f\"\\n{_intp_01}\\n{_intp_12}\\n{_intp_23}\")\n",
    "\n",
    "    _std_0 = np.std(_cluster_0)\n",
    "    _std_1 = np.std(_cluster_1)\n",
    "    _std_2 = np.std(_cluster_2)\n",
    "    _std_3 = np.std(_cluster_3)\n",
    "    _mean_0 = np.mean(_cluster_0)\n",
    "    _mean_1 = np.mean(_cluster_1)\n",
    "    _mean_2 = np.mean(_cluster_2)\n",
    "    _mean_3 = np.mean(_cluster_3)\n",
    "\n",
    "    print(f\"\\n{_mean_0}±{3*_std_0}\\n{_mean_1}±{3*_std_1}\\n{_mean_2}±{3*_std_2}\\n{_mean_3}±{3*_std_3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Single Cluster Outlier Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "6\n",
      "5\n",
      "4\n",
      "3\n",
      "2\n",
      "1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "eis_seq = np.array(leaf_optimal_seq)\n",
    "eis_cluster = np.array(dtw_cluster_revision)\n",
    "eis_cluster_n = len(np.unique(eis_cluster))\n",
    "\n",
    "for i in reversed(range(len(eis_seq))):\n",
    "    print(i)\n",
    "    if len(eis_cluster[eis_cluster == eis_cluster[i]]) == 1:\n",
    "        eis_cluster[i:] = eis_cluster[i:] - 1\n",
    "        eis_cluster = np.delete(eis_cluster, i)\n",
    "        eis_seq = np.delete(eis_seq, i)\n",
    "        eis_cluster_n = eis_cluster_n-1\n",
    "    # elif len(eis_cluster[eis_cluster == eis_cluster[i]]) == 2:\n",
    "    #     if eis_cluster[i] != 0 and eis_cluster[i] != eis_cluster_n-1:\n",
    "    #         eis_cluster[i:] = eis_cluster[i:] - 1\n",
    "            \n",
    "    #         eis_cluster = np.delete(eis_cluster, i)\n",
    "    #         eis_seq = np.delete(eis_seq, i)\n",
    "    #         i = i-1\n",
    "    #         eis_cluster = np.delete(eis_cluster, i)\n",
    "    #         eis_seq = np.delete(eis_seq, i)\n",
    "\n",
    "    #         eis_cluster_n = eis_cluster_n-1\n",
    "\n",
    "eis_anomaly = np.array([poi for poi in leaf_full if poi not in eis_seq])\n",
    "eis_cluster_n = len(np.unique(eis_cluster))\n",
    "\n",
    "\n",
    "# Old Version: Only remove Single Cluster Outlier at Terminal\n",
    "# if dtw_cluster_revision[0] != dtw_cluster_revision[1]:\n",
    "#     eis_anomaly = np.append(eis_anomaly[::-1],eis_seq[0])[::-1]\n",
    "#     eis_seq = eis_seq[1:]\n",
    "#     eis_cluster = eis_cluster[1:] - 1\n",
    "# if dtw_cluster_revision[-1] != dtw_cluster_revision[-2]:\n",
    "#     eis_anomaly = np.append(eis_anomaly,eis_seq[-1])\n",
    "#     eis_seq = eis_seq[:-1]\n",
    "#     eis_cluster = eis_cluster[:-1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot Cluster Before/After"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PLOT_FLAG:\n",
    "# if True:\n",
    "    # fig= plt.figure(figsize=(12,6), constrained_layout=True)\n",
    "    fig= plt.figure(figsize=(15,8), constrained_layout=False)\n",
    "    axis = [0] * 9\n",
    "    axis[0] = fig.add_subplot(2,3,1)        # Origin Sequence\n",
    "    axis[1] = fig.add_subplot(2,3,2)        # Origin Anomaly\n",
    "    axis[2] = fig.add_subplot(2,3,3)        # Cluster Sequence\n",
    "    axis[3] = fig.add_subplot(2,3,4)        # Cluster Anomaly\n",
    "    axis[4] = fig.add_subplot(2,3,5)        # Merge Sequence\n",
    "    axis[5] = fig.add_subplot(2,3,6)        # Merge Anomaly\n",
    "\n",
    "\n",
    "\n",
    "    axis[0].set_title(\"W/O Cluster\")\n",
    "    axis[1].set_title(\"Cluster Original\")\n",
    "    axis[2].set_title(\"Cluster Merged\")\n",
    "\n",
    "\n",
    "    ## W/O Cluster\n",
    "    cmap = plt.colormaps.get_cmap('rainbow_r')\n",
    "    for i in range(leaf_optimal_len):\n",
    "        _x = leaf_optimal_seq[i]\n",
    "        ch_eis = chData[_x,:,:]\n",
    "        _color = cmap(_x/num_samples)\n",
    "        axis[0].loglog(ch_eis[0,:], np.abs(ch_eis[1,:]+1j*ch_eis[2,:]), color = _color, linewidth=2, label=f\"S{i:02d}\")\n",
    "\n",
    "\n",
    "    cmap = plt.colormaps.get_cmap('rainbow_r')\n",
    "    for i in range(len(leaf_anomaly)):\n",
    "        _x = leaf_anomaly[i]\n",
    "        ch_eis = chData[_x,:,:]\n",
    "        _color = cmap(_x/num_samples)\n",
    "        axis[3].loglog(ch_eis[0,:], np.abs(ch_eis[1,:]+1j*ch_eis[2,:]), color = _color, linewidth=2, label=f\"S{_x:02d}\")\n",
    "    axis[3].legend()\n",
    "    axis[3].sharex(axis[0])\n",
    "    axis[3].sharey(axis[0])\n",
    "\n",
    "\n",
    "    ## Cluster Original\n",
    "    cmap = plt.colormaps.get_cmap('Set1')\n",
    "    for i in range(leaf_optimal_len):\n",
    "        _x = leaf_optimal_seq[i]\n",
    "        ch_eis = chData[_x,:,:]\n",
    "        _color = cmap(dtw_cluster[i])\n",
    "        axis[1].loglog(ch_eis[0,:], np.abs(ch_eis[1,:]+1j*ch_eis[2,:]), color = _color, linewidth=2, label=f\"{chr(ord('A')+dtw_cluster[i])}\")\n",
    "\n",
    "    _legend_handle = []\n",
    "    for i in range(n_clusters):\n",
    "        _legend_handle.append(mpatches.Patch(color = cmap(i), label = f\"{chr(ord('A')+i)}:{len(dtw_cluster[dtw_cluster==i])}\"))\n",
    "    axis[1].legend(handles=_legend_handle)\n",
    "\n",
    "\n",
    "\n",
    "    cmap = plt.colormaps.get_cmap('rainbow_r')\n",
    "    for i in range(len(leaf_anomaly)):\n",
    "        _x = leaf_anomaly[i]\n",
    "        ch_eis = chData[_x,:,:]\n",
    "        _color = cmap(_x/num_samples)\n",
    "        axis[4].loglog(ch_eis[0,:], np.abs(ch_eis[1,:]+1j*ch_eis[2,:]), color = _color, linewidth=2, label=f\"S{_x:02d}\")\n",
    "    axis[4].legend()\n",
    "    axis[4].sharex(axis[0])\n",
    "    axis[4].sharey(axis[0])\n",
    "\n",
    "\n",
    "\n",
    "    ## Merged\n",
    "    cmap = plt.colormaps.get_cmap('Set1')\n",
    "    for i in range(len(eis_seq)):\n",
    "        _x = eis_seq[i]\n",
    "        ch_eis = chData[_x,:,:]\n",
    "        _color = cmap(eis_cluster[i])\n",
    "        axis[2].loglog(ch_eis[0,:], np.abs(ch_eis[1,:]+1j*ch_eis[2,:]), color = _color, linewidth=2, label=f\"{chr(ord('A')+eis_cluster[i])}\")\n",
    "\n",
    "    _legend_handle = []\n",
    "    for i in range(eis_cluster_n):\n",
    "        _legend_handle.append(mpatches.Patch(color = cmap(i), label = f\"{chr(ord('A')+i)}:{len(eis_cluster[eis_cluster==i])}\"))\n",
    "    axis[2].legend(handles=_legend_handle)\n",
    "\n",
    "    axis[2].sharex(axis[0])\n",
    "    axis[2].sharey(axis[0])\n",
    "\n",
    "\n",
    "    cmap = plt.colormaps.get_cmap('rainbow_r')\n",
    "    for i in range(len(eis_anomaly)):\n",
    "        _x = eis_anomaly[i]\n",
    "        ch_eis = chData[_x,:,:]\n",
    "        _color = cmap(_x/num_samples)\n",
    "        axis[5].loglog(ch_eis[0,:], np.abs(ch_eis[1,:]+1j*ch_eis[2,:]), color = _color, linewidth=2, label=f\"S{_x:02d}\")\n",
    "    axis[5].legend()\n",
    "    axis[5].sharex(axis[0])\n",
    "    axis[5].sharey(axis[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig= plt.figure(figsize=(12,6), constrained_layout=True)\n",
    "fig= plt.figure(figsize=(15,8), constrained_layout=False)\n",
    "axis = [0] * 9\n",
    "axis[0] = fig.add_subplot(3,3,1, projection='3d')   # Original 3D\n",
    "axis[1] = fig.add_subplot(3,3,2)                    # Original 2D\n",
    "\n",
    "axis[3] = fig.add_subplot(3,3,4, projection='3d')   # Linkage 3D\n",
    "axis[4] = fig.add_subplot(3,3,5)                    # Linkage Sequence\n",
    "axis[5] = fig.add_subplot(3,3,6)                    # Linkage Anomaly\n",
    "\n",
    "axis[6] = fig.add_subplot(3,3,7, projection='3d')   # AP 3D\n",
    "axis[7] = fig.add_subplot(3,3,8)                    # AP Sequence\n",
    "axis[8] = fig.add_subplot(3,3,9)                    # AP Anomaly\n",
    "\n",
    "\n",
    "init_elev = 21  # 仰角\n",
    "init_azim = 55  # 方位角\n",
    "axis[0].view_init(elev=init_elev, azim=init_azim)\n",
    "axis[3].view_init(elev=init_elev, azim=init_azim)\n",
    "axis[6].view_init(elev=init_elev, azim=init_azim)\n",
    "\n",
    "\n",
    "axis[0].set_title(\"Original\")\n",
    "axis[3].set_title(\"Anomaly Detection\")\n",
    "axis[6].set_title(\"Cluster Analysis\")\n",
    "\n",
    "\n",
    "## Original\n",
    "_x = np.arange(num_samples)\n",
    "_y = np.log10(chData[0,0,:]).flatten()\n",
    "X, Y = np.meshgrid(_x, _y, indexing='ij')\n",
    "axis[0].plot_surface(X, Y, np.log10(np.abs(chData[:,1,:]+1j*chData[:,2,:])), cmap='viridis_r', alpha=0.8)\n",
    "\n",
    "\n",
    "cmap = plt.colormaps.get_cmap('rainbow_r')\n",
    "for i in range(num_samples):\n",
    "    ch_eis = chData[i,:,:]\n",
    "    _color = cmap(i/num_samples)\n",
    "    axis[1].loglog(ch_eis[0,:], np.abs(ch_eis[1,:]+1j*ch_eis[2,:]), color = _color, linewidth=2, label=f\"S{i:02d}\")\n",
    "    # axis[1].semilogx(ch_eis[0,:], np.angle(ch_eis[1,:]+1j*ch_eis[2,:]), color = _color, linewidth=2, label=f\"S{i:02d}\")\n",
    "\n",
    "\n",
    "## Anomaly Detection\n",
    "\n",
    "_x = np.arange(num_samples)[leaf_optimal_seq]\n",
    "_y = np.log10(chData[0,0,:]).flatten()\n",
    "X, Y = np.meshgrid(_x, _y, indexing='ij')\n",
    "axis[3].plot_surface(X, Y, np.log10(np.abs(chData[leaf_optimal_seq,1,:]+1j*chData[leaf_optimal_seq,2,:])), cmap='viridis_r', alpha=0.8)\n",
    "\n",
    "\n",
    "cmap = plt.colormaps.get_cmap('rainbow_r')\n",
    "for i in leaf_optimal_seq:\n",
    "    ch_eis = chData[i,:,:]\n",
    "    _color = cmap(i/num_samples)\n",
    "    axis[4].loglog(ch_eis[0,:], np.abs(ch_eis[1,:]+1j*ch_eis[2,:]), color = _color, linewidth=2, label=f\"S{i:02d}\")\n",
    "    # axis[4].semilogy(ch_eis[0,:], np.angle(ch_eis[1,:]+1j*ch_eis[2,:]), color = _color, linewidth=2, label=f\"S{i:02d}\")\n",
    "axis[4].sharex(axis[1])\n",
    "axis[4].sharey(axis[1])\n",
    "\n",
    "\n",
    "for i in leaf_anomaly:\n",
    "    ch_eis = chData[i,:,:]\n",
    "    _color = cmap(i/num_samples)\n",
    "    axis[5].loglog(ch_eis[0,:], np.abs(ch_eis[1,:]+1j*ch_eis[2,:]), color = _color, linewidth=2, label=f\"S{i:02d}\")\n",
    "axis[5].legend()\n",
    "axis[5].sharex(axis[1])\n",
    "axis[5].sharey(axis[1])\n",
    "\n",
    "\n",
    "## Cluster Analysis\n",
    "\n",
    "_x = np.arange(num_samples)[eis_seq]\n",
    "_y = np.log10(chData[0,0,:]).flatten()\n",
    "X, Y = np.meshgrid(_x, _y, indexing='ij')\n",
    "axis[6].plot_surface(X, Y, np.log10(np.abs(chData[eis_seq,1,:]+1j*chData[eis_seq,2,:])), cmap='viridis_r', alpha=0.8)\n",
    "\n",
    "\n",
    "cmap = plt.colormaps.get_cmap('Set1')\n",
    "for i in range(len(eis_seq)):\n",
    "    _x = eis_seq[i]\n",
    "    ch_eis = chData[_x,:,:]\n",
    "    _color = cmap(eis_cluster[i])\n",
    "    axis[7].loglog(ch_eis[0,:], np.abs(ch_eis[1,:]+1j*ch_eis[2,:]), color = _color, linewidth=2, label=f\"{chr(ord('A')+eis_cluster[i])}\")\n",
    "\n",
    "_legend_handle = []\n",
    "for i in range(eis_cluster_n):\n",
    "    _legend_handle.append(mpatches.Patch(color = cmap(i), label = f\"{chr(ord('A')+i)}:{len(eis_cluster[eis_cluster==i])}\"))\n",
    "axis[7].legend(handles=_legend_handle)\n",
    "\n",
    "axis[7].sharex(axis[1])\n",
    "axis[7].sharey(axis[1])\n",
    "\n",
    "\n",
    "cmap = plt.colormaps.get_cmap('rainbow_r')\n",
    "for i in range(len(eis_anomaly)):\n",
    "    _x = eis_anomaly[i]\n",
    "    ch_eis = chData[_x,:,:]\n",
    "    _color = cmap(_x/num_samples)\n",
    "    axis[8].loglog(ch_eis[0,:], np.abs(ch_eis[1,:]+1j*ch_eis[2,:]), color = _color, linewidth=2, label=f\"S{_x:02d}\")\n",
    "    # axis[8].semilogx(ch_eis[0,:], np.angle(ch_eis[1,:]+1j*ch_eis[2,:]), color = _color, linewidth=2, label=f\"S{_x:02d}\")\n",
    "axis[8].legend()\n",
    "axis[8].sharex(axis[1])\n",
    "axis[8].sharey(axis[1])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline Test Bench"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "PIPELINE_FLAG = True\n",
    "# if PIPELINE_FLAG:\n",
    "if False:\n",
    "    PLOT_FLAG = True\n",
    "\n",
    "\n",
    "    # rootPath = \"D:/Baihm/EISNN/Archive/01037160_归档\"  \n",
    "    # ch_id = 0   # Default  \n",
    "    # ch_id = 1   # 居然分成一类了？？？\n",
    "    # ch_id = 9   # 异常值没被筛出去，类似还有3\n",
    "    # ch_id = 7  # Normal Example\n",
    "    # ch_id = 20  # Normal to Short, Same to GPR  \n",
    "    # ch_id = 52    \n",
    "    # ch_id = 89  \n",
    "\n",
    "\n",
    "    # rootPath = \"D:/Baihm/EISNN/Archive/01037161_归档\"   # #2\n",
    "    # ch_id = 10  # Error - Ap结果不连续\n",
    "\n",
    "    # rootPath = \"D:/Baihm/EISNN/Dataset/05087163_归档\"\n",
    "    # ch_id = 7   # one outlier\n",
    "    # ch_id = 50  # No outlier but in two Phases\n",
    "    # ch_id = 55  # One outlier &wired end point\n",
    "    # ch_id = 114 # Open Circuit with on outpler\n",
    "\n",
    "\n",
    "    # rootPath = \"D:\\Baihm\\EISNN\\Archive/06047730_归档\"\n",
    "    # ch_id = 41  # outlier detection error but has 20 samples\n",
    "\n",
    "    # rootPath = \"D:/Baihm/EISNN/Archive/02067447_归档\"\n",
    "    # ch_id = 68  # Short all the time\n",
    "\n",
    "    # rootPath = \"D:/Baihm/EISNN/Archive/01067095_归档\"\n",
    "    # ch_id = 19    # First Sample is outlier\n",
    "\n",
    "    rootPath = \"D:/Baihm/EISNN/Archive/09290511_归档\"\n",
    "    ch_id = 13    # Up & Down, 2 outliers\n",
    "    # ch_id = 21    # Normal + 2 outlier\n",
    "    # ch_id = 41    # Normal + 2 outlier - *(Hard To Tell)\n",
    "    # ch_id = 79    # 3-class, What a mess\n",
    "\n",
    "    # rootPath = \"D:/Baihm/EISNN/Archive/11057712_归档\"\n",
    "    # ch_id = 106    # Very Good Electrode with 1 hidden outlier, and one phase shift\n",
    "\n",
    "    # rootPath = \"D:\\Baihm\\EISNN\\Archive/10057084_归档\"\n",
    "    # ch_id = 16    # Totaly Mess\n",
    "    # ch_id = 18    # Totaly Mess\n",
    "\n",
    "    # rootPath = \"D:\\Baihm\\EISNN\\Archive/11067223_归档\"\n",
    "    # ch_id = 124     # Perfect but two phase with one outlier\n",
    "\n",
    "\n",
    "    # rootPath = \"D:\\Baihm\\EISNN\\Archive/15361101_归档\"\n",
    "    # ch_id = 0     # Only One Sample - Run With Error\n",
    "\n",
    "\n",
    "    # rootPath = \"D:\\Baihm\\EISNN\\Archive/11207147_归档\"\n",
    "    # ch_id = 0     # Only Three Sample - Run With Error\n",
    "\n",
    "\n",
    "    # rootPath = \"D:\\Baihm\\EISNN\\Archive/06017758_归档\"\n",
    "    # ch_id = 96     # Perfect of Perfect\n",
    "\n",
    "    \n",
    "    # rootPath = \"D:\\Baihm\\EISNN\\Archive/22037380_归档\"\n",
    "    # ch_id = 20     # Connection Error\n",
    "    \n",
    "    # rootPath = \"D:\\Baihm\\EISNN\\Archive/22027365_归档\"\n",
    "    # ch_id = 27     # Connection Error\n",
    "\n",
    "    \n",
    "    # rootPath = \"D:\\Baihm\\EISNN\\Archive/10067077_归档\"\n",
    "    # ch_id = 20     # Connection Error\n",
    "\n",
    "    # rootPath = \"D:\\Baihm\\EISNN\\Archive/01067094_归档\"\n",
    "    # ch_id = 105     # Connection Error\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    EISDict = gatherCSV(rootPath)\n",
    "    chData_full = readChannel(ch_id, EISDict)\n",
    "    freq_list = np.linspace(1000,np.shape(chData_full)[2]-1,101,dtype=int, endpoint=True)\n",
    "\n",
    "    if False:\n",
    "        phz_calibration = np.loadtxt(\"./phz_Calib.txt\")\n",
    "        for i in range(np.shape(chData)[0]):\n",
    "            # ch_eis = EIS_recal(chData[i,:,:])\n",
    "            ch_eis = EIS_recal_ver02(chData[i,:,:], phz_calibration)\n",
    "            chData[i,:,:] = ch_eis\n",
    "    chData = chData_full[:,:,freq_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PIPELINE_FLAG:\n",
    "\n",
    "    import OutlierDetection\n",
    "\n",
    "    seq_weird = None\n",
    "    # eis_seq, eis_cluster, eis_anomaly, leaf_anomaly = OutlierDetection.OutlierDetection(chData)\n",
    "    eis_seq, eis_cluster, eis_anomaly, leaf_anomaly, seq_weird = OutlierDetection.OutlierDetection_Ver02(chData_full)\n",
    "    num_samples = chData.shape[0]\n",
    "    num_cluster = len(np.unique(eis_cluster))\n",
    "\n",
    "\n",
    "    ## Plot\n",
    "\n",
    "    # fig= plt.figure(figsize=(12,6), constrained_layout=True)\n",
    "    fig= plt.figure(figsize=(15,8), constrained_layout=False)\n",
    "    axis = [0] * 9\n",
    "    axis[0] = fig.add_subplot(3,3,1, projection='3d')   # Original 3D\n",
    "    axis[1] = fig.add_subplot(3,3,2)                    # Original 2D\n",
    "    if seq_weird is not None:\n",
    "        axis[2] = fig.add_subplot(3,3,3)                    # Original 2D\n",
    "\n",
    "\n",
    "    axis[3] = fig.add_subplot(3,3,4, projection='3d')   # Linkage 3D\n",
    "    axis[4] = fig.add_subplot(3,3,5)                    # Linkage Sequence\n",
    "    axis[5] = fig.add_subplot(3,3,6)                    # Linkage Anomaly\n",
    "\n",
    "    axis[6] = fig.add_subplot(3,3,7, projection='3d')   # AP 3D\n",
    "    axis[7] = fig.add_subplot(3,3,8)                    # AP Sequence\n",
    "    axis[8] = fig.add_subplot(3,3,9)                    # AP Anomaly\n",
    "\n",
    "\n",
    "    init_elev = 21  # 仰角\n",
    "    init_azim = 55  # 方位角\n",
    "    axis[0].view_init(elev=init_elev, azim=init_azim)\n",
    "    axis[3].view_init(elev=init_elev, azim=init_azim)\n",
    "    axis[6].view_init(elev=init_elev, azim=init_azim)\n",
    "\n",
    "\n",
    "    axis[0].set_title(\"Original\")\n",
    "    axis[3].set_title(\"Anomaly Detection\")\n",
    "    axis[6].set_title(\"Cluster Analysis\")\n",
    "\n",
    "\n",
    "    ## Original\n",
    "    _x = np.arange(num_samples)\n",
    "    _y = np.log10(chData[0,0,:]).flatten()\n",
    "    X, Y = np.meshgrid(_x, _y, indexing='ij')\n",
    "    axis[0].plot_surface(X, Y, np.log10(np.abs(chData[:,1,:]+1j*chData[:,2,:])), cmap='viridis_r', alpha=0.8)\n",
    "\n",
    "\n",
    "    cmap = plt.colormaps.get_cmap('rainbow_r')\n",
    "    for i in range(num_samples):\n",
    "        ch_eis = chData[i,:,:]\n",
    "        _color = cmap(i/num_samples)\n",
    "        axis[1].loglog(ch_eis[0,:], np.abs(ch_eis[1,:]+1j*ch_eis[2,:]), color = _color, linewidth=2, label=f\"S{i:02d}\")\n",
    "\n",
    "    if seq_weird is not None:\n",
    "        for i in seq_weird:\n",
    "            ch_eis = chData[i,:,:]\n",
    "            _color = cmap(i/num_samples)\n",
    "            axis[2].loglog(ch_eis[0,:], np.abs(ch_eis[1,:]+1j*ch_eis[2,:]), color = _color, linewidth=2, label=f\"S{i:02d}\")\n",
    "        axis[2].legend()\n",
    "        axis[2].sharex(axis[1])\n",
    "        axis[2].sharey(axis[1])\n",
    "\n",
    "    ## Anomaly Detection\n",
    "\n",
    "    _x = np.arange(num_samples)[eis_seq]\n",
    "    _y = np.log10(chData[0,0,:]).flatten()\n",
    "    X, Y = np.meshgrid(_x, _y, indexing='ij')\n",
    "    axis[3].plot_surface(X, Y, np.log10(np.abs(chData[eis_seq,1,:]+1j*chData[eis_seq,2,:])), cmap='viridis_r', alpha=0.8)\n",
    "\n",
    "\n",
    "    cmap = plt.colormaps.get_cmap('rainbow_r')\n",
    "    for i in eis_seq:\n",
    "        ch_eis = chData[i,:,:]\n",
    "        _color = cmap(i/num_samples)\n",
    "        axis[4].loglog(ch_eis[0,:], np.abs(ch_eis[1,:]+1j*ch_eis[2,:]), color = _color, linewidth=2, label=f\"S{i:02d}\")\n",
    "    axis[4].sharex(axis[1])\n",
    "    axis[4].sharey(axis[1])\n",
    "\n",
    "\n",
    "    for i in leaf_anomaly:\n",
    "        ch_eis = chData[i,:,:]\n",
    "        _color = cmap(i/num_samples)\n",
    "        axis[5].loglog(ch_eis[0,:], np.abs(ch_eis[1,:]+1j*ch_eis[2,:]), color = _color, linewidth=2, label=f\"S{i:02d}\")\n",
    "    axis[5].legend()\n",
    "    axis[5].sharex(axis[1])\n",
    "    axis[5].sharey(axis[1])\n",
    "\n",
    "\n",
    "    ## Cluster Analysis\n",
    "\n",
    "    _x = np.arange(num_samples)[eis_seq]\n",
    "    _y = np.log10(chData[0,0,:]).flatten()\n",
    "    X, Y = np.meshgrid(_x, _y, indexing='ij')\n",
    "    axis[6].plot_surface(X, Y, np.log10(np.abs(chData[eis_seq,1,:]+1j*chData[eis_seq,2,:])), cmap='viridis_r', alpha=0.8)\n",
    "\n",
    "\n",
    "    cmap = plt.colormaps.get_cmap('Set1')\n",
    "    for i in range(len(eis_seq)):\n",
    "        _x = eis_seq[i]\n",
    "        ch_eis = chData[_x,:,:]\n",
    "        _color = cmap(eis_cluster[i])\n",
    "        axis[7].loglog(ch_eis[0,:], np.abs(ch_eis[1,:]+1j*ch_eis[2,:]), color = _color, linewidth=2, label=f\"{chr(ord('A')+eis_cluster[i])}\")\n",
    "\n",
    "    _legend_handle = []\n",
    "    for i in range(num_cluster):\n",
    "        _legend_handle.append(mpatches.Patch(color = cmap(i), label = f\"{chr(ord('A')+i)}:{len(eis_cluster[eis_cluster==i])}\"))\n",
    "    axis[7].legend(handles=_legend_handle)\n",
    "\n",
    "    axis[7].sharex(axis[1])\n",
    "    axis[7].sharey(axis[1])\n",
    "\n",
    "\n",
    "    cmap = plt.colormaps.get_cmap('rainbow_r')\n",
    "    for i in range(len(eis_anomaly)):\n",
    "        _x = eis_anomaly[i]\n",
    "        ch_eis = chData[_x,:,:]\n",
    "        _color = cmap(_x/num_samples)\n",
    "        axis[8].loglog(ch_eis[0,:], np.abs(ch_eis[1,:]+1j*ch_eis[2,:]), color = _color, linewidth=2, label=f\"S{_x:02d}\")\n",
    "    axis[8].legend()\n",
    "    axis[8].sharex(axis[1])\n",
    "    axis[8].sharey(axis[1])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline + Short & Open"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import OutlierDetection\n",
    "# eis_seq, eis_cluster, eis_anomaly, leaf_anomaly, weird_mask = OutlierDetection.OutlierDetection_Ver02(chData_full, mask_flag=False)\n",
    "# open_mask, short_mask = OutlierDetection.OpenShortDetection(chData, mask_flag = False)\n",
    "eis_seq, eis_cluster, eis_anomaly, leaf_anomaly, seq_weird = OutlierDetection.OutlierDetection_Ver02(chData_full, mask_flag=False)\n",
    "seq_open, seq_short = OutlierDetection.OpenShortDetection(chData_full, mask_flag = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = chData.shape[0]\n",
    "num_cluster = len(np.unique(eis_cluster))\n",
    "\n",
    "# seq_full    = np.arange(num_samples)\n",
    "# seq_weird   = seq_full[weird_mask]\n",
    "# seq_open    = seq_full[open_mask]\n",
    "# seq_short   = seq_full[short_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OutlierDetectionPlot(fig, chData, eis_seq, eis_cluster, eis_anomaly, leaf_anomaly, seq_weird, seq_open, seq_short):\n",
    "    # fig= plt.figure(figsize=(15,8), constrained_layout=False)\n",
    "    num_samples = chData.shape[0]\n",
    "    num_cluster = len(np.unique(eis_cluster))\n",
    "\n",
    "    seq_open_eis    = np.intersect1d(eis_seq, seq_open)\n",
    "    seq_short_eis   = np.intersect1d(eis_seq, seq_short)\n",
    "\n",
    "\n",
    "    axis    = [0] * 11\n",
    "    axis[0] = fig.add_subplot(3,3,1, projection='3d')   # Original 3D\n",
    "    axis[1] = fig.add_subplot(3,3,2)                    # Original 2D\n",
    "    axis[2] = fig.add_subplot(3,3,3)                    # Original 2D\n",
    "    axis[3] = fig.add_subplot(3,3,4, projection='3d')   # Linkage 3D\n",
    "    axis[4] = fig.add_subplot(3,3,5)                    # Linkage Sequence\n",
    "    axis[5] = fig.add_subplot(3,3,6)                    # Linkage Anomaly\n",
    "    axis[7] = fig.add_subplot(3,3,8)                    # AP Sequence\n",
    "    axis[8] = fig.add_subplot(3,3,9)                    # AP Anomaly\n",
    "    \n",
    "    \n",
    "    axis[6] = fig.add_subplot(3,3,7)   # Text\n",
    "    text_axis = axis[6]\n",
    "    text_axis.axis('off')\n",
    "\n",
    "    init_elev = 21  # 仰角\n",
    "    init_azim = 55  # 方位角\n",
    "    axis[0].view_init(elev=init_elev, azim=init_azim)\n",
    "    axis[3].view_init(elev=init_elev, azim=init_azim)\n",
    "    # axis[6].view_init(elev=init_elev, azim=init_azim)\n",
    "\n",
    "\n",
    "    axis[0].set_title(\"Original\")\n",
    "    axis[3].set_title(\"Anomaly Detection\")\n",
    "    # axis[6].set_title(\"Cluster Analysis\")\n",
    "\n",
    "    \n",
    "    axis[1].set_title(\"Original Data\")\n",
    "    axis[4].set_title(\"After Outlier Detection\")\n",
    "    axis[7].set_title(\"After Cluster\")\n",
    "\n",
    "    \n",
    "    axis[2].set_title(\"Type I Outlier\")\n",
    "    axis[5].set_title(\"Type II Outlier\")\n",
    "    axis[8].set_title(\"Open-Short\")\n",
    "\n",
    "\n",
    "    ## Original\n",
    "    _x = np.arange(num_samples)\n",
    "    _y = np.log10(chData[0,0,:]).flatten()\n",
    "    X, Y = np.meshgrid(_x, _y, indexing='ij')\n",
    "    axis[0].plot_surface(X, Y, np.log10(np.abs(chData[:,1,:]+1j*chData[:,2,:])), cmap='viridis_r', alpha=0.8)\n",
    "\n",
    "\n",
    "    cmap = plt.colormaps.get_cmap('rainbow_r')\n",
    "    for i in range(num_samples):\n",
    "        ch_eis = chData[i,:,:]\n",
    "        _color = cmap(i/num_samples)\n",
    "        axis[1].loglog(ch_eis[0,:], np.abs(ch_eis[1,:]+1j*ch_eis[2,:]), color = _color, linewidth=2, label=f\"S{i:02d}\")\n",
    "\n",
    "    for i in seq_weird:\n",
    "        ch_eis = chData[i,:,:]\n",
    "        _color = cmap(i/num_samples)\n",
    "        axis[2].loglog(ch_eis[0,:], np.abs(ch_eis[1,:]+1j*ch_eis[2,:]), color = _color, linewidth=2, label=f\"S{i:02d}\")\n",
    "    axis[2].legend()\n",
    "    axis[2].sharex(axis[1])\n",
    "    axis[2].sharey(axis[1])\n",
    "\n",
    "    ## Anomaly Detection\n",
    "\n",
    "    _x = np.arange(num_samples)[eis_seq]\n",
    "    _y = np.log10(chData[0,0,:]).flatten()\n",
    "    X, Y = np.meshgrid(_x, _y, indexing='ij')\n",
    "    axis[3].plot_surface(X, Y, np.log10(np.abs(chData[eis_seq,1,:]+1j*chData[eis_seq,2,:])), cmap='viridis_r', alpha=0.8)\n",
    "\n",
    "\n",
    "    cmap = plt.colormaps.get_cmap('rainbow_r')\n",
    "    for i in eis_seq:\n",
    "        ch_eis = chData[i,:,:]\n",
    "        _color = cmap(i/num_samples)\n",
    "        axis[4].loglog(ch_eis[0,:], np.abs(ch_eis[1,:]+1j*ch_eis[2,:]), color = _color, linewidth=2, label=f\"S{i:02d}\")\n",
    "    axis[4].sharex(axis[1])\n",
    "    axis[4].sharey(axis[1])\n",
    "\n",
    "\n",
    "    for i in leaf_anomaly:\n",
    "        ch_eis = chData[i,:,:]\n",
    "        _color = cmap(i/num_samples)\n",
    "        axis[5].loglog(ch_eis[0,:], np.abs(ch_eis[1,:]+1j*ch_eis[2,:]), color = _color, linewidth=2, label=f\"S{i:02d}\")\n",
    "    axis[5].legend()\n",
    "    axis[5].sharex(axis[1])\n",
    "    axis[5].sharey(axis[1])\n",
    "\n",
    "\n",
    "    ## Cluster Analysis\n",
    "\n",
    "    # _x = np.arange(num_samples)[eis_seq]\n",
    "    # _y = np.log10(chData[0,0,:]).flatten()\n",
    "    # X, Y = np.meshgrid(_x, _y, indexing='ij')\n",
    "    # axis[6].plot_surface(X, Y, np.log10(np.abs(chData[eis_seq,1,:]+1j*chData[eis_seq,2,:])), cmap='viridis_r', alpha=0.8)\n",
    "\n",
    "\n",
    "    cmap = plt.colormaps.get_cmap('Set1')\n",
    "    for i in range(len(eis_seq)):\n",
    "        _x = eis_seq[i]\n",
    "        ch_eis = chData[_x,:,:]\n",
    "        _color = cmap(eis_cluster[i])\n",
    "        axis[7].loglog(ch_eis[0,:], np.abs(ch_eis[1,:]+1j*ch_eis[2,:]), color = _color, linewidth=2, label=f\"{chr(ord('A')+eis_cluster[i])}\")\n",
    "\n",
    "    _legend_handle = []\n",
    "    for i in range(num_cluster):\n",
    "        _legend_handle.append(mpatches.Patch(color = cmap(i), label = f\"{chr(ord('A')+i)}:{len(eis_cluster[eis_cluster==i])}\"))\n",
    "    axis[7].legend(handles=_legend_handle)\n",
    "\n",
    "    axis[7].sharex(axis[1])\n",
    "    axis[7].sharey(axis[1])\n",
    "\n",
    "    # Open Short\n",
    "\n",
    "    cmap = plt.colormaps.get_cmap('managua')\n",
    "    for i in range(len(eis_seq)):\n",
    "        _x = eis_seq[i]\n",
    "        if _x in seq_open_eis:     \n",
    "            _color = cmap(0.0)\n",
    "            alpha = 1\n",
    "        elif _x in seq_short_eis:   \n",
    "            _color = cmap(1.0)\n",
    "            alpha = 1\n",
    "        else:                       \n",
    "            _color = cmap(0.5)\n",
    "            alpha = 0.2\n",
    "        ch_eis = chData[_x,:,:]\n",
    "        axis[8].loglog(ch_eis[0,:], np.abs(ch_eis[1,:]+1j*ch_eis[2,:]), color = _color, linewidth=2, alpha = alpha)\n",
    "\n",
    "    _legend_handle = []\n",
    "    _legend_handle.append(mpatches.Patch(color = cmap(0.5), label = f\"Norm:{len(eis_seq) - len(seq_open_eis) - len(seq_short_eis)}\"))\n",
    "    _legend_handle.append(mpatches.Patch(color = cmap(0.0), label = f\"Open:{len(seq_open_eis)}\"))\n",
    "    _legend_handle.append(mpatches.Patch(color = cmap(1.0), label = f\"Short:{len(seq_short_eis)}\"))\n",
    "    axis[8].legend(handles=_legend_handle)\n",
    "    axis[8].sharex(axis[1])\n",
    "    axis[8].sharey(axis[1])\n",
    "\n",
    "    return text_axis\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16, 9), constrained_layout=True)\n",
    "axis = OutlierDetectionPlot(fig, chData, eis_seq, eis_cluster, eis_anomaly, leaf_anomaly, seq_weird, seq_open, seq_short)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EISNN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
